{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XOsrfG4NoV2i","vscode":{"languageId":"bat"}},"outputs":[],"source":["!git clone https://github.com/DMwr2023/S4Dmodification.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"939d0OzWoV2l","scrolled":true,"vscode":{"languageId":"bat"}},"outputs":[],"source":["# Structured Kernels（restart kernelをしてからもう一回する必要があるかもしれない。その場合だいぶ時間がかかる。逆にだいぶ時間がかかんなかったらうまくいってない）\n","%cd /workspace/S4Dmodification\n","# ============= Set Up =============\n","# Requirements\n","# 入れなくても動く\n","# !conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n","!pip install -r requirements.txt\n","# Structured Kernels\n","%cd extensions/kernels/\n","!python setup.py install"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kw_9oXg0RnFs"},"outputs":[],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFrG-uLkRnFt"},"outputs":[],"source":["# うまくいかなければrestart kernelをする（もしくは上を再実行する）\n","import importlib\n","import structured_kernels\n","importlib.reload(structured_kernels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZ_AGAfNRnFt"},"outputs":[],"source":["import sys\n","sys.path.append(\"/workspace/S4Dmodification/extensions/kernels\")\n","import structured_kernels\n","print(\"Structured kernels module loaded successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3ACROoYRnFu"},"outputs":[],"source":["!pip show structured_kernels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16694,"status":"ok","timestamp":1737338398973,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"},"user_tz":-540},"id":"Y6bztFv3pmuc","outputId":"ced6bd94-ea4b-4243-f11a-15ea0c39f373"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/structured_kernels-0.1.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n","\u001b[0mCollecting pybullet\n","  Downloading pybullet-3.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Downloading pybullet-3.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pybullet\n","Successfully installed pybullet-3.2.6\n"]}],"source":["!pip install pybullet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDkoT6sbpM5j"},"outputs":[],"source":["%cd /workspace/S4Dmodification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20039,"status":"ok","timestamp":1737339754366,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"},"user_tz":-540},"id":"g08VOGf8oV2m","outputId":"7de36d42-2137-4d13-fa81-e33d567b0a11"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:models.s4.s4:CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) found.\n","/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n","  from jax import xla_computation as _xla_computation\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions.kl import kl_divergence\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import time\n","import argparse\n","\n","from models.s4.s4 import S4Block as S4  # Can use full version instead of minimal S4D standalone below\n","from models.s4.s4d import S4D\n","#from models.s4.s4d import S4D\n","from tqdm.auto import tqdm\n","import copy\n","import gc\n","\n","from typing import Any, List, Tuple\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import gym\n","import pybullet_envs  # PyBulletの環境をgymに登録する\n","from einops import rearrange\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Dropout broke in PyTorch 1.11\n","if tuple(map(int, torch.__version__.split('.')[:2])) == (1, 11):\n","    print(\"WARNING: Dropout is bugged in PyTorch 1.11. Results may be worse.\")\n","    dropout_fn = nn.Dropout\n","if tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 12):\n","    dropout_fn = nn.Dropout1d\n","else:\n","    dropout_fn = nn.Dropout2d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmSpRiG-oV2o"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"LO_awH5qoV2r"},"source":["### 環境のWrapper（カメラに関する）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeUt9xnPoV2r"},"outputs":[],"source":["class GymWrapper_PyBullet(object):\n","    \"\"\"\n","    PyBullet環境のためのラッパー\n","    \"\"\"\n","\n","    metadata = {\"render.modes\": [\"human\", \"rgb_array\"]}\n","    reward_range = (-np.inf, np.inf)\n","\n","    # __init__でカメラ位置に関するパラメータ（ cam_dist:カメラ距離，cam_yaw：カメラの水平面での回転，cam_pitch:カメラの縦方向での回転）を受け取り，カメラの位置を調整できるようにします.\n","    # 　同時に画像の大きさも変更できるようにします\n","    def __init__(\n","        self,\n","        env: gym.Env,\n","        cam_dist: int = 3,\n","        cam_yaw: int = 0,\n","        cam_pitch: int = -30,\n","        render_width: int = 320,\n","        render_height: int = 240,\n","    ) -> None:\n","        \"\"\"\n","        コンストラクタ．\n","\n","        Parameters\n","        ----------\n","        env : gym.Env\n","            gymで提供されている環境のインスタンス．\n","        cam_dist : int\n","            カメラの距離．\n","        cam_yaw : int\n","            カメラの水平面での回転．\n","        cam_pitch : int\n","            カメラの縦方向での回転．\n","        render_width : int\n","            観測画像の幅．\n","        render_height : int\n","            観測画像の高さ．\n","        \"\"\"\n","        self._env = env\n","\n","        self._render_width = render_width\n","        self._render_height = render_height\n","        self._set_nested_attr(self._env, cam_dist, \"_cam_dist\")\n","        self._set_nested_attr(self._env, cam_yaw, \"_cam_yaw\")\n","        self._set_nested_attr(self._env, cam_pitch, \"_cam_pitch\")\n","        self._set_nested_attr(self._env, render_width, \"_render_width\")\n","        self._set_nested_attr(self._env, render_height, \"_render_height\")\n","\n","    def _set_nested_attr(self, env: gym.Env, value: int, attr: str) -> None:\n","        \"\"\"\n","        多重継承の属性に再帰的にアクセスして値を変更する．\n","        カメラの設定に利用．\n","\n","        Parameters\n","        ----------\n","        value : int\n","            設定したい値．\n","        attr : str\n","            変更したい属性の名前．\n","        \"\"\"\n","        if hasattr(env, attr):\n","            setattr(env, attr, value)\n","        else:\n","            self._set_nested_attr(env.env, value, attr)\n","\n","    def __getattr(self, name: str) -> Any:\n","        \"\"\"\n","        環境が保持している属性値を取得するメソッド．\n","\n","        Parameters\n","        ----------\n","        name : str\n","            取得したい属性値の名前．\n","\n","        Returns\n","        -------\n","        _env.name : Any\n","            環境が保持している属性値．\n","        \"\"\"\n","        return getattr(self._env, name)\n","\n","    @property\n","    def observation_space(self) -> gym.spaces.Box:\n","        \"\"\"\n","        観測空間に関する情報を取得するメソッド．\n","\n","        Returns\n","        -------\n","        space : gym.spaces.Box\n","            観測空間に関する情報（各画素値の最小値，各画素値の最大値，観測データの形状， データの型）．\n","        \"\"\"\n","        width = self._render_width\n","        height = self._render_height\n","        return gym.spaces.Box(0, 255, (height, width, 3), dtype=np.uint8)\n","\n","    @property\n","    def action_space(self) -> gym.spaces.Box:\n","        \"\"\"\n","        行動空間に関する情報を取得するメソッド．\n","\n","        Returns\n","        -------\n","        space : gym.spaces.Box\n","            行動空間に関する情報（各行動の最小値，各行動の最大値，行動空間の次元， データの型） ．\n","        \"\"\"\n","        return self._env.action_space\n","\n","    # 　元の観測（低次元の状態）は今回は捨てて，env.render()で取得した画像を観測とします.\n","    #  画像，報酬，終了シグナルが得られます.\n","    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n","        \"\"\"\n","        環境に行動を与え次の観測，報酬，終了フラグを取得するメソッド．\n","\n","        Parameters\n","        ----------\n","        action : np.dnarray (action_dim, )\n","            与える行動．\n","\n","        Returns\n","        -------\n","        obs : np.ndarray (height, width, 3)\n","            行動を与えたときの次の観測．\n","        reward : float\n","            行動を与えたときに得られる報酬．\n","        done : bool\n","            エピソードが終了したかどうか表すフラグ．\n","        info : dict\n","            その他の環境に関する情報．\n","        \"\"\"\n","        _, reward, done, info = self._env.step(action)\n","        obs = self._env.render(mode=\"rgb_array\") # 今回状態として画像を扱いたいため\n","        return obs, reward, done, info\n","\n","    def reset(self) -> np.ndarray:\n","        \"\"\"\n","        環境をリセットするためのメソッド．\n","\n","        Returns\n","        -------\n","        obs : np.ndarray (height, width, 3)\n","            環境をリセットしたときの初期の観測．\n","        \"\"\"\n","        self._env.reset()\n","        obs = self._env.render(mode=\"rgb_array\")\n","        return obs\n","\n","    def render(self, mode=\"human\", **kwargs) -> np.ndarray:\n","        \"\"\"\n","        観測をレンダリングするためのメソッド．\n","\n","        Parameters\n","        ----------\n","        mode : str\n","            レンダリング方法に関するオプション． (default='human')\n","\n","        Returns\n","        -------\n","        obs : np.ndarray (height, width, 3)\n","            観測をレンダリングした結果．\n","        \"\"\"\n","        return self._env.render(mode, **kwargs)\n","\n","    def close(self) -> None:\n","        \"\"\"\n","        環境を閉じるためのメソッド．\n","        \"\"\"\n","        self._env.close()\n"]},{"cell_type":"markdown","metadata":{"id":"_mu7zty7oV2s"},"source":["#### カメラに関するWrapperのテスト"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":719},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1737339754682,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"},"user_tz":-540},"id":"bssBjMXHoV2s","outputId":"05ab5dde-7562-4758-bf97-86bcebb6bfb9"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n","  logger.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["env = gym.make(\"HalfCheetahBulletEnv-v0\")\n","# カメラのパラメータを与えてカメラの位置と角度，画像の大きさを調整\n","env = GymWrapper_PyBullet(\n","    env, cam_dist=2, cam_pitch=0, render_width=64, render_height=64\n",")\n","\n","env.reset()\n","image = env.render(mode=\"rgb_array\")\n","plt.imshow(image)\n","plt.show()\n","env.close()\n","del env"]},{"cell_type":"markdown","metadata":{"id":"XKpfbv9loV2s"},"source":["### 環境のWrapper（行動の連続入力に関する）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pVE1R8koV2t"},"outputs":[],"source":["class RepeatAction(gym.Wrapper):\n","    \"\"\"\n","    同じ行動を指定された回数自動的に繰り返すラッパー．観測は最後の行動に対応するものになる\n","    \"\"\"\n","\n","    def __init__(self, env: GymWrapper_PyBullet, skip: int = 4) -> None:\n","        \"\"\"\n","        コンストラクタ．\n","\n","        Parameters\n","        ----------\n","        env : GymWrapper_PyBullet\n","            環境のインスタンス．今回は先程定義したラッパーでラップした環境を利用する．\n","        skip : int\n","            同じ行動を繰り返す回数．\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        self._skip = skip\n","\n","    def reset(self) -> np.ndarray:\n","        \"\"\"\n","        環境をリセットするためのメソッド．\n","\n","        Returns\n","        -------\n","        obs : np.ndarray (width, height, 3)\n","            環境をリセットしたときの初期の観測．\n","        \"\"\"\n","        return self.env.reset()\n","\n","    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n","        \"\"\"\n","        環境に行動を与え次の観測，報酬，終了フラグを取得するメソッド．\n","        与えられた行動をskipの回数だけ繰り返した結果を返す．\n","\n","        Parameters\n","        ----------\n","        action : np.ndarray (action_dim, )\n","            与える行動．\n","\n","        Returns\n","        -------\n","        obs : np.ndarray (width, height, 3)\n","            行動をskipの回数だけ繰り返したあとの観測．\n","        total_reawrd : float\n","            行動をskipの回数だけ繰り返したときの報酬和．\n","        done : bool\n","            エピソードが終了したかどうか表すフラグ．\n","        info : dict\n","            その他の環境に関する情報．\n","        \"\"\"\n","        total_reward = 0.0\n","        for _ in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","            if done:\n","                break\n","        return obs, total_reward, done, info"]},{"cell_type":"markdown","metadata":{"id":"fJzUltaMoV2t"},"source":["#### Wrapperを通した環境を作る関数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxP0b5VaoV2t"},"outputs":[],"source":["def make_env() -> RepeatAction:\n","    \"\"\"\n","    作成たラッパーをまとめて適用して環境を作成する関数．\n","\n","    Returns\n","    -------\n","    env : RepeatAction\n","        ラッパーを適用した環境．\n","    \"\"\"\n","    env = gym.make(\"HalfCheetahBulletEnv-v0\")  # 環境を読み込む．今回はHalfCheetah\n","    # Dreamerでは観測は64x64のRGB画像\n","    env = GymWrapper_PyBullet(\n","        env, cam_dist=2, cam_pitch=0, render_width=64, render_height=64\n","    )\n","    env = RepeatAction(env, skip=2)  # DreamerではActionRepeatは2\n","    return env"]},{"cell_type":"markdown","metadata":{"id":"rTf30JzooV2u"},"source":["### Replay Buffer\n","連続した経験をとってくるのでDQNとは少し違う"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKX042qhoV2u"},"outputs":[],"source":["# 　今回のReplayBuffer\n","class ReplayBuffer(object):\n","    \"\"\"\n","    RNNを用いて訓練するのに適したリプレイバッファ．\n","    \"\"\"\n","\n","    def __init__(\n","        self, capacity: int, observation_shape: List[int], action_dim: int\n","    ) -> None:\n","        \"\"\"\n","        コンストラクタ．\n","\n","        Parameters\n","        ----------\n","        capacity : int\n","            リプレイバッファにためておくことができる経験の上限．\n","        observation_shape : List[int]\n","            環境から与えられる観測の形状．\n","        action_dim : int\n","            行動空間の次元数．\n","        \"\"\"\n","\n","        self.observations = np.zeros((capacity, *observation_shape), dtype=np.uint8)\n","        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n","        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n","        self.done = np.zeros((capacity, 1), dtype=bool)\n","        # self.done = np.zeros((capacity, 1), dtype=np.bool)\n","\n","        self.index = 0\n","        self.is_filled = False\n","        self.capacity = capacity\n","\n","    def push(\n","        self, observation: np.ndarray, action: np.ndarray, reward: float, done: bool\n","    ) -> None:\n","        \"\"\"\n","        リプレイバッファに経験を追加するメソッド．\n","\n","        Parameters\n","        ----------\n","        observation : np.ndarray (64, 64, 3)\n","            環境から得られた観測．\n","        action : np.ndarray (action_dim, )\n","            エージェントがとった（もしくは経験を貯める際のランダムな）行動．\n","        reward : float\n","            観測に対して行動をとったときに得られる報酬．\n","        done : bool\n","            エピソードが終了するかどうかのフラグ．\n","        \"\"\"\n","        self.observations[self.index] = observation\n","        self.actions[self.index] = action\n","        self.rewards[self.index] = reward\n","        self.done[self.index] = done\n","\n","        # indexは巡回し，最も古い経験を上書きする\n","        if self.index == self.capacity - 1:\n","            self.is_filled = True\n","        self.index = (self.index + 1) % self.capacity\n","\n","    def sample(self, batch_size: int, chunk_length: int) -> Tuple[np.ndarray]:\n","        \"\"\"\n","        経験をリプレイバッファからサンプルします．（ほぼ）一様なサンプルです．\n","        結果として返ってくるのは観測（画像），行動，報酬，終了シグナルについての(batch_size, chunk_length, 各要素の次元)の配列です．\n","        各バッチは連続した経験になっています．\n","        注意: chunk_lengthをあまり大きな値にすると問題が発生する場合があります．\n","\n","        Parameters\n","        ----------\n","        batch_size : int\n","            バッチサイズ．\n","        chunk_length : int\n","            バッチあたりの系列長．\n","\n","\n","        Returns\n","        -------\n","        sampled_observations : np.ndarray (batch size, chunk length, 3, 64, 64)\n","            バッファからサンプリングされた観測．\n","        sampled_actions : np.ndarray (batch size, chunk length, action dim)\n","            バッファからサンプリングされた行動．\n","        sampled_rewards : np.ndarray (batch size, chunk length, 1)\n","            バッファからサンプリングされた報酬．\n","        sampled_done : np.ndarray (batch size, chunk length, 1)\n","            バッファからサンプリングされたエピソードの終了フラグ．\n","        \"\"\"\n","        episode_borders = np.where(self.done)[0]\n","        sampled_indexes = []\n","        for _ in range(batch_size):\n","            cross_border = True\n","            while cross_border:\n","                initial_index = np.random.randint(len(self) - chunk_length + 1)\n","                final_index = initial_index + chunk_length - 1\n","                cross_border = np.logical_and(\n","                    initial_index <= episode_borders, episode_borders < final_index\n","                ).any()  # 論理積\n","            sampled_indexes += list(range(initial_index, final_index + 1))\n","\n","        sampled_observations = self.observations[sampled_indexes].reshape(\n","            batch_size, chunk_length, *self.observations.shape[1:]\n","        )\n","        sampled_actions = self.actions[sampled_indexes].reshape(\n","            batch_size, chunk_length, self.actions.shape[1]\n","        )\n","        sampled_rewards = self.rewards[sampled_indexes].reshape(\n","            batch_size, chunk_length, 1\n","        )\n","        sampled_done = self.done[sampled_indexes].reshape(batch_size, chunk_length, 1)\n","        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        バッファに貯められている経験の数を返すメソッド．\n","\n","        Returns\n","        -------\n","        length : int\n","            バッファに貯められている経験の数．\n","        \"\"\"\n","        return self.capacity if self.is_filled else self.index"]},{"cell_type":"markdown","metadata":{"id":"7IeElUxMoV2u"},"source":["#### 観測の前処理を行う関数\n","ラッパーとして最初から適用してしまわないのは，リプレイバッファにはより容量の小さなnp．uint8の形式で保存しておきたいためです．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YwMN-_OOoV2u"},"outputs":[],"source":["def preprocess_obs(obs: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    画像を正規化する．[0, 255] -> [-0.5, 0.5]．\n","\n","    Parameters\n","    ----------\n","    obs : np.ndarray (64, 64, 3) or (chank length, batch size, 64, 64, 3)\n","        環境から得られた観測．画素値は[0, 255]．\n","\n","    Returns\n","    -------\n","    normalized_obs : np.ndarray (64, 64, 3) or (chank length, batch size, 64, 64, 3)\n","        画素値を[-0.5, 0.5]で正規化した観測．\n","    \"\"\"\n","    obs = obs.astype(np.float32)\n","    normalized_obs = obs / 255.0 - 0.5\n","    return normalized_obs"]},{"cell_type":"markdown","metadata":{"id":"nOqUE9TCoV2v"},"source":["#### λ-returnを計算する関数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SW_a1HQsoV2v"},"outputs":[],"source":["def lambda_target(\n","    rewards: torch.Tensor, values: torch.Tensor, gamma: float, lambda_: float\n",") -> torch.Tensor:\n","    \"\"\"\n","    価値関数の学習のためのλ-returnを計算する関数．\n","\n","    Parameters\n","    ----------\n","    rewards : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n","        報酬モデルによる報酬の推定値．\n","    values : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n","        価値関数を近似するValueモデルによる状態価値観数の推定値．\n","    gamma : float\n","        割引率．\n","    lambda_ : float\n","        λ-returnのパラメータλ．\n","\n","    V_lambda : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n","        各状態に対するλ-returnの値．\n","    \"\"\"\n","    V_lambda = torch.zeros_like(rewards, device=rewards.device)\n","\n","    H = rewards.shape[0] - 1\n","    V_n = torch.zeros_like(rewards, device=rewards.device)\n","    V_n[H] = values[H]\n","    for n in range(1, H + 1):\n","        # まずn-step returnを計算します\n","        # 注意: 系列が途中で終わってしまったら，可能な中で最大のnを用いたn-stepを使います\n","        V_n[:-n] = (gamma**n) * values[n:]\n","        for k in range(1, n + 1):\n","            if k == n:\n","                V_n[:-n] += (gamma ** (n - 1)) * rewards[k:]\n","            else:\n","                V_n[:-n] += (gamma ** (k - 1)) * rewards[k : -n + k]\n","\n","        # lambda_でn-step returnを重みづけてλ-returnを計算します\n","        if n == H:\n","            V_lambda += (lambda_ ** (H - 1)) * V_n\n","        else:\n","            V_lambda += (1 - lambda_) * (lambda_ ** (n - 1)) * V_n\n","\n","    return V_lambda"]},{"cell_type":"markdown","metadata":{"id":"rhNpM4fRoV2v"},"source":["## ここからはモデルの実装編"]},{"cell_type":"markdown","metadata":{"id":"CkMC4L_coV2v"},"source":["### S4Block (p26 Figure21参照)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PePcmo4koV2v"},"outputs":[],"source":["class S4Block(nn.Module):\n","\n","    def __init__(\n","        self,\n","        d_model=256,\n","        d_mlp = 512,\n","        n_layers=2,\n","        dropout=0.2,\n","        prenorm=True,\n","        lr=0.001,\n","        dropout_fn=nn.Dropout\n","    ):\n","        super(S4Block, self).__init__()\n","\n","        self.prenorm = prenorm\n","\n","        # Stack S4 layers as residual blocks\n","        self.norms = nn.ModuleList()\n","        self.s4_layers = nn.ModuleList()\n","        self.dropouts = nn.ModuleList()\n","        for _ in range(n_layers):\n","            self.norms.append(nn.LayerNorm(d_model))\n","            self.s4_layers.append(\n","                S4D(d_model, dropout=dropout, transposed=True, lr=min(0.001, lr))\n","            )\n","            self.dropouts.append(dropout_fn(dropout))\n","\n","        self.norm_mlp = nn.Sequential(\n","            nn.LayerNorm(d_model),\n","            nn.Linear(d_model, d_mlp),\n","            nn.GELU(),\n","            dropout_fn(dropout),\n","            nn.Linear(d_mlp, d_model),\n","            dropout_fn(dropout))\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Input x is shape (B, L, d_model), L is the length of continuous observations, B is the batch size\n","        \"\"\"\n","        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n","        for norm, s4, dropout in \\\n","            zip(self.norms, self.s4_layers, self.dropouts):\n","            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n","\n","            z = x #z.shape=torch.Size([8, 512, 100])\n","            if self.prenorm:\n","                # Prenorm\n","                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n","\n","            # Apply S4 block: we ignore the state input and output\n","            z, _ = s4(z)\n","\n","            # Dropout on the output of the S4 block\n","            z = dropout(z)\n","\n","            # Residual connection\n","            x = z + x\n","\n","            if not self.prenorm:\n","                # Postnorm\n","                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n","\n","        x = x.transpose(-1, -2)  # (B, d_model, L) -> (B, L, d_model)\n","\n","        #TODO: x_にも操作が反映されてたりしないか確認する. residual connectionのため\n","        x_ = x\n","        x = x_ + self.norm_mlp(x)\n","\n","        return x\n","\n","    def imagine(self, x, S4hidden):\n","        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n","        # assert x.size(1) == 1 and S4hidden.size(1) == 1 d_model 1になるか？要確認\n","        for norm, s4, dropout in \\\n","            zip(self.norms, self.s4_layers, self.dropouts):\n","            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n","\n","            z = x\n","            if self.prenorm:\n","                # Prenorm\n","                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n","\n","            # print(\"z(B, H(d_model), L)\", z.shape)\n","            # print(\"S4hidden(B, H(d_model), L)\", S4hidden.shape)\n","\n","            # Apply S4 block: y, x_kを出力\n","            # A_bar.shape, A_bar.dtype=torch.Size([512, 32]) torch.complex64\n","            # B_bar.shape, B_bar.dtype=torch.Size([512, 32]) torch.complex64\n","            # C.shape=torch.Size([512, 32])\n","            # D.shape=torch.Size([512])\n","            z, S4hidden_next = s4.generate(z, S4hidden)\n","\n","            # Dropout on the output of the S4 block\n","            z = dropout(z)\n","\n","            # Residual connection\n","            x = z + x\n","\n","            if not self.prenorm:\n","                # Postnorm\n","                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n","\n","        x = x.transpose(-1, -2)  # (B, d_model, L) -> (B, L, d_model)\n","\n","        #TODO: x_にも操作が反映されてたりしないか確認する. residual connectionのため\n","        x_ = x\n","        x = x_ + self.norm_mlp(x)\n","\n","        return x, S4hidden_next"]},{"cell_type":"markdown","metadata":{"id":"N6kDdlwHoV2w"},"source":["### HistoryEncoder\n","HistoryEncoderはPriorに当たる<br>\n","(次元を表す変数が\"*\\_dim\"だったり\"d\\_\\*\"だったりして紛らわしかもしれません)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-kI32yUoV2w"},"outputs":[],"source":["class HistoryEncoder(nn.Module):\n","\n","    def __init__(\n","        self,\n","\n","        z_dim, # z_dim=1024にする予定. Encoderからの出力zの次元と合わせる必要がある\n","        action_dim,\n","        gMLP_dim=512,\n","\n","        history_dim=512,\n","        S4_mlp_dim=512,\n","        S4_n_layers=8,\n","        dropout=0.2,\n","        prenorm=True,\n","\n","        zMLP_dim=512,\n","        class_size=32,\n","        category_size=32,\n","        lr=0.001,\n","    ):\n","        super(HistoryEncoder, self).__init__()\n","        self.class_size = class_size\n","        self.category_size = category_size\n","        self.gMLP = nn.Sequential(\n","            nn.Linear(z_dim + action_dim, gMLP_dim),\n","            nn.ReLU(),\n","            nn.Linear(gMLP_dim, history_dim)\n","        )\n","\n","        self.S4s = nn.ModuleList(\n","            [S4Block(d_model=history_dim, d_mlp=S4_mlp_dim, n_layers=2, dropout=dropout, prenorm=prenorm, lr=lr) for _ in range(S4_n_layers)]\n","            )\n","\n","        self.zMLP = nn.Sequential(\n","            nn.Linear(history_dim, zMLP_dim),\n","            nn.ReLU(),\n","            nn.Linear(zMLP_dim, z_dim)\n","        )\n","\n","    def forward(self, z, action):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        z : torch.Tensor (batch_size, L, z_dim)\n","            環境から得られた観測画像の潜在表現. この時点ではone-hot vectorである\n","\n","        action : torch.Tensor (batch_size, L, action_dim)\n","\n","        Returns\n","        ----------\n","        h : torch.Tensor (batch_size, L, history_dim)\n","            観測画像を埋め込み、カテゴリカル分布からサンプルしたもの(この時点ではone-hot vector)\n","            勾配を通してあるのはreward lossからの勾配を計算するため\n","\n","        z : torch.Tensor (batch_size, L, z_dim)\n","            次の環境の観測画像の潜在表現. この時点ではone-hot vectorである\n","\n","        dist: torch.distribution\n","            zの分布.ELBOのKL-divergenceを計算するために必要. (B, L-1, category_size, class_size)の形の分布となる\n","\n","        dist_no_grad: torch.distribution\n","            zの分布.ELBOのKL-divergenceを計算するために必要\n","        \"\"\"\n","        g = self.gMLP(torch.cat([z, action], dim=-1))\n","        for S4 in self.S4s:\n","            g = S4(g) # 最終的にhが出力される\n","        h = g\n","        logits = self.zMLP(h).reshape(*h.shape[:-1], self.category_size, self.class_size) # (batch_size, L, z_dim) -> (batch_size, L, category_dim, class_dim)\n","        probs = torch.softmax(logits, dim=-1) * 0.99 + (0.01 / self.class_size)\n","        one_hot_dist = torch.distributions.OneHotCategorical(probs=probs)\n","        if z.size(1) > 1:\n","            dist = torch.distributions.OneHotCategorical(probs=probs[:, :-1, :, :])\n","            dist_no_grad = torch.distributions.OneHotCategorical(probs=probs[:, :-1, :, :].detach())\n","        else:\n","            dist, dist_no_grad = None, None\n","        stoch = one_hot_dist.sample()\n","        stoch += probs - probs.detach() # using \"straight-through gradients\"\n","        z = torch.flatten(stoch, start_dim=-2, end_dim=-1)\n","\n","        return h, z, dist, dist_no_grad\n","\n","\n","    def imagine(self, z, action, S4_hiddens=None):\n","        # S4_hiddens is list of S4_hidden\n","        g = self.gMLP(torch.cat([z, action], dim=-1))\n","        if S4_hiddens is None:\n","            S4_hidden = torch.zeros(512, 32, dtype=torch.complex64)  #初期隠れ状態として1×1×H×N//2のゼロ行列を作る（history_dim×64//2）\n","            S4_hidden = S4_hidden.reshape(1, 1, 1, *S4_hidden.shape) #リストから取り出すと0次元目が消えるので\n","            S4_hiddens = list(S4_hidden.to(device))\n","        S4_hiddens_next = []\n","        for S4, S4_hidden in zip(self.S4s, S4_hiddens):\n","            g, S4_hidden_next = S4.imagine(g, S4_hidden) # 最終的にhが出力される S4_hiddenがx_k-1\n","            S4_hiddens_next.append(S4_hidden_next)\n","        h = g\n","        logits = self.zMLP(h).reshape(*h.shape[:-1], self.category_size, self.class_size) # (batch_size, L, z_dim) -> (batch_size, L, category_dim, class_dim)\n","        probs = torch.softmax(logits, dim=-1) * 0.99 + (0.01 / self.class_size)\n","        one_hot_dist = torch.distributions.OneHotCategorical(probs=probs)\n","        stoch = one_hot_dist.sample()\n","        stoch += probs - probs.detach() # using \"straight-through gradients\"\n","        z = torch.flatten(stoch, start_dim=-2, end_dim=-1)\n","\n","        return h, z, S4_hiddens_next\n","\n","    def deter(self, z, action):\n","        with torch.no_grad():\n","            g = self.gMLP(torch.cat([z, action], dim=-1))\n","            for S4 in self.S4s:\n","                g = self.S4(g) # 最終的にhが出力される\n","            h = g\n","            logits = self.zMLP(h).reshape(*h.shape[:-1], self.category_size, self.class_size) # (batch_size, L, z_dim) -> (batch_size, L, category_dim, class_dim)\n","\n","        max_indices = torch.argmax(logits, dim=-1)\n","        one_hot = F.one_hot(max_indices, num_classes=logits.size(-1))\n","        one_hot = one_hot.reshape(*one_hot.shape[:1], -1)\n","        assert one_hot.size(-1) == self.category_size * self.class_size\n","        return one_hot"]},{"cell_type":"markdown","metadata":{"id":"DiIAlKWEoV2w"},"source":["### Encoder, Decoder\n","EncoderはPosteriorに当たる<br>\n","(Decodeで画像にする必要はあるか？評価する上では画像にする必要はありそうだけど実際にモデルとしては軽いほうがいい)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2oZJZuF1oV2x"},"outputs":[],"source":["class Encoder(nn.Module):\n","    \"\"\"\n","    (input_dim, 64, 64)の画像を(1024,)のベクトルに変換する\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        input_dim=3, # grayscaleなら1\n","        category_size=32,\n","        class_size=32,\n","    ):\n","        super(Encoder, self).__init__()\n","        self.input_dim = input_dim\n","        self.cv1 = nn.Conv2d(input_dim, 32, kernel_size=4, stride=2) # (input_dim, 64, 64) -> (32, 31, 31)\n","        self.cv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2) # (32, 31, 31) -> (64, 14, 14)\n","        self.cv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2) # (64, 14, 14) -> (128, 6, 6)\n","        self.cv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2) # (128, 6, 6) -> (256, 2, 2)\n","        self.category_size = category_size\n","        self.class_size = class_size\n","\n","    def forward(self, obs):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        obs : torch.Tensor (batch_size, L, input_dim, 64, 64), Lは連続した観測画像の系列長\n","            環境から得られた観測画像\n","\n","        Returns\n","        ----------\n","        z : torch.Tensor (batch_size, L, z_dim)\n","            観測画像を埋め込み、カテゴリカル分布からサンプルしたもの(この時点ではone-hot vector)\n","            勾配を通してあるのはreward lossからの勾配を計算するため\n","\n","        dist: torch.distribution\n","            zの分布.ELBOのKL-divergenceを計算するために必要. (B, L-1, category_size, class_size)の形の分布となる\n","\n","        dist_no_grad: torch.distribution\n","            zの分布.ELBOのKL-divergenceを計算するために必要\n","        \"\"\"\n","        B, L = obs.shape[:2]\n","        hidden = F.silu(self.cv1(obs.reshape(B*L, *obs.shape[2:])))\n","        hidden = F.silu(self.cv2(hidden))\n","        hidden = F.silu(self.cv3(hidden))\n","        hidden = F.silu(self.cv4(hidden))\n","        # チャネルのハイパラがあっているかチェック\n","        assert hidden.size(-3) * hidden.size(-2) * hidden.size(-1) == self.category_size * self.class_size\n","        logits = hidden.reshape(B, L, self.category_size, self.class_size) # (B*L, 256, 2, 2) -> (B, L, category_size, class_size)\n","        probs = torch.softmax(logits, dim=-1) * 0.99 + (0.01 / self.class_size)\n","        one_hot_dist = torch.distributions.OneHotCategorical(probs=probs)\n","        if L > 1:\n","            dist = torch.distributions.OneHotCategorical(probs=probs[:, 1:, :, :])\n","            dist_no_grad = torch.distributions.OneHotCategorical(probs=probs[:, 1:, :, :].detach())\n","        else:\n","            dist, dist_no_grad = None, None\n","        stoch = one_hot_dist.sample()\n","        stoch += probs - probs.detach() # using \"straight-through gradients\"\n","        z = torch.flatten(stoch, start_dim=-2, end_dim=-1)\n","        assert z.size(-1) == self.category_size * self.class_size\n","\n","        return z, dist, dist_no_grad\n","\n","\n","\n","class Decoder(nn.Module):\n","    \"\"\"\n","    (z_dim + hidtory_dim,)のベクトルを(input_dim, 64, 64)の画像に変換する\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        output_dim=3, # grayscaleなら1\n","        z_dim=1024,\n","        history_dim=1024,\n","    ):\n","        super(Decoder, self).__init__()\n","        self.output_dim = output_dim\n","        self.fc = nn.Linear(z_dim + history_dim, 1024)\n","        self.cv1 = nn.ConvTranspose2d(1024, 128, kernel_size=5, stride=2) # (1024, 1, 1) -> (128, 5, 5)\n","        self.cv2 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2) # (128, 5, 5) -> (64, 13, 13)\n","        self.cv3 = nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2) # (64, 13, 13) -> (32, 30, 30)\n","        self.cv4 = nn.ConvTranspose2d(32, output_dim, kernel_size=6, stride=2) # (32, 30, 30) -> (input_dim, 64, 64)\n","\n","    def forward(self, h, z):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        h: torch.Tensor (batch_size, L, history_dim)\n","            これまでの履歴(S4Blockからの出力)\n","\n","        z : torch.Tensor (batch_size, L, z_dim)\n","            次の観測の潜在表現\n","\n","        Returns\n","        ----------\n","        obs : torch.Tensor (batch_size, L, output_dim, 64, 64)\n","            次の観測画像\n","        \"\"\"\n","        B, L = h.shape[:2]\n","        hidden = self.fc(torch.cat([z, h], dim=-1))\n","        hidden = hidden.reshape(B*L, 1024, 1, 1)\n","        hidden = F.silu(self.cv1(hidden))\n","        hidden = F.silu(self.cv2(hidden))\n","        hidden = F.silu(self.cv3(hidden))\n","        obs = self.cv4(hidden)\n","\n","        return obs.reshape(B, L, *obs.shape[-3:])\n"]},{"cell_type":"markdown","metadata":{"id":"S5WIpq6xoV2x"},"source":["### RewardModel\n","報酬モデル. 1層のMLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDVB0EnToV2x"},"outputs":[],"source":["class RewardModel(nn.Module):\n","\n","    def __init__(\n","        self,\n","        history_dim,\n","        z_dim,\n","        mlp_dim=512,\n","    ):\n","        super(RewardModel, self).__init__()\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(history_dim + z_dim, mlp_dim),\n","            nn.SiLU(),\n","            nn.Linear(mlp_dim, 1)\n","        )\n","\n","    def forward(self, h, z):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        h: torch.Tensor (batch_size, L, history_dim)\n","            これまでの履歴\n","\n","        z : torch.Tensor (batch_size, L, z_dim)\n","            次の観測の潜在表現\n","\n","        Returns\n","        ----------\n","        reward : torch.Tensor (batch_size, L, 1)\n","            報酬の予測値\n","        \"\"\"\n","        reward = self.fc1(torch.cat([h, z], dim=-1))\n","\n","        return reward\n"]},{"cell_type":"markdown","metadata":{"id":"nEOr8I4eoV2x"},"source":["### PolicyModel\n","まだ実装しない（世界モデルのみをテストしてから）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VL6_IGiMoV2x"},"outputs":[],"source":["class PolicyModel(nn.Module):\n","    def __init__(\n","            self,\n","            z_dim,\n","            history_dim,\n","            action_dim,\n","            mlp_dim=512,\n","            min_std = 0.01\n","        ):\n","        super(PolicyModel, self).__init__()\n","        self.min_std = min_std\n","\n","        self.fc1 = nn.Linear(history_dim + z_dim, mlp_dim)\n","        self.fc2 = nn.Linear(mlp_dim, mlp_dim)\n","        self.fc3 = nn.Linear(mlp_dim, mlp_dim)\n","        self.fc4 = nn.Linear(mlp_dim, mlp_dim)\n","        self.fc_mean = nn.Linear(mlp_dim, action_dim)\n","        self.fc_stddev = nn.Linear(mlp_dim, action_dim)\n","\n","    def forward(self, h, z):\n","        hidden = F.silu(self.fc1(torch.cat([h, z], dim=-1)))\n","        hidden = F.silu(self.fc2(hidden))\n","        hidden = F.silu(self.fc3(hidden))\n","        hidden = F.silu(self.fc4(hidden))\n","        mean = torch.tanh(self.fc_mean(hidden))\n","        std = self.fc_stddev(hidden)\n","        std = F.softplus(std) + self.min_std\n","\n","        eps = (torch.rand_like(mean) - 0.5) * 2\n","        action = mean + eps * std\n","\n","        return action\n","\n","    def deter(self, h, z):\n","        hidden = F.silu(self.fc1(torch.cat([h, z], dim=-1)))\n","        hidden = F.silu(self.fc2(hidden))\n","        hidden = F.silu(self.fc3(hidden))\n","        hidden = F.silu(self.fc4(hidden))\n","        mean = torch.tanh(self.fc_mean(hidden))\n","\n","        return mean"]},{"cell_type":"markdown","metadata":{"id":"MwQvbtB8oV2y"},"source":["### ValueModel\n","まだ実装しない（世界モデルのみをテストしてから）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HM8MrWJ_oV2y"},"outputs":[],"source":["class ValueModel(nn.Module):\n","    def __init__(self, z_dim, history_dim, mlp_dim=512):\n","        super(ValueModel, self).__init__()\n","\n","        self.fc1 = nn.Linear(history_dim + z_dim, mlp_dim)\n","        self.fc2 = nn.Linear(mlp_dim, mlp_dim)\n","        self.fc3 = nn.Linear(mlp_dim, mlp_dim)\n","        self.fc4 = nn.Linear(mlp_dim, 1)\n","\n","    def forward(self, h, z):\n","        hidden = F.silu(self.fc1(torch.cat([h, z], dim=-1)))\n","        hidden = F.silu(self.fc2(hidden))\n","        hidden = F.silu(self.fc3(hidden))\n","        value = self.fc4(hidden)\n","        return value"]},{"cell_type":"markdown","metadata":{"id":"O_HtwhDnoV2y"},"source":["## 学習の実装編"]},{"cell_type":"markdown","metadata":{"id":"eokVbZFzoV2z"},"source":["### 学習の実装（世界モデルのみテストする）"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118183,"status":"ok","timestamp":1737339997476,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"},"user_tz":-540},"id":"We9vag6OoV2z","outputId":"732209cd-ee27-470d-d9ae-9d4863485c87","scrolled":true},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(done, (bool, np.bool8)):\n"]},{"output_type":"execute_result","data":{"text/plain":["3396"]},"metadata":{},"execution_count":18}],"source":["#最初の数エピソードはランダムに行動して経験をリプレイバッファに集める\n","env = make_env()\n","buffer_capacity = 200000\n","replay_buffer = ReplayBuffer(capacity=buffer_capacity, observation_shape=env.observation_space.shape, action_dim=env.action_space.shape[0])\n","seed_episode=50\n","for _ in range(seed_episode):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","        action = env.action_space.sample()\n","        next_obs, reward, done, _ = env.step(action)\n","        replay_buffer.push(obs, action, reward, done)\n","        obs = next_obs\n","del env\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1737339997476,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"},"user_tz":-540},"id":"j8xpj05yoV2z","outputId":"26f3a9b1-aa2f-49f6-9eda-a4ea78866b3c","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/workspace/assets'\n","/content/drive/MyDrive/Colab Notebooks/WorldModel2024/最終課題/S4Dmodification\n"]}],"source":["# 可視化のためにTensorBoardを用いるので，Colab上でTensorBoardを表示するための宣言を行う\n","%cd /workspace/assets\n","#学習結果を確認するためにTensorBoardを立ち上げておく\n","log_dir = \"logs\"\n","writer = SummaryWriter(log_dir)"]},{"cell_type":"markdown","metadata":{"id":"goRLmViLoV20"},"source":["#### オプティマイザのセットアップ関数\n","S4は重み減衰から外します"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5raauUt_oV20"},"outputs":[],"source":["from timm.scheduler.cosine_lr import CosineLRScheduler\n","def setup_optimizer(model_params, lr, weight_decay, epochs):\n","    \"\"\"\n","    S4 requires a specific optimizer setup.\n","\n","    The S4 layer (A, B, C, dt) parameters typically\n","    require a smaller learning rate (typically 0.001), with no weight decay.\n","\n","    The rest of the model can be trained with a higher learning rate (e.g. 0.004, 0.01)\n","    and weight decay (if desired).\n","    \"\"\"\n","\n","    # General parameters don't contain the special _optim key\n","    params = [p for p in model_params if not hasattr(p, \"_optim\")]\n","\n","    # Create an optimizer with the general parameters\n","    optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n","\n","    # Add parameters with special hyperparameters\n","    hps = [getattr(p, \"_optim\") for p in model_params if hasattr(p, \"_optim\")]\n","    hps = [\n","        dict(s) for s in sorted(list(dict.fromkeys(frozenset(hp.items()) for hp in hps)))\n","    ]  # Unique dicts\n","    for hp in hps:\n","        params = [p for p in model_params if getattr(p, \"_optim\", None) == hp]\n","        optimizer.add_param_group(\n","            {\"params\": params, **hp}\n","        )\n","\n","    # Create a lr scheduler\n","    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.2)\n","    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs+20)\n","    # We use a linear warmup (1000 gradient steps) and cosine anneal learning rate schedule for S4WM.なので\n","    scheduler = CosineLRScheduler(optimizer, t_initial=1000, lr_min=1e-3, warmup_t=1000, warmup_lr_init=1e-4, warmup_prefix=True)\n","\n","    # Print optimizer info\n","    keys = sorted(set([k for hp in hps for k in hp.keys()]))\n","    for i, g in enumerate(optimizer.param_groups):\n","        group_hps = {k: g.get(k, None) for k in keys}\n","        print(' | '.join([\n","            f\"Optimizer group {i}\",\n","            f\"{len(g['params'])} tensors\",\n","        ] + [f\"{k} {v}\" for k, v in group_hps.items()]))\n","\n","    return optimizer, scheduler\n"]},{"cell_type":"markdown","metadata":{"id":"7F-ULMv1RnF6"},"source":["#### 学習ループ"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":477988,"status":"ok","timestamp":1737340475459,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"},"user_tz":-540},"id":"8IK7Pp3ooV20","outputId":"39aa966d-e2fe-4bc3-e29d-82110184ca5b","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n","Episode: 0\n","    Loss: 2345.716796875\n","    reconstruction loss: 2048.662841796875\n","    reward loss: 7.968855381011963\n","    kl loss: 1.8143948316574097\n","Episode: 1\n","    Loss: 2227.166015625\n","    reconstruction loss: 2029.905517578125\n","    reward loss: 5.117558002471924\n","    kl loss: 1.814598798751831\n","Episode: 2\n","    Loss: 2178.741455078125\n","    reconstruction loss: 2025.3018798828125\n","    reward loss: 3.8259716033935547\n","    kl loss: 1.9530590772628784\n","Episode: 3\n","    Loss: 2218.813720703125\n","    reconstruction loss: 2055.21923828125\n","    reward loss: 4.198847770690918\n","    kl loss: 1.6634799242019653\n","Episode: 4\n","    Loss: 2189.67138671875\n","    reconstruction loss: 2034.69921875\n","    reward loss: 4.022053241729736\n","    kl loss: 1.420028567314148\n","Episode: 5\n","    Loss: 2140.315185546875\n","    reconstruction loss: 2006.642333984375\n","    reward loss: 3.472414493560791\n","    kl loss: 1.2138530015945435\n","Episode: 6\n","    Loss: 2111.2119140625\n","    reconstruction loss: 1986.5731201171875\n","    reward loss: 3.251542806625366\n","    kl loss: 1.0834615230560303\n","Episode: 7\n","    Loss: 2098.2705078125\n","    reconstruction loss: 1987.6064453125\n","    reward loss: 2.8638930320739746\n","    kl loss: 1.04277503490448\n","Episode: 8\n","    Loss: 2098.118408203125\n","    reconstruction loss: 1978.265869140625\n","    reward loss: 3.1355693340301514\n","    kl loss: 1.0107771158218384\n","Episode: 9\n","    Loss: 2055.9033203125\n","    reconstruction loss: 1904.06103515625\n","    reward loss: 4.0517258644104\n","    kl loss: 1.0031921863555908\n","Episode: 10\n","    Loss: 1995.385009765625\n","    reconstruction loss: 1829.5166015625\n","    reward loss: 4.452207088470459\n","    kl loss: 1.0041115283966064\n","Episode: 11\n","    Loss: 1895.4366455078125\n","    reconstruction loss: 1753.413330078125\n","    reward loss: 3.76879620552063\n","    kl loss: 1.0115426778793335\n","Episode: 12\n","    Loss: 1810.81640625\n","    reconstruction loss: 1663.6273193359375\n","    reward loss: 3.9119677543640137\n","    kl loss: 1.0270297527313232\n","Episode: 13\n","    Loss: 1638.2493896484375\n","    reconstruction loss: 1501.829345703125\n","    reward loss: 3.58370304107666\n","    kl loss: 1.0990511178970337\n","Episode: 14\n","    Loss: 1617.3201904296875\n","    reconstruction loss: 1469.5921630859375\n","    reward loss: 3.8959693908691406\n","    kl loss: 1.1369129419326782\n","Episode: 15\n","    Loss: 1588.431884765625\n","    reconstruction loss: 1453.9091796875\n","    reward loss: 3.51735520362854\n","    kl loss: 1.1415326595306396\n","Episode: 16\n","    Loss: 1483.1097412109375\n","    reconstruction loss: 1342.4637451171875\n","    reward loss: 3.7125489711761475\n","    kl loss: 1.0706833600997925\n","Episode: 17\n","    Loss: 1405.0506591796875\n","    reconstruction loss: 1283.1439208984375\n","    reward loss: 3.194216251373291\n","    kl loss: 1.0109140872955322\n","Episode: 18\n","    Loss: 1326.7799072265625\n","    reconstruction loss: 1171.497314453125\n","    reward loss: 4.1507792472839355\n","    kl loss: 1.0005431175231934\n","Episode: 19\n","    Loss: 1206.236083984375\n","    reconstruction loss: 1068.554443359375\n","    reward loss: 3.648045539855957\n","    kl loss: 1.0\n","Episode: 20\n","    Loss: 1192.86376953125\n","    reconstruction loss: 1046.5997314453125\n","    reward loss: 3.8932583332061768\n","    kl loss: 1.0\n","Episode: 21\n","    Loss: 1127.15771484375\n","    reconstruction loss: 997.6884155273438\n","    reward loss: 3.413381576538086\n","    kl loss: 1.0001035928726196\n","Episode: 22\n","    Loss: 1076.4359130859375\n","    reconstruction loss: 933.8190307617188\n","    reward loss: 3.7890543937683105\n","    kl loss: 1.0\n","Episode: 23\n","    Loss: 1043.0059814453125\n","    reconstruction loss: 879.5199584960938\n","    reward loss: 4.385294437408447\n","    kl loss: 1.0000709295272827\n","Episode: 24\n","    Loss: 972.4509887695312\n","    reconstruction loss: 831.315673828125\n","    reward loss: 3.746722936630249\n","    kl loss: 1.0\n","Episode: 25\n","    Loss: 940.1858520507812\n","    reconstruction loss: 815.6035766601562\n","    reward loss: 3.273693561553955\n","    kl loss: 1.0003011226654053\n","Episode: 26\n","    Loss: 902.8099365234375\n","    reconstruction loss: 773.450927734375\n","    reward loss: 3.4102087020874023\n","    kl loss: 1.000168800354004\n","Episode: 27\n","    Loss: 878.4983520507812\n","    reconstruction loss: 758.84033203125\n","    reward loss: 3.1330864429473877\n","    kl loss: 1.0000015497207642\n","Episode: 28\n","    Loss: 833.8836059570312\n","    reconstruction loss: 701.8907470703125\n","    reward loss: 3.485502243041992\n","    kl loss: 1.0000319480895996\n","Episode: 29\n","    Loss: 839.6506958007812\n","    reconstruction loss: 695.8482055664062\n","    reward loss: 3.822815418243408\n","    kl loss: 1.0003966093063354\n","Episode: 30\n","    Loss: 781.1547241210938\n","    reconstruction loss: 660.771728515625\n","    reward loss: 3.1536989212036133\n","    kl loss: 1.000352144241333\n","Episode: 31\n","    Loss: 764.453857421875\n","    reconstruction loss: 657.241455078125\n","    reward loss: 2.7773334980010986\n","    kl loss: 1.000573754310608\n","Episode: 32\n","    Loss: 745.2373657226562\n","    reconstruction loss: 610.0325317382812\n","    reward loss: 3.5772414207458496\n","    kl loss: 1.0001400709152222\n","Episode: 33\n","    Loss: 724.448486328125\n","    reconstruction loss: 595.234375\n","    reward loss: 3.4060850143432617\n","    kl loss: 1.0001153945922852\n","Episode: 34\n","    Loss: 695.1713256835938\n","    reconstruction loss: 557.1512451171875\n","    reward loss: 3.657649517059326\n","    kl loss: 1.0002367496490479\n","Episode: 35\n","    Loss: 693.57080078125\n","    reconstruction loss: 555.572021484375\n","    reward loss: 3.657076835632324\n","    kl loss: 1.0001112222671509\n","Episode: 36\n","    Loss: 668.3504638671875\n","    reconstruction loss: 525.0768432617188\n","    reward loss: 3.807811737060547\n","    kl loss: 1.0000263452529907\n","Episode: 37\n","    Loss: 626.871337890625\n","    reconstruction loss: 505.06842041015625\n","    reward loss: 3.1943697929382324\n","    kl loss: 1.0\n","Episode: 38\n","    Loss: 620.9457397460938\n","    reconstruction loss: 494.437744140625\n","    reward loss: 3.328782558441162\n","    kl loss: 1.0000628232955933\n","Episode: 39\n","    Loss: 604.4703979492188\n","    reconstruction loss: 469.3748779296875\n","    reward loss: 3.574152708053589\n","    kl loss: 1.000016212463379\n","Episode: 40\n","    Loss: 593.4073486328125\n","    reconstruction loss: 464.10052490234375\n","    reward loss: 3.408766508102417\n","    kl loss: 1.0\n","Episode: 41\n","    Loss: 590.8099365234375\n","    reconstruction loss: 445.4180603027344\n","    reward loss: 3.868340253829956\n","    kl loss: 1.0\n","Episode: 42\n","    Loss: 557.3589477539062\n","    reconstruction loss: 424.26251220703125\n","    reward loss: 3.5170347690582275\n","    kl loss: 1.000023603439331\n","Episode: 43\n","    Loss: 551.920166015625\n","    reconstruction loss: 427.98992919921875\n","    reward loss: 3.25514817237854\n","    kl loss: 1.000004529953003\n","Episode: 44\n","    Loss: 540.0194702148438\n","    reconstruction loss: 408.4650573730469\n","    reward loss: 3.4729831218719482\n","    kl loss: 1.0\n","Episode: 45\n","    Loss: 511.48101806640625\n","    reconstruction loss: 392.6212158203125\n","    reward loss: 3.1102795600891113\n","    kl loss: 1.0\n","Episode: 46\n","    Loss: 505.0035705566406\n","    reconstruction loss: 387.2567138671875\n","    reward loss: 3.078481435775757\n","    kl loss: 1.0\n","Episode: 47\n","    Loss: 504.29119873046875\n","    reconstruction loss: 368.6617126464844\n","    reward loss: 3.5894134044647217\n","    kl loss: 1.0\n","Episode: 48\n","    Loss: 489.9801330566406\n","    reconstruction loss: 359.31597900390625\n","    reward loss: 3.447547435760498\n","    kl loss: 1.0\n","Episode: 49\n","    Loss: 507.0494384765625\n","    reconstruction loss: 357.54022216796875\n","    reward loss: 3.9859774112701416\n","    kl loss: 1.0\n","Episode: 50\n","    Loss: 480.6400451660156\n","    reconstruction loss: 348.94097900390625\n","    reward loss: 3.477116107940674\n","    kl loss: 1.0\n","Episode: 51\n","    Loss: 480.5794982910156\n","    reconstruction loss: 340.7133483886719\n","    reward loss: 3.710461378097534\n","    kl loss: 1.0\n","Episode: 52\n","    Loss: 440.1416015625\n","    reconstruction loss: 333.09295654296875\n","    reward loss: 2.7728185653686523\n","    kl loss: 1.0\n","Episode: 53\n","    Loss: 459.9415283203125\n","    reconstruction loss: 322.3317565917969\n","    reward loss: 3.6455066204071045\n","    kl loss: 1.001704454421997\n","Episode: 54\n","    Loss: 481.03863525390625\n","    reconstruction loss: 318.45947265625\n","    reward loss: 4.359365463256836\n","    kl loss: 1.0001362562179565\n","Episode: 55\n","    Loss: 436.8923645019531\n","    reconstruction loss: 316.4185791015625\n","    reward loss: 3.1563937664031982\n","    kl loss: 1.0\n","Episode: 56\n","    Loss: 434.8538513183594\n","    reconstruction loss: 302.87506103515625\n","    reward loss: 3.4851086139678955\n","    kl loss: 1.0\n","Episode: 57\n","    Loss: 434.0399169921875\n","    reconstruction loss: 294.506591796875\n","    reward loss: 3.7009527683258057\n","    kl loss: 1.0\n","Episode: 58\n","    Loss: 422.54345703125\n","    reconstruction loss: 292.9919128417969\n","    reward loss: 3.415757894515991\n","    kl loss: 1.0\n","Episode: 59\n","    Loss: 427.5487060546875\n","    reconstruction loss: 291.8021240234375\n","    reward loss: 3.592759370803833\n","    kl loss: 1.0\n","Episode: 60\n","    Loss: 454.0200500488281\n","    reconstruction loss: 289.17913818359375\n","    reward loss: 4.424026012420654\n","    kl loss: 1.0\n","Episode: 61\n","    Loss: 408.03582763671875\n","    reconstruction loss: 284.7506103515625\n","    reward loss: 3.236720561981201\n","    kl loss: 1.0\n","Episode: 62\n","    Loss: 393.3668212890625\n","    reconstruction loss: 279.1532897949219\n","    reward loss: 2.977529764175415\n","    kl loss: 1.0\n","Episode: 63\n","    Loss: 429.7700500488281\n","    reconstruction loss: 274.96087646484375\n","    reward loss: 4.137404918670654\n","    kl loss: 1.0\n","Episode: 64\n","    Loss: 420.8931884765625\n","    reconstruction loss: 266.01043701171875\n","    reward loss: 4.139506816864014\n","    kl loss: 1.0\n","Episode: 65\n","    Loss: 399.4939270019531\n","    reconstruction loss: 264.8309326171875\n","    reward loss: 3.5617997646331787\n","    kl loss: 1.0\n","Episode: 66\n","    Loss: 394.3050231933594\n","    reconstruction loss: 261.7331848144531\n","    reward loss: 3.5020527839660645\n","    kl loss: 1.0\n","Episode: 67\n","    Loss: 395.98028564453125\n","    reconstruction loss: 258.220703125\n","    reward loss: 3.650273323059082\n","    kl loss: 1.0\n","Episode: 68\n","    Loss: 384.5760498046875\n","    reconstruction loss: 253.92135620117188\n","    reward loss: 3.447277069091797\n","    kl loss: 1.0\n","Episode: 69\n","    Loss: 364.528076171875\n","    reconstruction loss: 249.95162963867188\n","    reward loss: 2.987898826599121\n","    kl loss: 1.0\n","Episode: 70\n","    Loss: 365.51214599609375\n","    reconstruction loss: 248.14108276367188\n","    reward loss: 3.0677452087402344\n","    kl loss: 1.0\n","Episode: 71\n","    Loss: 375.27362060546875\n","    reconstruction loss: 243.7316436767578\n","    reward loss: 3.472628116607666\n","    kl loss: 1.0\n","Episode: 72\n","    Loss: 362.77899169921875\n","    reconstruction loss: 243.59947204589844\n","    reward loss: 3.1194143295288086\n","    kl loss: 1.0\n","Episode: 73\n","    Loss: 396.197998046875\n","    reconstruction loss: 251.16542053222656\n","    reward loss: 3.858074188232422\n","    kl loss: 1.0\n","Episode: 74\n","    Loss: 373.4867858886719\n","    reconstruction loss: 245.86734008789062\n","    reward loss: 3.36055588722229\n","    kl loss: 1.0\n","Episode: 75\n","    Loss: 394.30572509765625\n","    reconstruction loss: 241.60108947753906\n","    reward loss: 4.07727575302124\n","    kl loss: 1.0\n","Episode: 76\n","    Loss: 382.89337158203125\n","    reconstruction loss: 231.85992431640625\n","    reward loss: 4.029526710510254\n","    kl loss: 1.0\n","Episode: 77\n","    Loss: 353.2140197753906\n","    reconstruction loss: 235.13189697265625\n","    reward loss: 3.0880606174468994\n","    kl loss: 1.0\n","Episode: 78\n","    Loss: 389.6283874511719\n","    reconstruction loss: 235.85159301757812\n","    reward loss: 4.107908248901367\n","    kl loss: 1.0\n","Episode: 79\n","    Loss: 393.26641845703125\n","    reconstruction loss: 236.2107391357422\n","    reward loss: 4.2015910148620605\n","    kl loss: 1.0\n","Episode: 80\n","    Loss: 358.00030517578125\n","    reconstruction loss: 227.02755737304688\n","    reward loss: 3.456364393234253\n","    kl loss: 1.0\n","Episode: 81\n","    Loss: 349.73272705078125\n","    reconstruction loss: 224.0037078857422\n","    reward loss: 3.3065428733825684\n","    kl loss: 1.0\n","Episode: 82\n","    Loss: 330.7295227050781\n","    reconstruction loss: 218.9195556640625\n","    reward loss: 2.908856153488159\n","    kl loss: 1.0\n","Episode: 83\n","    Loss: 341.1921691894531\n","    reconstruction loss: 218.525146484375\n","    reward loss: 3.219057559967041\n","    kl loss: 1.0\n","Episode: 84\n","    Loss: 346.8411560058594\n","    reconstruction loss: 222.56829833984375\n","    reward loss: 3.2649385929107666\n","    kl loss: 1.0\n","Episode: 85\n","    Loss: 364.17852783203125\n","    reconstruction loss: 227.7091522216797\n","    reward loss: 3.6134111881256104\n","    kl loss: 1.0\n","Episode: 86\n","    Loss: 346.6224365234375\n","    reconstruction loss: 220.13238525390625\n","    reward loss: 3.328287363052368\n","    kl loss: 1.0\n","Episode: 87\n","    Loss: 344.6846618652344\n","    reconstruction loss: 214.58795166015625\n","    reward loss: 3.4313342571258545\n","    kl loss: 1.0\n","Episode: 88\n","    Loss: 351.950927734375\n","    reconstruction loss: 213.026123046875\n","    reward loss: 3.683566093444824\n","    kl loss: 1.0\n","Episode: 89\n","    Loss: 380.7330322265625\n","    reconstruction loss: 220.37384033203125\n","    reward loss: 4.295976638793945\n","    kl loss: 1.0\n","Episode: 90\n","    Loss: 347.9229736328125\n","    reconstruction loss: 214.32986450195312\n","    reward loss: 3.531231641769409\n","    kl loss: 1.0\n","Episode: 91\n","    Loss: 351.7423095703125\n","    reconstruction loss: 211.16050720214844\n","    reward loss: 3.730908155441284\n","    kl loss: 1.0\n","Episode: 92\n","    Loss: 351.12518310546875\n","    reconstruction loss: 210.99407958984375\n","    reward loss: 3.718031644821167\n","    kl loss: 1.0\n","Episode: 93\n","    Loss: 365.39111328125\n","    reconstruction loss: 217.51356506347656\n","    reward loss: 3.939358949661255\n","    kl loss: 1.0\n","Episode: 94\n","    Loss: 345.6851806640625\n","    reconstruction loss: 212.98028564453125\n","    reward loss: 3.505854368209839\n","    kl loss: 1.0\n","Episode: 95\n","    Loss: 349.0065612792969\n","    reconstruction loss: 213.96922302246094\n","    reward loss: 3.572495460510254\n","    kl loss: 1.0\n","Episode: 96\n","    Loss: 339.4867858886719\n","    reconstruction loss: 210.92440795898438\n","    reward loss: 3.38749623298645\n","    kl loss: 1.0\n","Episode: 97\n","    Loss: 329.9259948730469\n","    reconstruction loss: 204.338134765625\n","    reward loss: 3.3025102615356445\n","    kl loss: 1.0\n","Episode: 98\n","    Loss: 351.1065979003906\n","    reconstruction loss: 205.4749755859375\n","    reward loss: 3.8751890659332275\n","    kl loss: 1.0\n","Episode: 99\n","    Loss: 355.6031799316406\n","    reconstruction loss: 213.03970336914062\n","    reward loss: 3.7875280380249023\n","    kl loss: 1.0\n","Episode: 100\n","    Loss: 352.48419189453125\n","    reconstruction loss: 207.6654052734375\n","    reward loss: 3.8519656658172607\n","    kl loss: 1.0\n","Episode: 101\n","    Loss: 333.69598388671875\n","    reconstruction loss: 202.20535278320312\n","    reward loss: 3.471161127090454\n","    kl loss: 1.0\n","Episode: 102\n","    Loss: 318.45806884765625\n","    reconstruction loss: 203.64007568359375\n","    reward loss: 2.994799852371216\n","    kl loss: 1.0\n","Episode: 103\n","    Loss: 364.91888427734375\n","    reconstruction loss: 214.1939239501953\n","    reward loss: 4.020713806152344\n","    kl loss: 1.0\n","Episode: 104\n","    Loss: 355.82012939453125\n","    reconstruction loss: 208.58859252929688\n","    reward loss: 3.920901298522949\n","    kl loss: 1.0\n","Episode: 105\n","    Loss: 345.8858642578125\n","    reconstruction loss: 210.38644409179688\n","    reward loss: 3.585697650909424\n","    kl loss: 1.0\n","Episode: 106\n","    Loss: 341.3163757324219\n","    reconstruction loss: 207.2532958984375\n","    reward loss: 3.544659376144409\n","    kl loss: 1.0\n","Episode: 107\n","    Loss: 319.52734375\n","    reconstruction loss: 198.2122802734375\n","    reward loss: 3.1804308891296387\n","    kl loss: 1.0\n","Episode: 108\n","    Loss: 333.94207763671875\n","    reconstruction loss: 204.22959899902344\n","    reward loss: 3.420356273651123\n","    kl loss: 1.0\n","Episode: 109\n","    Loss: 346.6233215332031\n","    reconstruction loss: 206.7108917236328\n","    reward loss: 3.7117838859558105\n","    kl loss: 1.0\n","Episode: 110\n","    Loss: 327.97955322265625\n","    reconstruction loss: 198.53662109375\n","    reward loss: 3.4126548767089844\n","    kl loss: 1.0\n","Episode: 111\n","    Loss: 341.7349853515625\n","    reconstruction loss: 206.2908172607422\n","    reward loss: 3.584118604660034\n","    kl loss: 1.0\n","Episode: 112\n","    Loss: 316.2858581542969\n","    reconstruction loss: 202.93341064453125\n","    reward loss: 2.9529271125793457\n","    kl loss: 1.0\n","Episode: 113\n","    Loss: 338.59552001953125\n","    reconstruction loss: 200.82101440429688\n","    reward loss: 3.650700569152832\n","    kl loss: 1.0\n","Episode: 114\n","    Loss: 348.44390869140625\n","    reconstruction loss: 205.09449768066406\n","    reward loss: 3.809983015060425\n","    kl loss: 1.0\n","Episode: 115\n","    Loss: 329.13714599609375\n","    reconstruction loss: 194.93890380859375\n","    reward loss: 3.548521041870117\n","    kl loss: 1.0\n","Episode: 116\n","    Loss: 352.3062438964844\n","    reconstruction loss: 199.3677978515625\n","    reward loss: 4.083955764770508\n","    kl loss: 1.0\n","Episode: 117\n","    Loss: 339.5935363769531\n","    reconstruction loss: 193.91635131835938\n","    reward loss: 3.876491069793701\n","    kl loss: 1.0\n","Episode: 118\n","    Loss: 370.06414794921875\n","    reconstruction loss: 204.76853942871094\n","    reward loss: 4.437017917633057\n","    kl loss: 1.0\n","Episode: 119\n","    Loss: 332.91778564453125\n","    reconstruction loss: 194.5024871826172\n","    reward loss: 3.6690080165863037\n","    kl loss: 1.0\n","Episode: 120\n","    Loss: 319.7408447265625\n","    reconstruction loss: 192.85122680664062\n","    reward loss: 3.33970308303833\n","    kl loss: 1.0\n","Episode: 121\n","    Loss: 305.6396789550781\n","    reconstruction loss: 193.6227264404297\n","    reward loss: 2.9147703647613525\n","    kl loss: 1.0\n","Episode: 122\n","    Loss: 337.13897705078125\n","    reconstruction loss: 195.190673828125\n","    reward loss: 3.769951105117798\n","    kl loss: 1.0\n","Episode: 123\n","    Loss: 341.5530090332031\n","    reconstruction loss: 196.07427978515625\n","    reward loss: 3.8708207607269287\n","    kl loss: 1.0\n","Episode: 124\n","    Loss: 348.7760009765625\n","    reconstruction loss: 205.04080200195312\n","    reward loss: 3.82100510597229\n","    kl loss: 1.0\n","Episode: 125\n","    Loss: 338.81915283203125\n","    reconstruction loss: 196.419677734375\n","    reward loss: 3.7828423976898193\n","    kl loss: 1.0\n","Episode: 126\n","    Loss: 326.3819580078125\n","    reconstruction loss: 198.201904296875\n","    reward loss: 3.376572847366333\n","    kl loss: 1.0\n","Episode: 127\n","    Loss: 327.7928466796875\n","    reconstruction loss: 195.04029846191406\n","    reward loss: 3.5072154998779297\n","    kl loss: 1.0\n","Episode: 128\n","    Loss: 317.5675964355469\n","    reconstruction loss: 194.8690643310547\n","    reward loss: 3.2199578285217285\n","    kl loss: 1.0\n","Episode: 129\n","    Loss: 333.1256408691406\n","    reconstruction loss: 198.03866577148438\n","    reward loss: 3.57391357421875\n","    kl loss: 1.0\n","Episode: 130\n","    Loss: 340.5071716308594\n","    reconstruction loss: 197.52101135253906\n","    reward loss: 3.7996044158935547\n","    kl loss: 1.0\n","Episode: 131\n","    Loss: 312.69866943359375\n","    reconstruction loss: 196.3241424560547\n","    reward loss: 3.0392723083496094\n","    kl loss: 1.0\n","Episode: 132\n","    Loss: 337.7173156738281\n","    reconstruction loss: 191.308349609375\n","    reward loss: 3.8973991870880127\n","    kl loss: 1.0\n","Episode: 133\n","    Loss: 355.5254821777344\n","    reconstruction loss: 196.14480590820312\n","    reward loss: 4.268019199371338\n","    kl loss: 1.0\n","Episode: 134\n","    Loss: 307.2705993652344\n","    reconstruction loss: 198.184326171875\n","    reward loss: 2.831036329269409\n","    kl loss: 1.0\n","Episode: 135\n","    Loss: 333.65582275390625\n","    reconstruction loss: 191.90809631347656\n","    reward loss: 3.7642202377319336\n","    kl loss: 1.0\n","Episode: 136\n","    Loss: 339.1541748046875\n","    reconstruction loss: 195.92913818359375\n","    reward loss: 3.806429386138916\n","    kl loss: 1.0\n","Episode: 137\n","    Loss: 316.90606689453125\n","    reconstruction loss: 193.65963745117188\n","    reward loss: 3.235612154006958\n","    kl loss: 1.0\n","Episode: 138\n","    Loss: 332.4307556152344\n","    reconstruction loss: 190.6778564453125\n","    reward loss: 3.7643685340881348\n","    kl loss: 1.0\n","Episode: 139\n","    Loss: 336.2010192871094\n","    reconstruction loss: 194.3504180908203\n","    reward loss: 3.767159938812256\n","    kl loss: 1.0\n","Episode: 140\n","    Loss: 306.5790100097656\n","    reconstruction loss: 190.26303100585938\n","    reward loss: 3.0375990867614746\n","    kl loss: 1.0\n","Episode: 141\n","    Loss: 321.43359375\n","    reconstruction loss: 198.12652587890625\n","    reward loss: 3.237344264984131\n","    kl loss: 1.0\n","Episode: 142\n","    Loss: 330.1300048828125\n","    reconstruction loss: 189.6637420654297\n","    reward loss: 3.7276079654693604\n","    kl loss: 1.0\n","Episode: 143\n","    Loss: 335.69805908203125\n","    reconstruction loss: 198.15359497070312\n","    reward loss: 3.644127130508423\n","    kl loss: 1.0\n","Episode: 144\n","    Loss: 322.2493896484375\n","    reconstruction loss: 190.79840087890625\n","    reward loss: 3.4700284004211426\n","    kl loss: 1.0\n","Episode: 145\n","    Loss: 324.02655029296875\n","    reconstruction loss: 189.16156005859375\n","    reward loss: 3.5675716400146484\n","    kl loss: 1.0\n","Episode: 146\n","    Loss: 327.4308776855469\n","    reconstruction loss: 197.36888122558594\n","    reward loss: 3.43034291267395\n","    kl loss: 1.0\n","Episode: 147\n","    Loss: 327.4288635253906\n","    reconstruction loss: 190.579833984375\n","    reward loss: 3.624257802963257\n","    kl loss: 1.0\n","Episode: 148\n","    Loss: 347.28802490234375\n","    reconstruction loss: 196.10397338867188\n","    reward loss: 4.033830165863037\n","    kl loss: 1.0\n","Episode: 149\n","    Loss: 357.0022888183594\n","    reconstruction loss: 196.26998901367188\n","    reward loss: 4.306637287139893\n","    kl loss: 1.0\n","Episode: 150\n","    Loss: 336.8504638671875\n","    reconstruction loss: 193.3771209716797\n","    reward loss: 3.813523769378662\n","    kl loss: 1.0\n","Episode: 151\n","    Loss: 321.3779602050781\n","    reconstruction loss: 188.63702392578125\n","    reward loss: 3.5068840980529785\n","    kl loss: 1.0\n","Episode: 152\n","    Loss: 318.0076599121094\n","    reconstruction loss: 190.90304565429688\n","    reward loss: 3.345845937728882\n","    kl loss: 1.0\n","Episode: 153\n","    Loss: 333.22088623046875\n","    reconstruction loss: 196.91583251953125\n","    reward loss: 3.6087162494659424\n","    kl loss: 1.0\n","Episode: 154\n","    Loss: 297.70166015625\n","    reconstruction loss: 185.98541259765625\n","    reward loss: 2.9061779975891113\n","    kl loss: 1.0\n","Episode: 155\n","    Loss: 338.3368835449219\n","    reconstruction loss: 197.384765625\n","    reward loss: 3.7414891719818115\n","    kl loss: 1.0\n","Episode: 156\n","    Loss: 340.8456726074219\n","    reconstruction loss: 196.28448486328125\n","    reward loss: 3.8446052074432373\n","    kl loss: 1.0\n","Episode: 157\n","    Loss: 328.68145751953125\n","    reconstruction loss: 191.36807250976562\n","    reward loss: 3.6375248432159424\n","    kl loss: 1.0\n","Episode: 158\n","    Loss: 346.77294921875\n","    reconstruction loss: 195.56008911132812\n","    reward loss: 4.0346527099609375\n","    kl loss: 1.0\n","Episode: 159\n","    Loss: 335.11932373046875\n","    reconstruction loss: 186.0413818359375\n","    reward loss: 3.973655939102173\n","    kl loss: 1.0\n","Episode: 160\n","    Loss: 316.0269775390625\n","    reconstruction loss: 186.5630340576172\n","    reward loss: 3.413255453109741\n","    kl loss: 1.0\n","Episode: 161\n","    Loss: 343.07696533203125\n","    reconstruction loss: 190.9813232421875\n","    reward loss: 4.059875965118408\n","    kl loss: 1.0\n","Episode: 162\n","    Loss: 314.116943359375\n","    reconstruction loss: 191.7078857421875\n","    reward loss: 3.2116878032684326\n","    kl loss: 1.0\n","Episode: 163\n","    Loss: 353.81927490234375\n","    reconstruction loss: 188.9522705078125\n","    reward loss: 4.424771785736084\n","    kl loss: 1.0\n","Episode: 164\n","    Loss: 313.1954040527344\n","    reconstruction loss: 191.66200256347656\n","    reward loss: 3.186668872833252\n","    kl loss: 1.0\n","Episode: 165\n","    Loss: 323.30108642578125\n","    reconstruction loss: 187.85140991210938\n","    reward loss: 3.584275960922241\n","    kl loss: 1.0\n","Episode: 166\n","    Loss: 336.324462890625\n","    reconstruction loss: 198.31155395507812\n","    reward loss: 3.6575121879577637\n","    kl loss: 1.0\n","Episode: 167\n","    Loss: 323.37359619140625\n","    reconstruction loss: 191.997802734375\n","    reward loss: 3.4678797721862793\n","    kl loss: 1.0\n","Episode: 168\n","    Loss: 336.37774658203125\n","    reconstruction loss: 190.42320251464844\n","    reward loss: 3.884415864944458\n","    kl loss: 1.0\n","Episode: 169\n","    Loss: 329.1463623046875\n","    reconstruction loss: 192.35235595703125\n","    reward loss: 3.622685432434082\n","    kl loss: 1.0\n","Episode: 170\n","    Loss: 328.67523193359375\n","    reconstruction loss: 189.8086700439453\n","    reward loss: 3.681901216506958\n","    kl loss: 1.0\n","Episode: 171\n","    Loss: 318.5213928222656\n","    reconstruction loss: 189.41925048828125\n","    reward loss: 3.4029183387756348\n","    kl loss: 1.0\n","Episode: 172\n","    Loss: 325.31341552734375\n","    reconstruction loss: 201.39947509765625\n","    reward loss: 3.25468373298645\n","    kl loss: 1.0\n","Episode: 173\n","    Loss: 334.36578369140625\n","    reconstruction loss: 190.9577178955078\n","    reward loss: 3.8116586208343506\n","    kl loss: 1.0\n","Episode: 174\n","    Loss: 336.51385498046875\n","    reconstruction loss: 190.54458618164062\n","    reward loss: 3.884835720062256\n","    kl loss: 1.0\n","Episode: 175\n","    Loss: 325.41009521484375\n","    reconstruction loss: 184.89532470703125\n","    reward loss: 3.7289931774139404\n","    kl loss: 1.0\n","Episode: 176\n","    Loss: 327.7120361328125\n","    reconstruction loss: 191.4210205078125\n","    reward loss: 3.6083145141601562\n","    kl loss: 1.0\n","Episode: 177\n","    Loss: 312.70135498046875\n","    reconstruction loss: 188.3474884033203\n","    reward loss: 3.2672536373138428\n","    kl loss: 1.0\n","Episode: 178\n","    Loss: 313.61907958984375\n","    reconstruction loss: 186.97531127929688\n","    reward loss: 3.332679271697998\n","    kl loss: 1.0\n","Episode: 179\n","    Loss: 340.0152282714844\n","    reconstruction loss: 191.154052734375\n","    reward loss: 3.9674620628356934\n","    kl loss: 1.0\n","Episode: 180\n","    Loss: 329.72467041015625\n","    reconstruction loss: 188.19619750976562\n","    reward loss: 3.75795578956604\n","    kl loss: 1.0\n","Episode: 181\n","    Loss: 326.3752746582031\n","    reconstruction loss: 189.30630493164062\n","    reward loss: 3.6305418014526367\n","    kl loss: 1.0\n","Episode: 182\n","    Loss: 319.7200927734375\n","    reconstruction loss: 190.3312225341797\n","    reward loss: 3.4111101627349854\n","    kl loss: 1.0\n","Episode: 183\n","    Loss: 333.736328125\n","    reconstruction loss: 195.792236328125\n","    reward loss: 3.655545473098755\n","    kl loss: 1.0\n","Episode: 184\n","    Loss: 324.2705078125\n","    reconstruction loss: 187.1654510498047\n","    reward loss: 3.631572723388672\n","    kl loss: 1.0\n","Episode: 185\n","    Loss: 321.134765625\n","    reconstruction loss: 189.22666931152344\n","    reward loss: 3.483088254928589\n","    kl loss: 1.0\n","Episode: 186\n","    Loss: 299.90972900390625\n","    reconstruction loss: 181.0331268310547\n","    reward loss: 3.110760450363159\n","    kl loss: 1.0\n","Episode: 187\n","    Loss: 332.802001953125\n","    reconstruction loss: 181.33645629882812\n","    reward loss: 4.041872501373291\n","    kl loss: 1.0\n","Episode: 188\n","    Loss: 330.50164794921875\n","    reconstruction loss: 189.64720153808594\n","    reward loss: 3.738698720932007\n","    kl loss: 1.0\n","Episode: 189\n","    Loss: 312.8448181152344\n","    reconstruction loss: 186.59158325195312\n","    reward loss: 3.321521282196045\n","    kl loss: 1.0\n","Episode: 190\n","    Loss: 342.61431884765625\n","    reconstruction loss: 193.705322265625\n","    reward loss: 3.9688286781311035\n","    kl loss: 1.0\n","Episode: 191\n","    Loss: 311.18719482421875\n","    reconstruction loss: 186.9763946533203\n","    reward loss: 3.2631657123565674\n","    kl loss: 1.0\n","Episode: 192\n","    Loss: 294.8887634277344\n","    reconstruction loss: 185.4495086669922\n","    reward loss: 2.8411214351654053\n","    kl loss: 1.0\n","Episode: 193\n","    Loss: 324.9239501953125\n","    reconstruction loss: 188.89755249023438\n","    reward loss: 3.600754737854004\n","    kl loss: 1.0\n","Episode: 194\n","    Loss: 316.3567810058594\n","    reconstruction loss: 193.63983154296875\n","    reward loss: 3.2204842567443848\n","    kl loss: 1.0\n","Episode: 195\n","    Loss: 312.48907470703125\n","    reconstruction loss: 188.91006469726562\n","    reward loss: 3.2232158184051514\n","    kl loss: 1.076643466949463\n","Episode: 196\n","    Loss: 326.0741882324219\n","    reconstruction loss: 187.09689331054688\n","    reward loss: 3.685065269470215\n","    kl loss: 1.0000016689300537\n","Episode: 197\n","    Loss: 299.7400207519531\n","    reconstruction loss: 186.53997802734375\n","    reward loss: 2.9485726356506348\n","    kl loss: 1.0\n","Episode: 198\n","    Loss: 322.1536560058594\n","    reconstruction loss: 195.77378845214844\n","    reward loss: 3.325138807296753\n","    kl loss: 1.0\n","Episode: 199\n","    Loss: 295.039794921875\n","    reconstruction loss: 182.34169006347656\n","    reward loss: 2.934231996536255\n","    kl loss: 1.0\n","Episode: 200\n","    Loss: 319.4619140625\n","    reconstruction loss: 183.45428466796875\n","    reward loss: 3.600217819213867\n","    kl loss: 1.0\n","Episode: 201\n","    Loss: 325.0815124511719\n","    reconstruction loss: 184.84768676757812\n","    reward loss: 3.720966339111328\n","    kl loss: 1.0\n","Episode: 202\n","    Loss: 333.92791748046875\n","    reconstruction loss: 190.76968383789062\n","    reward loss: 3.804521322250366\n","    kl loss: 1.0\n","Episode: 203\n","    Loss: 306.72314453125\n","    reconstruction loss: 186.02920532226562\n","    reward loss: 3.1626837253570557\n","    kl loss: 1.0\n","Episode: 204\n","    Loss: 322.48358154296875\n","    reconstruction loss: 185.26788330078125\n","    reward loss: 3.6347343921661377\n","    kl loss: 1.0\n","Episode: 205\n","    Loss: 344.92779541015625\n","    reconstruction loss: 190.77389526367188\n","    reward loss: 4.118683338165283\n","    kl loss: 1.0\n","Episode: 206\n","    Loss: 327.6355895996094\n","    reconstruction loss: 191.4169921875\n","    reward loss: 3.606245756149292\n","    kl loss: 1.0\n","Episode: 207\n","    Loss: 317.5915832519531\n","    reconstruction loss: 195.6675262451172\n","    reward loss: 3.1978302001953125\n","    kl loss: 1.0\n","Episode: 208\n","    Loss: 318.7060241699219\n","    reconstruction loss: 188.01284790039062\n","    reward loss: 3.448376417160034\n","    kl loss: 1.0\n","Episode: 209\n","    Loss: 325.5646057128906\n","    reconstruction loss: 191.54283142089844\n","    reward loss: 3.5434441566467285\n","    kl loss: 1.0001232624053955\n","Episode: 210\n","    Loss: 309.25732421875\n","    reconstruction loss: 181.7720947265625\n","    reward loss: 3.356720447540283\n","    kl loss: 1.0\n","Episode: 211\n","    Loss: 332.9931640625\n","    reconstruction loss: 190.16769409179688\n","    reward loss: 3.793304443359375\n","    kl loss: 1.005982756614685\n","Episode: 212\n","    Loss: 332.02734375\n","    reconstruction loss: 187.62747192382812\n","    reward loss: 3.8398115634918213\n","    kl loss: 1.000646948814392\n","Episode: 213\n","    Loss: 307.41058349609375\n","    reconstruction loss: 188.41796875\n","    reward loss: 3.1137678623199463\n","    kl loss: 1.0010733604431152\n","Episode: 214\n","    Loss: 310.77484130859375\n","    reconstruction loss: 189.97879028320312\n","    reward loss: 3.1656010150909424\n","    kl loss: 1.0\n","Episode: 215\n","    Loss: 346.2205810546875\n","    reconstruction loss: 191.85317993164062\n","    reward loss: 4.124782562255859\n","    kl loss: 1.0\n","Episode: 216\n","    Loss: 326.0523681640625\n","    reconstruction loss: 191.06703186035156\n","    reward loss: 3.571009397506714\n","    kl loss: 1.0\n","Episode: 217\n","    Loss: 327.6539001464844\n","    reconstruction loss: 193.9654541015625\n","    reward loss: 3.5339555740356445\n","    kl loss: 1.0\n","Episode: 218\n","    Loss: 334.5140686035156\n","    reconstruction loss: 193.88522338867188\n","    reward loss: 3.732210397720337\n","    kl loss: 1.0001506805419922\n","Episode: 219\n","    Loss: 330.8883056640625\n","    reconstruction loss: 186.64988708496094\n","    reward loss: 3.8353829383850098\n","    kl loss: 1.0\n","Episode: 220\n","    Loss: 328.432373046875\n","    reconstruction loss: 196.6637420654297\n","    reward loss: 3.4785375595092773\n","    kl loss: 1.0019794702529907\n","Episode: 221\n","    Loss: 314.8595275878906\n","    reconstruction loss: 186.0880126953125\n","    reward loss: 3.393101692199707\n","    kl loss: 1.0012975931167603\n","Episode: 222\n","    Loss: 297.49090576171875\n","    reconstruction loss: 186.61239624023438\n","    reward loss: 2.8684849739074707\n","    kl loss: 1.0481524467468262\n","Episode: 223\n","    Loss: 318.2511901855469\n","    reconstruction loss: 196.40170288085938\n","    reward loss: 3.1764519214630127\n","    kl loss: 1.0673664808273315\n","Episode: 224\n","    Loss: 324.0703430175781\n","    reconstruction loss: 189.9416961669922\n","    reward loss: 3.532226800918579\n","    kl loss: 1.0500705242156982\n","Episode: 225\n","    Loss: 313.4110412597656\n","    reconstruction loss: 185.40213012695312\n","    reward loss: 3.3681249618530273\n","    kl loss: 1.0124551057815552\n","Episode: 226\n","    Loss: 325.7737121582031\n","    reconstruction loss: 195.32012939453125\n","    reward loss: 3.439800262451172\n","    kl loss: 1.006056785583496\n","Episode: 227\n","    Loss: 337.2734069824219\n","    reconstruction loss: 190.28054809570312\n","    reward loss: 3.911802291870117\n","    kl loss: 1.00797700881958\n","Episode: 228\n","    Loss: 325.7908935546875\n","    reconstruction loss: 194.65792846679688\n","    reward loss: 3.4590871334075928\n","    kl loss: 1.006489634513855\n","Episode: 229\n","    Loss: 319.24713134765625\n","    reconstruction loss: 185.85137939453125\n","    reward loss: 3.5247018337249756\n","    kl loss: 1.0031176805496216\n","Episode: 230\n","    Loss: 341.2923583984375\n","    reconstruction loss: 193.3739013671875\n","    reward loss: 3.9336626529693604\n","    kl loss: 1.0240261554718018\n","Episode: 231\n","    Loss: 302.9736328125\n","    reconstruction loss: 183.0260009765625\n","    reward loss: 3.1390199661254883\n","    kl loss: 1.008195161819458\n","Episode: 232\n","    Loss: 301.07080078125\n","    reconstruction loss: 189.43783569335938\n","    reward loss: 2.9025380611419678\n","    kl loss: 1.0044113397598267\n","Episode: 233\n","    Loss: 312.6896057128906\n","    reconstruction loss: 185.9970703125\n","    reward loss: 3.3340725898742676\n","    kl loss: 1.0\n","Episode: 234\n","    Loss: 321.291015625\n","    reconstruction loss: 184.55438232421875\n","    reward loss: 3.6209099292755127\n","    kl loss: 1.000478744506836\n","Episode: 235\n","    Loss: 324.4628601074219\n","    reconstruction loss: 188.97811889648438\n","    reward loss: 3.5809731483459473\n","    kl loss: 1.0150678157806396\n","Episode: 236\n","    Loss: 308.731201171875\n","    reconstruction loss: 188.88063049316406\n","    reward loss: 3.1354167461395264\n","    kl loss: 1.0111005306243896\n","Episode: 237\n","    Loss: 301.8180236816406\n","    reconstruction loss: 187.59759521484375\n","    reward loss: 2.975507974624634\n","    kl loss: 1.007766842842102\n","Episode: 238\n","    Loss: 320.4246826171875\n","    reconstruction loss: 190.00381469726562\n","    reward loss: 3.4118406772613525\n","    kl loss: 1.1006447076797485\n","Episode: 239\n","    Loss: 338.68438720703125\n","    reconstruction loss: 191.16720581054688\n","    reward loss: 3.89906907081604\n","    kl loss: 1.1049755811691284\n","Episode: 240\n","    Loss: 321.3990478515625\n","    reconstruction loss: 184.9276580810547\n","    reward loss: 3.573509693145752\n","    kl loss: 1.1398569345474243\n","Episode: 241\n","    Loss: 314.0676574707031\n","    reconstruction loss: 186.31182861328125\n","    reward loss: 3.32322096824646\n","    kl loss: 1.1443098783493042\n","Episode: 242\n","    Loss: 343.43951416015625\n","    reconstruction loss: 196.08456420898438\n","    reward loss: 3.8851826190948486\n","    kl loss: 1.1373562812805176\n","Episode: 243\n","    Loss: 324.2673034667969\n","    reconstruction loss: 180.82061767578125\n","    reward loss: 3.7078990936279297\n","    kl loss: 1.367019772529602\n","Episode: 244\n","    Loss: 330.43994140625\n","    reconstruction loss: 193.010009765625\n","    reward loss: 3.562819242477417\n","    kl loss: 1.2731249332427979\n","Episode: 245\n","    Loss: 321.27227783203125\n","    reconstruction loss: 193.49508666992188\n","    reward loss: 3.3058018684387207\n","    kl loss: 1.2074130773544312\n","Episode: 246\n","    Loss: 316.501708984375\n","    reconstruction loss: 184.89083862304688\n","    reward loss: 3.422868251800537\n","    kl loss: 1.1810482740402222\n","Episode: 247\n","    Loss: 341.3963317871094\n","    reconstruction loss: 196.71633911132812\n","    reward loss: 3.778820753097534\n","    kl loss: 1.2421252727508545\n","Episode: 248\n","    Loss: 326.44097900390625\n","    reconstruction loss: 181.26695251464844\n","    reward loss: 3.798027276992798\n","    kl loss: 1.224307894706726\n","Episode: 249\n","    Loss: 321.3991394042969\n","    reconstruction loss: 184.34686279296875\n","    reward loss: 3.5535271167755127\n","    kl loss: 1.2678840160369873\n","Episode: 250\n","    Loss: 297.09552001953125\n","    reconstruction loss: 178.43865966796875\n","    reward loss: 3.058602809906006\n","    kl loss: 1.1605762243270874\n","Episode: 251\n","    Loss: 323.2839050292969\n","    reconstruction loss: 193.3596649169922\n","    reward loss: 3.384626626968384\n","    kl loss: 1.146230936050415\n","Episode: 252\n","    Loss: 324.8471374511719\n","    reconstruction loss: 186.64468383789062\n","    reward loss: 3.6137642860412598\n","    kl loss: 1.172070860862732\n","Episode: 253\n","    Loss: 336.5760498046875\n","    reconstruction loss: 189.9933319091797\n","    reward loss: 3.8838865756988525\n","    kl loss: 1.0646674633026123\n","Episode: 254\n","    Loss: 314.4581298828125\n","    reconstruction loss: 187.09344482421875\n","    reward loss: 3.3208203315734863\n","    kl loss: 1.113599419593811\n","Episode: 255\n","    Loss: 331.60076904296875\n","    reconstruction loss: 187.36964416503906\n","    reward loss: 3.8101210594177246\n","    kl loss: 1.0876880884170532\n","Episode: 256\n","    Loss: 314.5563049316406\n","    reconstruction loss: 183.71192932128906\n","    reward loss: 3.402960777282715\n","    kl loss: 1.1740756034851074\n","Episode: 257\n","    Loss: 325.8475341796875\n","    reconstruction loss: 192.59075927734375\n","    reward loss: 3.4748730659484863\n","    kl loss: 1.1636245250701904\n","Episode: 258\n","    Loss: 299.6995849609375\n","    reconstruction loss: 182.8584442138672\n","    reward loss: 2.985140085220337\n","    kl loss: 1.2361233234405518\n","Episode: 259\n","    Loss: 332.73236083984375\n","    reconstruction loss: 194.19338989257812\n","    reward loss: 3.5889997482299805\n","    kl loss: 1.2923966646194458\n","Episode: 260\n","    Loss: 320.156005859375\n","    reconstruction loss: 182.77218627929688\n","    reward loss: 3.456120491027832\n","    kl loss: 1.6419614553451538\n","Episode: 261\n","    Loss: 319.7182922363281\n","    reconstruction loss: 192.2857666015625\n","    reward loss: 3.135369300842285\n","    kl loss: 1.7694597244262695\n","Episode: 262\n","    Loss: 318.77069091796875\n","    reconstruction loss: 188.39535522460938\n","    reward loss: 3.262266159057617\n","    kl loss: 1.6196013689041138\n","Episode: 263\n","    Loss: 334.8625793457031\n","    reconstruction loss: 187.20355224609375\n","    reward loss: 3.6004419326782227\n","    kl loss: 2.1643550395965576\n","Episode: 264\n","    Loss: 301.1858825683594\n","    reconstruction loss: 182.47760009765625\n","    reward loss: 2.8869903087615967\n","    kl loss: 1.7663607597351074\n","Episode: 265\n","    Loss: 311.3370666503906\n","    reconstruction loss: 188.70608520507812\n","    reward loss: 2.94406795501709\n","    kl loss: 1.9588584899902344\n","Episode: 266\n","    Loss: 323.71905517578125\n","    reconstruction loss: 190.01596069335938\n","    reward loss: 3.2131078243255615\n","    kl loss: 2.124432325363159\n","Episode: 267\n","    Loss: 302.95355224609375\n","    reconstruction loss: 175.36514282226562\n","    reward loss: 3.1838176250457764\n","    kl loss: 1.615479588508606\n","Episode: 268\n","    Loss: 325.301025390625\n","    reconstruction loss: 186.832275390625\n","    reward loss: 3.4481923580169678\n","    kl loss: 1.7782001495361328\n","Episode: 269\n","    Loss: 300.1734619140625\n","    reconstruction loss: 183.57208251953125\n","    reward loss: 2.91513991355896\n","    kl loss: 1.4571471214294434\n","Episode: 270\n","    Loss: 303.6119079589844\n","    reconstruction loss: 195.025634765625\n","    reward loss: 2.6848156452178955\n","    kl loss: 1.4617729187011719\n","Episode: 271\n","    Loss: 307.1900634765625\n","    reconstruction loss: 191.53675842285156\n","    reward loss: 2.8806512355804443\n","    kl loss: 1.4830505847930908\n","Episode: 272\n","    Loss: 338.51153564453125\n","    reconstruction loss: 192.17599487304688\n","    reward loss: 3.7765655517578125\n","    kl loss: 1.4155775308609009\n","Episode: 273\n","    Loss: 320.8472595214844\n","    reconstruction loss: 183.77821350097656\n","    reward loss: 3.521029472351074\n","    kl loss: 1.3832995891571045\n","Episode: 274\n","    Loss: 334.7153625488281\n","    reconstruction loss: 186.33529663085938\n","    reward loss: 3.8287353515625\n","    kl loss: 1.4374330043792725\n","Episode: 275\n","    Loss: 327.1417236328125\n","    reconstruction loss: 192.31631469726562\n","    reward loss: 3.456885814666748\n","    kl loss: 1.3834396600723267\n","Episode: 276\n","    Loss: 303.8363342285156\n","    reconstruction loss: 179.03887939453125\n","    reward loss: 3.0836095809936523\n","    kl loss: 1.6871134042739868\n","Episode: 277\n","    Loss: 297.1407165527344\n","    reconstruction loss: 190.45016479492188\n","    reward loss: 2.5826377868652344\n","    kl loss: 1.6298251152038574\n","Episode: 278\n","    Loss: 313.7641906738281\n","    reconstruction loss: 186.41928100585938\n","    reward loss: 3.185606002807617\n","    kl loss: 1.5848681926727295\n","Episode: 279\n","    Loss: 323.7494812011719\n","    reconstruction loss: 189.69482421875\n","    reward loss: 3.3598453998565674\n","    kl loss: 1.6460040807724\n","Episode: 280\n","    Loss: 322.8229675292969\n","    reconstruction loss: 186.3433380126953\n","    reward loss: 3.3948962688446045\n","    kl loss: 1.7658253908157349\n","Episode: 281\n","    Loss: 323.7470397949219\n","    reconstruction loss: 183.45159912109375\n","    reward loss: 3.5783793926239014\n","    kl loss: 1.505216360092163\n","Episode: 282\n","    Loss: 300.583740234375\n","    reconstruction loss: 179.0706024169922\n","    reward loss: 3.04925274848938\n","    kl loss: 1.4789307117462158\n","Episode: 283\n","    Loss: 306.8232116699219\n","    reconstruction loss: 181.62893676757812\n","    reward loss: 3.1136457920074463\n","    kl loss: 1.6216661930084229\n","Episode: 284\n","    Loss: 310.9135437011719\n","    reconstruction loss: 188.58840942382812\n","    reward loss: 3.0884315967559814\n","    kl loss: 1.423002004623413\n","Episode: 285\n","    Loss: 315.2979736328125\n","    reconstruction loss: 187.67593383789062\n","    reward loss: 3.2128686904907227\n","    kl loss: 1.5171635150909424\n","Episode: 286\n","    Loss: 302.4723205566406\n","    reconstruction loss: 185.74929809570312\n","    reward loss: 2.946708917617798\n","    kl loss: 1.3588218688964844\n","Episode: 287\n","    Loss: 310.4940185546875\n","    reconstruction loss: 183.9560546875\n","    reward loss: 3.228055238723755\n","    kl loss: 1.355602502822876\n","Episode: 288\n","    Loss: 324.1517028808594\n","    reconstruction loss: 188.78884887695312\n","    reward loss: 3.4413750171661377\n","    kl loss: 1.4914722442626953\n","Episode: 289\n","    Loss: 327.7121887207031\n","    reconstruction loss: 189.6003875732422\n","    reward loss: 3.5627543926239014\n","    kl loss: 1.3415404558181763\n","Episode: 290\n","    Loss: 318.1356506347656\n","    reconstruction loss: 181.476806640625\n","    reward loss: 3.523637056350708\n","    kl loss: 1.3331546783447266\n","Episode: 291\n","    Loss: 316.12725830078125\n","    reconstruction loss: 185.91143798828125\n","    reward loss: 3.3604140281677246\n","    kl loss: 1.260132908821106\n","Episode: 292\n","    Loss: 303.4351806640625\n","    reconstruction loss: 180.47296142578125\n","    reward loss: 3.141580820083618\n","    kl loss: 1.3006885051727295\n","Episode: 293\n","    Loss: 306.5492858886719\n","    reconstruction loss: 184.28372192382812\n","    reward loss: 3.110520362854004\n","    kl loss: 1.339735507965088\n","Episode: 294\n","    Loss: 311.95806884765625\n","    reconstruction loss: 189.7167510986328\n","    reward loss: 3.045593500137329\n","    kl loss: 1.5645536184310913\n","Episode: 295\n","    Loss: 300.8432312011719\n","    reconstruction loss: 188.1153564453125\n","    reward loss: 2.796020030975342\n","    kl loss: 1.4867169857025146\n","Episode: 296\n","    Loss: 308.642578125\n","    reconstruction loss: 186.17327880859375\n","    reward loss: 3.05240535736084\n","    kl loss: 1.5635087490081787\n","Episode: 297\n","    Loss: 300.9471435546875\n","    reconstruction loss: 179.34271240234375\n","    reward loss: 2.9944798946380615\n","    kl loss: 1.679764747619629\n","Episode: 298\n","    Loss: 324.5191345214844\n","    reconstruction loss: 191.04660034179688\n","    reward loss: 3.349456787109375\n","    kl loss: 1.6241538524627686\n","Episode: 299\n","    Loss: 302.5106506347656\n","    reconstruction loss: 191.79165649414062\n","    reward loss: 2.70766544342041\n","    kl loss: 1.5950713157653809\n","Episode: 300\n","    Loss: 314.135986328125\n","    reconstruction loss: 178.16445922851562\n","    reward loss: 3.4159770011901855\n","    kl loss: 1.6412338018417358\n","Episode: 301\n","    Loss: 291.6952209472656\n","    reconstruction loss: 181.87274169921875\n","    reward loss: 2.6970536708831787\n","    kl loss: 1.5425608158111572\n","Episode: 302\n","    Loss: 306.6314392089844\n","    reconstruction loss: 186.07720947265625\n","    reward loss: 3.0021603107452393\n","    kl loss: 1.5478593111038208\n","Episode: 303\n","    Loss: 321.978515625\n","    reconstruction loss: 186.56195068359375\n","    reward loss: 3.387028217315674\n","    kl loss: 1.6870582103729248\n","Episode: 304\n","    Loss: 305.10711669921875\n","    reconstruction loss: 180.5457763671875\n","    reward loss: 3.1095778942108154\n","    kl loss: 1.5726096630096436\n","Episode: 305\n","    Loss: 297.5088195800781\n","    reconstruction loss: 177.8931427001953\n","    reward loss: 2.9167802333831787\n","    kl loss: 1.7528350353240967\n","Episode: 306\n","    Loss: 321.54071044921875\n","    reconstruction loss: 182.449951171875\n","    reward loss: 3.5225603580474854\n","    kl loss: 1.5801159143447876\n","Episode: 307\n","    Loss: 311.9976806640625\n","    reconstruction loss: 182.74111938476562\n","    reward loss: 3.207109212875366\n","    kl loss: 1.7007761001586914\n","Episode: 308\n","    Loss: 331.211181640625\n","    reconstruction loss: 189.21920776367188\n","    reward loss: 3.469743013381958\n","    kl loss: 2.0550954341888428\n","Episode: 309\n","    Loss: 298.9317626953125\n","    reconstruction loss: 181.83958435058594\n","    reward loss: 2.8884506225585938\n","    kl loss: 1.5996392965316772\n","Episode: 310\n","    Loss: 314.858642578125\n","    reconstruction loss: 182.68267822265625\n","    reward loss: 3.168131113052368\n","    kl loss: 2.1291396617889404\n","Episode: 311\n","    Loss: 318.3680419921875\n","    reconstruction loss: 184.1580810546875\n","    reward loss: 3.3096184730529785\n","    kl loss: 1.8373308181762695\n","Episode: 312\n","    Loss: 327.3607177734375\n","    reconstruction loss: 183.94949340820312\n","    reward loss: 3.6106154918670654\n","    kl loss: 1.703965663909912\n","Episode: 313\n","    Loss: 310.0241394042969\n","    reconstruction loss: 180.3211669921875\n","    reward loss: 3.206735134124756\n","    kl loss: 1.746725082397461\n","Episode: 314\n","    Loss: 297.9702453613281\n","    reconstruction loss: 181.371337890625\n","    reward loss: 2.7268176078796387\n","    kl loss: 2.116030216217041\n","Episode: 315\n","    Loss: 311.054443359375\n","    reconstruction loss: 183.0545654296875\n","    reward loss: 3.125262975692749\n","    kl loss: 1.8615648746490479\n","Episode: 316\n","    Loss: 323.61431884765625\n","    reconstruction loss: 184.18795776367188\n","    reward loss: 3.4276063442230225\n","    kl loss: 1.9460150003433228\n","Episode: 317\n","    Loss: 312.53094482421875\n","    reconstruction loss: 178.70806884765625\n","    reward loss: 3.32377552986145\n","    kl loss: 1.7490730285644531\n","Episode: 318\n","    Loss: 320.56219482421875\n","    reconstruction loss: 182.34059143066406\n","    reward loss: 3.487730026245117\n","    kl loss: 1.6151067018508911\n","Episode: 319\n","    Loss: 314.9757080078125\n","    reconstruction loss: 183.58291625976562\n","    reward loss: 3.3287353515625\n","    kl loss: 1.4887052774429321\n","Episode: 320\n","    Loss: 303.83795166015625\n","    reconstruction loss: 187.85523986816406\n","    reward loss: 2.8857109546661377\n","    kl loss: 1.4982808828353882\n","Episode: 321\n","    Loss: 297.6197204589844\n","    reconstruction loss: 179.12918090820312\n","    reward loss: 2.968628406524658\n","    kl loss: 1.458853006362915\n","Episode: 322\n","    Loss: 282.61480712890625\n","    reconstruction loss: 172.61386108398438\n","    reward loss: 2.726865291595459\n","    kl loss: 1.456066370010376\n","Episode: 323\n","    Loss: 291.2888488769531\n","    reconstruction loss: 181.66030883789062\n","    reward loss: 2.688516139984131\n","    kl loss: 1.5530500411987305\n","Episode: 324\n","    Loss: 298.6829833984375\n","    reconstruction loss: 186.76913452148438\n","    reward loss: 2.7609057426452637\n","    kl loss: 1.528217077255249\n","Episode: 325\n","    Loss: 295.1468505859375\n","    reconstruction loss: 184.71084594726562\n","    reward loss: 2.6888890266418457\n","    kl loss: 1.6324875354766846\n","Episode: 326\n","    Loss: 314.4361572265625\n","    reconstruction loss: 188.20326232910156\n","    reward loss: 3.1536471843719482\n","    kl loss: 1.585522174835205\n","Episode: 327\n","    Loss: 295.6883239746094\n","    reconstruction loss: 175.43505859375\n","    reward loss: 2.9549992084503174\n","    kl loss: 1.6828289031982422\n","Episode: 328\n","    Loss: 317.0372619628906\n","    reconstruction loss: 180.83094787597656\n","    reward loss: 3.413701295852661\n","    kl loss: 1.6726763248443604\n","Episode: 329\n","    Loss: 289.5074157714844\n","    reconstruction loss: 176.26832580566406\n","    reward loss: 2.8331387042999268\n","    kl loss: 1.4079251289367676\n","Episode: 330\n","    Loss: 308.1434326171875\n","    reconstruction loss: 179.58721923828125\n","    reward loss: 3.208296060562134\n","    kl loss: 1.6265875101089478\n","Episode: 331\n","    Loss: 305.7415466308594\n","    reconstruction loss: 182.9112091064453\n","    reward loss: 3.0971672534942627\n","    kl loss: 1.4429466724395752\n","Episode: 332\n","    Loss: 305.54754638671875\n","    reconstruction loss: 181.90150451660156\n","    reward loss: 3.103985548019409\n","    kl loss: 1.5006529092788696\n","Episode: 333\n","    Loss: 312.1454162597656\n","    reconstruction loss: 182.68809509277344\n","    reward loss: 3.268853187561035\n","    kl loss: 1.504746437072754\n","Episode: 334\n","    Loss: 303.86346435546875\n","    reconstruction loss: 183.39187622070312\n","    reward loss: 3.002864360809326\n","    kl loss: 1.5371342897415161\n","Episode: 335\n","    Loss: 291.1614074707031\n","    reconstruction loss: 177.859375\n","    reward loss: 2.7571120262145996\n","    kl loss: 1.6803098917007446\n","Episode: 336\n","    Loss: 304.3272705078125\n","    reconstruction loss: 175.189208984375\n","    reward loss: 3.217402935028076\n","    kl loss: 1.6528960466384888\n","Episode: 337\n","    Loss: 307.1837463378906\n","    reconstruction loss: 177.554931640625\n","    reward loss: 3.224658966064453\n","    kl loss: 1.6765753030776978\n","Episode: 338\n","    Loss: 293.9551696777344\n","    reconstruction loss: 190.73678588867188\n","    reward loss: 2.4398694038391113\n","    kl loss: 1.7822980880737305\n","Episode: 339\n","    Loss: 299.59210205078125\n","    reconstruction loss: 179.90536499023438\n","    reward loss: 2.917409658432007\n","    kl loss: 1.7577406167984009\n","Episode: 340\n","    Loss: 315.7180480957031\n","    reconstruction loss: 176.71322631835938\n","    reward loss: 3.460750102996826\n","    kl loss: 1.7878577709197998\n","Episode: 341\n","    Loss: 327.03411865234375\n","    reconstruction loss: 192.32916259765625\n","    reward loss: 3.3277320861816406\n","    kl loss: 1.8234299421310425\n","Episode: 342\n","    Loss: 295.18011474609375\n","    reconstruction loss: 177.66879272460938\n","    reward loss: 2.8529980182647705\n","    kl loss: 1.765641212463379\n","Episode: 343\n","    Loss: 335.5205383300781\n","    reconstruction loss: 183.0734405517578\n","    reward loss: 3.7888526916503906\n","    kl loss: 1.9837244749069214\n","Episode: 344\n","    Loss: 308.21917724609375\n","    reconstruction loss: 178.9425048828125\n","    reward loss: 3.1880297660827637\n","    kl loss: 1.7695622444152832\n","Episode: 345\n","    Loss: 305.39532470703125\n","    reconstruction loss: 181.47418212890625\n","    reward loss: 3.0528509616851807\n","    kl loss: 1.707136631011963\n","Episode: 346\n","    Loss: 301.865234375\n","    reconstruction loss: 172.3268280029297\n","    reward loss: 3.2143545150756836\n","    kl loss: 1.703600287437439\n","Episode: 347\n","    Loss: 292.9140319824219\n","    reconstruction loss: 174.44688415527344\n","    reward loss: 2.9112813472747803\n","    kl loss: 1.6572309732437134\n","Episode: 348\n","    Loss: 306.4173278808594\n","    reconstruction loss: 183.7130126953125\n","    reward loss: 3.0423099994659424\n","    kl loss: 1.6223491430282593\n","Episode: 349\n","    Loss: 292.74005126953125\n","    reconstruction loss: 177.52764892578125\n","    reward loss: 2.800018787384033\n","    kl loss: 1.7211735248565674\n","Episode: 350\n","    Loss: 292.1095275878906\n","    reconstruction loss: 179.87327575683594\n","    reward loss: 2.705465316772461\n","    kl loss: 1.7544950246810913\n","Episode: 351\n","    Loss: 317.0408630371094\n","    reconstruction loss: 178.61080932617188\n","    reward loss: 3.4755702018737793\n","    kl loss: 1.6785095930099487\n","Episode: 352\n","    Loss: 305.7320861816406\n","    reconstruction loss: 169.40481567382812\n","    reward loss: 3.4117138385772705\n","    kl loss: 1.6917308568954468\n","Episode: 353\n","    Loss: 304.93206787109375\n","    reconstruction loss: 174.86956787109375\n","    reward loss: 3.2332756519317627\n","    kl loss: 1.6897869110107422\n","Episode: 354\n","    Loss: 304.0854797363281\n","    reconstruction loss: 179.33053588867188\n","    reward loss: 3.0819337368011475\n","    kl loss: 1.6887277364730835\n","Episode: 355\n","    Loss: 299.8742370605469\n","    reconstruction loss: 170.6351776123047\n","    reward loss: 3.211354970932007\n","    kl loss: 1.6841650009155273\n","Episode: 356\n","    Loss: 291.7664489746094\n","    reconstruction loss: 184.98117065429688\n","    reward loss: 2.575441598892212\n","    kl loss: 1.6644803285598755\n","Episode: 357\n","    Loss: 293.1693420410156\n","    reconstruction loss: 174.73382568359375\n","    reward loss: 2.8969719409942627\n","    kl loss: 1.704149603843689\n","Episode: 358\n","    Loss: 303.99884033203125\n","    reconstruction loss: 172.9796142578125\n","    reward loss: 3.233049154281616\n","    kl loss: 1.7862489223480225\n","Episode: 359\n","    Loss: 293.3409423828125\n","    reconstruction loss: 179.48590087890625\n","    reward loss: 2.73964524269104\n","    kl loss: 1.7967464923858643\n","Episode: 360\n","    Loss: 311.29730224609375\n","    reconstruction loss: 169.70852661132812\n","    reward loss: 3.481571912765503\n","    kl loss: 1.973376989364624\n","Episode: 361\n","    Loss: 287.5202941894531\n","    reconstruction loss: 165.36642456054688\n","    reward loss: 2.830267906188965\n","    kl loss: 2.3094494342803955\n","Episode: 362\n","    Loss: 301.1640625\n","    reconstruction loss: 175.89801025390625\n","    reward loss: 3.0104284286499023\n","    kl loss: 1.990107774734497\n","Episode: 363\n","    Loss: 311.59222412109375\n","    reconstruction loss: 175.95069885253906\n","    reward loss: 3.2506566047668457\n","    kl loss: 2.186854600906372\n","Episode: 364\n","    Loss: 312.83538818359375\n","    reconstruction loss: 173.0780792236328\n","    reward loss: 3.380625009536743\n","    kl loss: 2.143543243408203\n","Episode: 365\n","    Loss: 298.939697265625\n","    reconstruction loss: 169.10186767578125\n","    reward loss: 3.1047911643981934\n","    kl loss: 2.1170125007629395\n","Episode: 366\n","    Loss: 297.2496337890625\n","    reconstruction loss: 171.3349609375\n","    reward loss: 2.75461483001709\n","    kl loss: 2.950315475463867\n","Episode: 367\n","    Loss: 306.4817199707031\n","    reconstruction loss: 172.60784912109375\n","    reward loss: 3.1962525844573975\n","    kl loss: 2.200503349304199\n","Episode: 368\n","    Loss: 316.7947692871094\n","    reconstruction loss: 175.64981079101562\n","    reward loss: 3.3123388290405273\n","    kl loss: 2.5213093757629395\n","Episode: 369\n","    Loss: 284.58599853515625\n","    reconstruction loss: 162.40785217285156\n","    reward loss: 2.8382694721221924\n","    kl loss: 2.2838711738586426\n","Episode: 370\n","    Loss: 317.01165771484375\n","    reconstruction loss: 174.95925903320312\n","    reward loss: 3.235217809677124\n","    kl loss: 2.8819777965545654\n","Episode: 371\n","    Loss: 289.9669494628906\n","    reconstruction loss: 165.2762451171875\n","    reward loss: 2.797468900680542\n","    kl loss: 2.677931070327759\n","Episode: 372\n","    Loss: 299.3785400390625\n","    reconstruction loss: 168.94015502929688\n","    reward loss: 3.063265323638916\n","    kl loss: 2.3224074840545654\n","Episode: 373\n","    Loss: 312.25286865234375\n","    reconstruction loss: 169.32662963867188\n","    reward loss: 3.337409019470215\n","    kl loss: 2.611691951751709\n","Episode: 374\n","    Loss: 294.98223876953125\n","    reconstruction loss: 172.8828887939453\n","    reward loss: 2.826406717300415\n","    kl loss: 2.3175110816955566\n","Episode: 375\n","    Loss: 291.5244445800781\n","    reconstruction loss: 166.2418670654297\n","    reward loss: 2.86285662651062\n","    kl loss: 2.508260488510132\n","Episode: 376\n","    Loss: 302.6408386230469\n","    reconstruction loss: 164.19952392578125\n","    reward loss: 3.2357888221740723\n","    kl loss: 2.5188701152801514\n","Episode: 377\n","    Loss: 297.4482421875\n","    reconstruction loss: 167.4539794921875\n","    reward loss: 2.9326744079589844\n","    kl loss: 2.7350640296936035\n","Episode: 378\n","    Loss: 293.2307434082031\n","    reconstruction loss: 166.88584899902344\n","    reward loss: 2.8688995838165283\n","    kl loss: 2.5933399200439453\n","Episode: 379\n","    Loss: 263.43072509765625\n","    reconstruction loss: 155.61810302734375\n","    reward loss: 2.4631078243255615\n","    kl loss: 2.160386323928833\n","Episode: 380\n","    Loss: 304.5014343261719\n","    reconstruction loss: 168.77444458007812\n","    reward loss: 3.110927104949951\n","    kl loss: 2.684454917907715\n","Episode: 381\n","    Loss: 315.98980712890625\n","    reconstruction loss: 175.01773071289062\n","    reward loss: 3.337437391281128\n","    kl loss: 2.41617751121521\n","Episode: 382\n","    Loss: 285.027099609375\n","    reconstruction loss: 162.08755493164062\n","    reward loss: 2.8919644355773926\n","    kl loss: 2.1720802783966064\n","Episode: 383\n","    Loss: 287.0555725097656\n","    reconstruction loss: 165.75814819335938\n","    reward loss: 2.760350227355957\n","    kl loss: 2.4685163497924805\n","Episode: 384\n","    Loss: 297.9042663574219\n","    reconstruction loss: 165.35206604003906\n","    reward loss: 3.122619152069092\n","    kl loss: 2.326052665710449\n","Episode: 385\n","    Loss: 286.1309814453125\n","    reconstruction loss: 161.263916015625\n","    reward loss: 2.8998262882232666\n","    kl loss: 2.3373122215270996\n","Episode: 386\n","    Loss: 298.2594909667969\n","    reconstruction loss: 167.3404541015625\n","    reward loss: 3.0880513191223145\n","    kl loss: 2.2837252616882324\n","Episode: 387\n","    Loss: 280.7099304199219\n","    reconstruction loss: 156.69454956054688\n","    reward loss: 2.832064390182495\n","    kl loss: 2.489311695098877\n","Episode: 388\n","    Loss: 286.89080810546875\n","    reconstruction loss: 167.21145629882812\n","    reward loss: 2.775982618331909\n","    kl loss: 2.2519943714141846\n","Episode: 389\n","    Loss: 270.44610595703125\n","    reconstruction loss: 156.00509643554688\n","    reward loss: 2.6497387886047363\n","    kl loss: 2.170015335083008\n","Episode: 390\n","    Loss: 283.7968444824219\n","    reconstruction loss: 160.52835083007812\n","    reward loss: 2.8996849060058594\n","    kl loss: 2.177950382232666\n","Episode: 391\n","    Loss: 280.2147216796875\n","    reconstruction loss: 163.183349609375\n","    reward loss: 2.741572141647339\n","    kl loss: 2.1076340675354004\n","Episode: 392\n","    Loss: 293.18310546875\n","    reconstruction loss: 165.3258056640625\n","    reward loss: 2.947160005569458\n","    kl loss: 2.470670461654663\n","Episode: 393\n","    Loss: 300.4075012207031\n","    reconstruction loss: 163.265625\n","    reward loss: 3.196791648864746\n","    kl loss: 2.5254170894622803\n","Episode: 394\n","    Loss: 287.0464782714844\n","    reconstruction loss: 160.62973022460938\n","    reward loss: 2.9828054904937744\n","    kl loss: 2.201855421066284\n","Episode: 395\n","    Loss: 303.6542663574219\n","    reconstruction loss: 170.1057891845703\n","    reward loss: 3.0855352878570557\n","    kl loss: 2.555473566055298\n","Episode: 396\n","    Loss: 275.775634765625\n","    reconstruction loss: 154.7705078125\n","    reward loss: 2.7278640270233154\n","    kl loss: 2.5529870986938477\n","Episode: 397\n","    Loss: 304.85955810546875\n","    reconstruction loss: 172.84974670410156\n","    reward loss: 3.1274735927581787\n","    kl loss: 2.254821538925171\n","Episode: 398\n","    Loss: 289.2037353515625\n","    reconstruction loss: 159.11492919921875\n","    reward loss: 2.993804454803467\n","    kl loss: 2.5305652618408203\n","Episode: 399\n","    Loss: 307.7017822265625\n","    reconstruction loss: 164.99102783203125\n","    reward loss: 3.2472589015960693\n","    kl loss: 2.9056715965270996\n","Episode: 400\n","    Loss: 310.3289794921875\n","    reconstruction loss: 162.25405883789062\n","    reward loss: 3.5238475799560547\n","    kl loss: 2.4740262031555176\n","Episode: 401\n","    Loss: 275.480712890625\n","    reconstruction loss: 156.1073455810547\n","    reward loss: 2.6870315074920654\n","    kl loss: 2.5327281951904297\n","Episode: 402\n","    Loss: 301.2068786621094\n","    reconstruction loss: 164.86276245117188\n","    reward loss: 3.206113815307617\n","    kl loss: 2.4130139350891113\n","Episode: 403\n","    Loss: 282.92828369140625\n","    reconstruction loss: 164.02337646484375\n","    reward loss: 2.6990082263946533\n","    kl loss: 2.443962812423706\n","Episode: 404\n","    Loss: 309.57769775390625\n","    reconstruction loss: 171.6849365234375\n","    reward loss: 3.1528778076171875\n","    kl loss: 2.754206418991089\n","Episode: 405\n","    Loss: 297.5568542480469\n","    reconstruction loss: 166.2303009033203\n","    reward loss: 3.063516139984131\n","    kl loss: 2.410348892211914\n","Episode: 406\n","    Loss: 282.4728698730469\n","    reconstruction loss: 154.1739044189453\n","    reward loss: 3.03605580329895\n","    kl loss: 2.2037007808685303\n","Episode: 407\n","    Loss: 289.32958984375\n","    reconstruction loss: 158.02239990234375\n","    reward loss: 3.0293266773223877\n","    kl loss: 2.5280747413635254\n","Episode: 408\n","    Loss: 291.5009765625\n","    reconstruction loss: 166.02783203125\n","    reward loss: 2.964492082595825\n","    kl loss: 2.171590805053711\n","Episode: 409\n","    Loss: 268.9912414550781\n","    reconstruction loss: 152.99911499023438\n","    reward loss: 2.6964781284332275\n","    kl loss: 2.1615374088287354\n","Episode: 410\n","    Loss: 308.6605529785156\n","    reconstruction loss: 167.2139129638672\n","    reward loss: 3.3145744800567627\n","    kl loss: 2.5436532497406006\n","Episode: 411\n","    Loss: 285.072265625\n","    reconstruction loss: 163.8934783935547\n","    reward loss: 2.775660991668701\n","    kl loss: 2.403062582015991\n","Episode: 412\n","    Loss: 278.3656311035156\n","    reconstruction loss: 160.63275146484375\n","    reward loss: 2.748948097229004\n","    kl loss: 2.151968240737915\n","Episode: 413\n","    Loss: 277.91278076171875\n","    reconstruction loss: 155.10110473632812\n","    reward loss: 2.849261522293091\n","    kl loss: 2.3087525367736816\n","Episode: 414\n","    Loss: 295.69915771484375\n","    reconstruction loss: 166.15042114257812\n","    reward loss: 2.9997973442077637\n","    kl loss: 2.4555857181549072\n","Episode: 415\n","    Loss: 248.53817749023438\n","    reconstruction loss: 147.12533569335938\n","    reward loss: 2.297318935394287\n","    kl loss: 2.100668430328369\n","Episode: 416\n","    Loss: 291.4242858886719\n","    reconstruction loss: 162.7943878173828\n","    reward loss: 3.031756639480591\n","    kl loss: 2.251840353012085\n","Episode: 417\n","    Loss: 274.2004089355469\n","    reconstruction loss: 156.2322998046875\n","    reward loss: 2.723935127258301\n","    kl loss: 2.2630364894866943\n","Episode: 418\n","    Loss: 287.26654052734375\n","    reconstruction loss: 161.56533813476562\n","    reward loss: 2.9056549072265625\n","    kl loss: 2.400330066680908\n","Episode: 419\n","    Loss: 297.9289245605469\n","    reconstruction loss: 164.56756591796875\n","    reward loss: 3.052302837371826\n","    kl loss: 2.6530749797821045\n","Episode: 420\n","    Loss: 281.4693298339844\n","    reconstruction loss: 153.99960327148438\n","    reward loss: 2.981450080871582\n","    kl loss: 2.3118982315063477\n","Episode: 421\n","    Loss: 297.8024597167969\n","    reconstruction loss: 161.41600036621094\n","    reward loss: 3.1960299015045166\n","    kl loss: 2.452542304992676\n","Episode: 422\n","    Loss: 277.9170227050781\n","    reconstruction loss: 164.15040588378906\n","    reward loss: 2.562401056289673\n","    kl loss: 2.4082577228546143\n","Episode: 423\n","    Loss: 302.73846435546875\n","    reconstruction loss: 166.2208709716797\n","    reward loss: 3.1909403800964355\n","    kl loss: 2.4834682941436768\n","Episode: 424\n","    Loss: 286.9494323730469\n","    reconstruction loss: 158.92909240722656\n","    reward loss: 2.9628942012786865\n","    kl loss: 2.431906223297119\n","Episode: 425\n","    Loss: 277.751953125\n","    reconstruction loss: 158.41944885253906\n","    reward loss: 2.656787872314453\n","    kl loss: 2.634492874145508\n","Episode: 426\n","    Loss: 294.6417541503906\n","    reconstruction loss: 163.9738311767578\n","    reward loss: 2.9629995822906494\n","    kl loss: 2.696293354034424\n","Episode: 427\n","    Loss: 308.05657958984375\n","    reconstruction loss: 168.1619415283203\n","    reward loss: 3.17203950881958\n","    kl loss: 2.887326717376709\n","Episode: 428\n","    Loss: 286.1772766113281\n","    reconstruction loss: 162.86831665039062\n","    reward loss: 2.8386309146881104\n","    kl loss: 2.395689010620117\n","Episode: 429\n","    Loss: 259.001953125\n","    reconstruction loss: 147.03717041015625\n","    reward loss: 2.5815722942352295\n","    kl loss: 2.16097354888916\n","Episode: 430\n","    Loss: 289.51226806640625\n","    reconstruction loss: 156.724609375\n","    reward loss: 3.0603365898132324\n","    kl loss: 2.567586660385132\n","Episode: 431\n","    Loss: 288.59820556640625\n","    reconstruction loss: 161.156982421875\n","    reward loss: 2.924835681915283\n","    kl loss: 2.5071964263916016\n","Episode: 432\n","    Loss: 271.4432373046875\n","    reconstruction loss: 149.07046508789062\n","    reward loss: 2.8016295433044434\n","    kl loss: 2.4315731525421143\n","Episode: 433\n","    Loss: 284.7277526855469\n","    reconstruction loss: 158.10592651367188\n","    reward loss: 2.9111931324005127\n","    kl loss: 2.4730069637298584\n","Episode: 434\n","    Loss: 286.6869812011719\n","    reconstruction loss: 155.16445922851562\n","    reward loss: 3.0390868186950684\n","    kl loss: 2.5154473781585693\n","Episode: 435\n","    Loss: 260.0911560058594\n","    reconstruction loss: 154.99319458007812\n","    reward loss: 2.337336301803589\n","    kl loss: 2.32912015914917\n","Episode: 436\n","    Loss: 276.7881774902344\n","    reconstruction loss: 153.29371643066406\n","    reward loss: 2.76910400390625\n","    kl loss: 2.657582998275757\n","Episode: 437\n","    Loss: 265.6958312988281\n","    reconstruction loss: 145.9154815673828\n","    reward loss: 2.702281951904297\n","    kl loss: 2.5200488567352295\n","Episode: 438\n","    Loss: 290.6423034667969\n","    reconstruction loss: 157.79632568359375\n","    reward loss: 2.900407552719116\n","    kl loss: 3.1331701278686523\n","Episode: 439\n","    Loss: 278.6192626953125\n","    reconstruction loss: 155.51388549804688\n","    reward loss: 2.784590482711792\n","    kl loss: 2.564472198486328\n","Episode: 440\n","    Loss: 287.7414855957031\n","    reconstruction loss: 154.6097412109375\n","    reward loss: 3.025906562805176\n","    kl loss: 2.7224996089935303\n","Episode: 441\n","    Loss: 270.373779296875\n","    reconstruction loss: 151.59005737304688\n","    reward loss: 2.57887601852417\n","    kl loss: 2.852306604385376\n","Episode: 442\n","    Loss: 285.0893859863281\n","    reconstruction loss: 150.4466552734375\n","    reward loss: 3.0382354259490967\n","    kl loss: 2.8304474353790283\n","Episode: 443\n","    Loss: 288.5955505371094\n","    reconstruction loss: 158.92764282226562\n","    reward loss: 2.904696226119995\n","    kl loss: 2.80035400390625\n","Episode: 444\n","    Loss: 276.2258605957031\n","    reconstruction loss: 150.19969177246094\n","    reward loss: 2.794809103012085\n","    kl loss: 2.8207852840423584\n","Episode: 445\n","    Loss: 293.2602844238281\n","    reconstruction loss: 151.3287353515625\n","    reward loss: 3.2379894256591797\n","    kl loss: 2.8601925373077393\n","Episode: 446\n","    Loss: 288.5912780761719\n","    reconstruction loss: 156.77809143066406\n","    reward loss: 2.998297929763794\n","    kl loss: 2.687276840209961\n","Episode: 447\n","    Loss: 274.62164306640625\n","    reconstruction loss: 155.54983520507812\n","    reward loss: 2.572535753250122\n","    kl loss: 2.903303623199463\n","Episode: 448\n","    Loss: 287.1895751953125\n","    reconstruction loss: 158.20062255859375\n","    reward loss: 2.798557996749878\n","    kl loss: 3.1039435863494873\n","Episode: 449\n","    Loss: 287.556396484375\n","    reconstruction loss: 156.27699279785156\n","    reward loss: 2.813380479812622\n","    kl loss: 3.281111001968384\n","Episode: 450\n","    Loss: 287.02020263671875\n","    reconstruction loss: 150.6942596435547\n","    reward loss: 3.0430214405059814\n","    kl loss: 2.9820199012756348\n","Episode: 451\n","    Loss: 264.4013366699219\n","    reconstruction loss: 146.7354278564453\n","    reward loss: 2.4381682872772217\n","    kl loss: 3.2330005168914795\n","Episode: 452\n","    Loss: 306.9609375\n","    reconstruction loss: 162.42904663085938\n","    reward loss: 3.21124005317688\n","    kl loss: 3.2138493061065674\n","Episode: 453\n","    Loss: 309.2558898925781\n","    reconstruction loss: 158.2752685546875\n","    reward loss: 3.3934593200683594\n","    kl loss: 3.2209558486938477\n","Episode: 454\n","    Loss: 291.3692932128906\n","    reconstruction loss: 159.66741943359375\n","    reward loss: 2.867645263671875\n","    kl loss: 3.133427858352661\n","Episode: 455\n","    Loss: 299.8511657714844\n","    reconstruction loss: 160.56842041015625\n","    reward loss: 2.9893126487731934\n","    kl loss: 3.465679168701172\n","Episode: 456\n","    Loss: 261.31475830078125\n","    reconstruction loss: 148.09588623046875\n","    reward loss: 2.3663907051086426\n","    kl loss: 3.0395212173461914\n","Episode: 457\n","    Loss: 271.47998046875\n","    reconstruction loss: 154.00831604003906\n","    reward loss: 2.476276159286499\n","    kl loss: 3.0802016258239746\n","Episode: 458\n","    Loss: 261.7597351074219\n","    reconstruction loss: 145.97647094726562\n","    reward loss: 2.385606288909912\n","    kl loss: 3.228706121444702\n","Episode: 459\n","    Loss: 283.1222229003906\n","    reconstruction loss: 153.6478271484375\n","    reward loss: 2.6879754066467285\n","    kl loss: 3.5395255088806152\n","Episode: 460\n","    Loss: 270.5329895019531\n","    reconstruction loss: 153.78970336914062\n","    reward loss: 2.4818804264068604\n","    kl loss: 2.9877469539642334\n","Episode: 461\n","    Loss: 265.356201171875\n","    reconstruction loss: 149.02767944335938\n","    reward loss: 2.5412051677703857\n","    kl loss: 2.7386350631713867\n","Episode: 462\n","    Loss: 286.4202575683594\n","    reconstruction loss: 160.40029907226562\n","    reward loss: 2.6641287803649902\n","    kl loss: 3.2775440216064453\n","Episode: 463\n","    Loss: 292.0650939941406\n","    reconstruction loss: 156.88934326171875\n","    reward loss: 2.9801700115203857\n","    kl loss: 3.086977005004883\n","Episode: 464\n","    Loss: 284.0973815917969\n","    reconstruction loss: 154.08778381347656\n","    reward loss: 2.7344141006469727\n","    kl loss: 3.4305126667022705\n","Episode: 465\n","    Loss: 297.1039733886719\n","    reconstruction loss: 161.0894012451172\n","    reward loss: 2.8671998977661133\n","    kl loss: 3.5662569999694824\n","Episode: 466\n","    Loss: 285.0933837890625\n","    reconstruction loss: 152.5948486328125\n","    reward loss: 2.84911847114563\n","    kl loss: 3.2779386043548584\n","Episode: 467\n","    Loss: 268.447998046875\n","    reconstruction loss: 146.95822143554688\n","    reward loss: 2.641728162765503\n","    kl loss: 2.902930736541748\n","Episode: 468\n","    Loss: 265.2796936035156\n","    reconstruction loss: 148.5847625732422\n","    reward loss: 2.480604410171509\n","    kl loss: 2.987377643585205\n","Episode: 469\n","    Loss: 264.0713195800781\n","    reconstruction loss: 147.75503540039062\n","    reward loss: 2.509021282196045\n","    kl loss: 2.850055694580078\n","Episode: 470\n","    Loss: 273.6618957519531\n","    reconstruction loss: 152.34109497070312\n","    reward loss: 2.6197469234466553\n","    kl loss: 2.9629673957824707\n","Episode: 471\n","    Loss: 275.227783203125\n","    reconstruction loss: 154.18711853027344\n","    reward loss: 2.545560359954834\n","    kl loss: 3.1946053504943848\n","Episode: 472\n","    Loss: 253.77017211914062\n","    reconstruction loss: 141.49520874023438\n","    reward loss: 2.297438859939575\n","    kl loss: 3.1864585876464844\n","Episode: 473\n","    Loss: 265.4383239746094\n","    reconstruction loss: 144.87625122070312\n","    reward loss: 2.62585711479187\n","    kl loss: 2.865708112716675\n","Episode: 474\n","    Loss: 257.0521545410156\n","    reconstruction loss: 146.019775390625\n","    reward loss: 2.4095311164855957\n","    kl loss: 2.6698787212371826\n","Episode: 475\n","    Loss: 273.4018859863281\n","    reconstruction loss: 149.4272003173828\n","    reward loss: 2.696272134780884\n","    kl loss: 2.9605157375335693\n","Episode: 476\n","    Loss: 273.842041015625\n","    reconstruction loss: 149.65902709960938\n","    reward loss: 2.6673715114593506\n","    kl loss: 3.08250093460083\n","Episode: 477\n","    Loss: 266.0421142578125\n","    reconstruction loss: 149.2203369140625\n","    reward loss: 2.475450277328491\n","    kl loss: 3.018101453781128\n","Episode: 478\n","    Loss: 263.98492431640625\n","    reconstruction loss: 152.52752685546875\n","    reward loss: 2.4350950717926025\n","    kl loss: 2.622908115386963\n","Episode: 479\n","    Loss: 296.0411376953125\n","    reconstruction loss: 157.21255493164062\n","    reward loss: 3.1206963062286377\n","    kl loss: 2.960423231124878\n","Episode: 480\n","    Loss: 266.91424560546875\n","    reconstruction loss: 144.021240234375\n","    reward loss: 2.603083372116089\n","    kl loss: 3.178508758544922\n","Episode: 481\n","    Loss: 273.568115234375\n","    reconstruction loss: 148.1790771484375\n","    reward loss: 2.7657454013824463\n","    kl loss: 2.8587961196899414\n","Episode: 482\n","    Loss: 299.25592041015625\n","    reconstruction loss: 160.79776000976562\n","    reward loss: 3.044618844985962\n","    kl loss: 3.189649820327759\n","Episode: 483\n","    Loss: 288.405029296875\n","    reconstruction loss: 158.07752990722656\n","    reward loss: 2.7990572452545166\n","    kl loss: 3.2360494136810303\n","Episode: 484\n","    Loss: 281.9615783691406\n","    reconstruction loss: 150.87754821777344\n","    reward loss: 2.8679888248443604\n","    kl loss: 3.070441246032715\n","Episode: 485\n","    Loss: 281.2882995605469\n","    reconstruction loss: 150.52581787109375\n","    reward loss: 2.919013500213623\n","    kl loss: 2.8597023487091064\n","Episode: 486\n","    Loss: 283.3385314941406\n","    reconstruction loss: 156.86178588867188\n","    reward loss: 2.687445640563965\n","    kl loss: 3.241614818572998\n","Episode: 487\n","    Loss: 272.77203369140625\n","    reconstruction loss: 149.61911010742188\n","    reward loss: 2.6981756687164307\n","    kl loss: 2.871676445007324\n","Episode: 488\n","    Loss: 280.8992614746094\n","    reconstruction loss: 146.79318237304688\n","    reward loss: 2.9799141883850098\n","    kl loss: 2.980907678604126\n","Episode: 489\n","    Loss: 281.7863464355469\n","    reconstruction loss: 144.3565216064453\n","    reward loss: 3.113626003265381\n","    kl loss: 2.8452913761138916\n","Episode: 490\n","    Loss: 267.2745056152344\n","    reconstruction loss: 148.3060302734375\n","    reward loss: 2.639752149581909\n","    kl loss: 2.6577138900756836\n","Episode: 491\n","    Loss: 292.30340576171875\n","    reconstruction loss: 155.79025268554688\n","    reward loss: 3.0520284175872803\n","    kl loss: 2.9692161083221436\n","Episode: 492\n","    Loss: 271.3899841308594\n","    reconstruction loss: 154.13592529296875\n","    reward loss: 2.580650806427002\n","    kl loss: 2.693128824234009\n","Episode: 493\n","    Loss: 281.08001708984375\n","    reconstruction loss: 150.2611541748047\n","    reward loss: 2.9440982341766357\n","    kl loss: 2.77754282951355\n","Episode: 494\n","    Loss: 258.8077697753906\n","    reconstruction loss: 145.93727111816406\n","    reward loss: 2.4524073600769043\n","    kl loss: 2.703622817993164\n","Episode: 495\n","    Loss: 266.67987060546875\n","    reconstruction loss: 146.1685028076172\n","    reward loss: 2.6168670654296875\n","    kl loss: 2.8921027183532715\n","Episode: 496\n","    Loss: 265.7801208496094\n","    reconstruction loss: 151.9973602294922\n","    reward loss: 2.4123904705047607\n","    kl loss: 2.9349100589752197\n","Episode: 497\n","    Loss: 257.947509765625\n","    reconstruction loss: 147.61373901367188\n","    reward loss: 2.366917133331299\n","    kl loss: 2.7491683959960938\n","Episode: 498\n","    Loss: 293.08416748046875\n","    reconstruction loss: 150.71890258789062\n","    reward loss: 3.102811098098755\n","    kl loss: 3.3766891956329346\n","Episode: 499\n","    Loss: 275.2167053222656\n","    reconstruction loss: 155.768310546875\n","    reward loss: 2.578185558319092\n","    kl loss: 2.921190023422241\n","Episode: 500\n","    Loss: 278.8233642578125\n","    reconstruction loss: 150.53147888183594\n","    reward loss: 2.7708327770233154\n","    kl loss: 3.1312737464904785\n","Episode: 501\n","    Loss: 260.1619567871094\n","    reconstruction loss: 149.78555297851562\n","    reward loss: 2.3088417053222656\n","    kl loss: 2.9566946029663086\n","Episode: 502\n","    Loss: 267.96783447265625\n","    reconstruction loss: 144.00930786132812\n","    reward loss: 2.715704917907715\n","    kl loss: 2.8908851146698\n","Episode: 503\n","    Loss: 267.29052734375\n","    reconstruction loss: 147.34259033203125\n","    reward loss: 2.483287811279297\n","    kl loss: 3.3032875061035156\n","Episode: 504\n","    Loss: 284.7118225097656\n","    reconstruction loss: 150.77081298828125\n","    reward loss: 2.887529134750366\n","    kl loss: 3.2877495288848877\n","Episode: 505\n","    Loss: 295.0377502441406\n","    reconstruction loss: 156.40432739257812\n","    reward loss: 3.0465731620788574\n","    kl loss: 3.2003352642059326\n","Episode: 506\n","    Loss: 292.2420654296875\n","    reconstruction loss: 151.4609832763672\n","    reward loss: 3.0740580558776855\n","    kl loss: 3.3189055919647217\n","Episode: 507\n","    Loss: 285.775634765625\n","    reconstruction loss: 158.59127807617188\n","    reward loss: 2.807835817337036\n","    kl loss: 2.891007900238037\n","Episode: 508\n","    Loss: 287.49261474609375\n","    reconstruction loss: 156.30517578125\n","    reward loss: 2.8222477436065674\n","    kl loss: 3.2408761978149414\n","Episode: 509\n","    Loss: 259.4137878417969\n","    reconstruction loss: 140.22195434570312\n","    reward loss: 2.5153026580810547\n","    kl loss: 3.1156246662139893\n","Episode: 510\n","    Loss: 290.71270751953125\n","    reconstruction loss: 153.5228271484375\n","    reward loss: 2.885676860809326\n","    kl loss: 3.6191177368164062\n","Episode: 511\n","    Loss: 265.6767272949219\n","    reconstruction loss: 145.0036163330078\n","    reward loss: 2.5842175483703613\n","    kl loss: 3.022550344467163\n","Episode: 512\n","    Loss: 286.2359619140625\n","    reconstruction loss: 153.4378662109375\n","    reward loss: 2.8388586044311523\n","    kl loss: 3.343803644180298\n","Episode: 513\n","    Loss: 266.3250427246094\n","    reconstruction loss: 142.75148010253906\n","    reward loss: 2.6309075355529785\n","    kl loss: 3.149179220199585\n","Episode: 514\n","    Loss: 260.163818359375\n","    reconstruction loss: 150.16844177246094\n","    reward loss: 2.2180917263031006\n","    kl loss: 3.236215591430664\n","Episode: 515\n","    Loss: 281.2666015625\n","    reconstruction loss: 144.3058624267578\n","    reward loss: 2.903568983078003\n","    kl loss: 3.5335805416107178\n","Episode: 516\n","    Loss: 275.0299377441406\n","    reconstruction loss: 146.87274169921875\n","    reward loss: 2.672823429107666\n","    kl loss: 3.4608376026153564\n","Episode: 517\n","    Loss: 272.1003723144531\n","    reconstruction loss: 139.61697387695312\n","    reward loss: 2.8200981616973877\n","    kl loss: 3.377997398376465\n","Episode: 518\n","    Loss: 268.791748046875\n","    reconstruction loss: 144.9268035888672\n","    reward loss: 2.607429027557373\n","    kl loss: 3.2604925632476807\n","Episode: 519\n","    Loss: 279.82513427734375\n","    reconstruction loss: 146.926513671875\n","    reward loss: 2.885646343231201\n","    kl loss: 3.190098524093628\n","Episode: 520\n","    Loss: 260.4714660644531\n","    reconstruction loss: 139.8829803466797\n","    reward loss: 2.6021525859832764\n","    kl loss: 2.9513161182403564\n","Episode: 521\n","    Loss: 268.5608215332031\n","    reconstruction loss: 152.48553466796875\n","    reward loss: 2.477646827697754\n","    kl loss: 2.9357657432556152\n","Episode: 522\n","    Loss: 271.4850158691406\n","    reconstruction loss: 141.7827606201172\n","    reward loss: 2.809614419937134\n","    kl loss: 3.136575698852539\n","Episode: 523\n","    Loss: 287.82196044921875\n","    reconstruction loss: 147.31015014648438\n","    reward loss: 2.9966087341308594\n","    kl loss: 3.5630500316619873\n","Episode: 524\n","    Loss: 271.6239929199219\n","    reconstruction loss: 148.08033752441406\n","    reward loss: 2.630497932434082\n","    kl loss: 3.1476237773895264\n","Episode: 525\n","    Loss: 293.9907531738281\n","    reconstruction loss: 150.89468383789062\n","    reward loss: 3.0720326900482178\n","    kl loss: 3.557493209838867\n","Episode: 526\n","    Loss: 261.9898986816406\n","    reconstruction loss: 143.5223388671875\n","    reward loss: 2.4943647384643555\n","    kl loss: 3.11647891998291\n","Episode: 527\n","    Loss: 270.6273498535156\n","    reconstruction loss: 144.2384033203125\n","    reward loss: 2.699162006378174\n","    kl loss: 3.1918270587921143\n","Episode: 528\n","    Loss: 283.1699523925781\n","    reconstruction loss: 153.13584899902344\n","    reward loss: 2.6953988075256348\n","    kl loss: 3.5695135593414307\n","Episode: 529\n","    Loss: 271.9656982421875\n","    reconstruction loss: 145.39468383789062\n","    reward loss: 2.636822462081909\n","    kl loss: 3.4282238483428955\n","Episode: 530\n","    Loss: 275.762451171875\n","    reconstruction loss: 145.95240783691406\n","    reward loss: 2.816128730773926\n","    kl loss: 3.1245551109313965\n","Episode: 531\n","    Loss: 285.397216796875\n","    reconstruction loss: 147.19931030273438\n","    reward loss: 2.9495131969451904\n","    kl loss: 3.4964945316314697\n","Episode: 532\n","    Loss: 276.4715576171875\n","    reconstruction loss: 150.5443115234375\n","    reward loss: 2.678199291229248\n","    kl loss: 3.219028949737549\n","Episode: 533\n","    Loss: 261.3902282714844\n","    reconstruction loss: 143.94073486328125\n","    reward loss: 2.4560632705688477\n","    kl loss: 3.14872670173645\n","Episode: 534\n","    Loss: 275.96075439453125\n","    reconstruction loss: 149.2036590576172\n","    reward loss: 2.6782591342926025\n","    kl loss: 3.3018033504486084\n","Episode: 535\n","    Loss: 277.6026916503906\n","    reconstruction loss: 143.59671020507812\n","    reward loss: 2.843984365463257\n","    kl loss: 3.4466514587402344\n","Episode: 536\n","    Loss: 276.9847106933594\n","    reconstruction loss: 143.42401123046875\n","    reward loss: 2.775942325592041\n","    kl loss: 3.6402699947357178\n","Episode: 537\n","    Loss: 285.7101135253906\n","    reconstruction loss: 146.4287109375\n","    reward loss: 2.95086669921875\n","    kl loss: 3.600106954574585\n","Episode: 538\n","    Loss: 267.5169372558594\n","    reconstruction loss: 142.056396484375\n","    reward loss: 2.627626895904541\n","    kl loss: 3.3493592739105225\n","Episode: 539\n","    Loss: 267.41796875\n","    reconstruction loss: 142.73919677734375\n","    reward loss: 2.6786916255950928\n","    kl loss: 3.092456817626953\n","Episode: 540\n","    Loss: 257.62530517578125\n","    reconstruction loss: 137.53741455078125\n","    reward loss: 2.455364227294922\n","    kl loss: 3.4150140285491943\n","Episode: 541\n","    Loss: 276.1119384765625\n","    reconstruction loss: 139.3421630859375\n","    reward loss: 3.038682222366333\n","    kl loss: 3.04158878326416\n","Episode: 542\n","    Loss: 287.23590087890625\n","    reconstruction loss: 149.79312133789062\n","    reward loss: 2.928431272506714\n","    kl loss: 3.4947688579559326\n","Episode: 543\n","    Loss: 275.2818298339844\n","    reconstruction loss: 153.6915740966797\n","    reward loss: 2.604339599609375\n","    kl loss: 3.043836832046509\n","Episode: 544\n","    Loss: 267.48272705078125\n","    reconstruction loss: 140.34329223632812\n","    reward loss: 2.6667320728302\n","    kl loss: 3.3803799152374268\n","Episode: 545\n","    Loss: 280.48291015625\n","    reconstruction loss: 147.01234436035156\n","    reward loss: 2.7921879291534424\n","    kl loss: 3.574399471282959\n","Episode: 546\n","    Loss: 241.99346923828125\n","    reconstruction loss: 134.25985717773438\n","    reward loss: 2.21468448638916\n","    kl loss: 3.0219664573669434\n","Episode: 547\n","    Loss: 280.37677001953125\n","    reconstruction loss: 147.77011108398438\n","    reward loss: 2.81496262550354\n","    kl loss: 3.4082984924316406\n","Episode: 548\n","    Loss: 275.682373046875\n","    reconstruction loss: 149.0112762451172\n","    reward loss: 2.6380465030670166\n","    kl loss: 3.4339487552642822\n","Episode: 549\n","    Loss: 269.78662109375\n","    reconstruction loss: 140.60264587402344\n","    reward loss: 2.6383450031280518\n","    kl loss: 3.6841893196105957\n","Episode: 550\n","    Loss: 281.27587890625\n","    reconstruction loss: 141.32742309570312\n","    reward loss: 3.0365538597106934\n","    kl loss: 3.3669075965881348\n","Episode: 551\n","    Loss: 268.37841796875\n","    reconstruction loss: 142.04586791992188\n","    reward loss: 2.6157610416412354\n","    kl loss: 3.478092670440674\n","Episode: 552\n","    Loss: 237.99017333984375\n","    reconstruction loss: 133.79379272460938\n","    reward loss: 2.154378890991211\n","    kl loss: 2.879312515258789\n","Episode: 553\n","    Loss: 266.3912353515625\n","    reconstruction loss: 140.23953247070312\n","    reward loss: 2.6851696968078613\n","    kl loss: 3.2170748710632324\n","Episode: 554\n","    Loss: 259.1119384765625\n","    reconstruction loss: 142.69842529296875\n","    reward loss: 2.330883026123047\n","    kl loss: 3.4832613468170166\n","Episode: 555\n","    Loss: 284.66094970703125\n","    reconstruction loss: 148.33279418945312\n","    reward loss: 2.806682586669922\n","    kl loss: 3.8094279766082764\n","Episode: 556\n","    Loss: 249.08480834960938\n","    reconstruction loss: 135.4912567138672\n","    reward loss: 2.277742624282837\n","    kl loss: 3.3872556686401367\n","Episode: 557\n","    Loss: 271.0982666015625\n","    reconstruction loss: 141.18820190429688\n","    reward loss: 2.855172634124756\n","    kl loss: 2.9979007244110107\n","Episode: 558\n","    Loss: 269.75982666015625\n","    reconstruction loss: 146.60427856445312\n","    reward loss: 2.5120649337768555\n","    kl loss: 3.5233278274536133\n","Episode: 559\n","    Loss: 257.4173583984375\n","    reconstruction loss: 138.09454345703125\n","    reward loss: 2.41818904876709\n","    kl loss: 3.4686174392700195\n","Episode: 560\n","    Loss: 263.07745361328125\n","    reconstruction loss: 143.05564880371094\n","    reward loss: 2.5344884395599365\n","    kl loss: 3.131472110748291\n","Episode: 561\n","    Loss: 300.7880859375\n","    reconstruction loss: 154.42056274414062\n","    reward loss: 3.048173666000366\n","    kl loss: 3.9681472778320312\n","Episode: 562\n","    Loss: 262.3792724609375\n","    reconstruction loss: 138.393310546875\n","    reward loss: 2.511500835418701\n","    kl loss: 3.608342409133911\n","Episode: 563\n","    Loss: 257.0280456542969\n","    reconstruction loss: 139.4609375\n","    reward loss: 2.3052139282226562\n","    kl loss: 3.688462495803833\n","Episode: 564\n","    Loss: 241.33856201171875\n","    reconstruction loss: 139.34613037109375\n","    reward loss: 2.0191521644592285\n","    kl loss: 3.132211446762085\n","Episode: 565\n","    Loss: 277.41815185546875\n","    reconstruction loss: 142.19508361816406\n","    reward loss: 2.738041400909424\n","    kl loss: 3.9391627311706543\n","Episode: 566\n","    Loss: 268.7105712890625\n","    reconstruction loss: 141.63043212890625\n","    reward loss: 2.507401943206787\n","    kl loss: 3.9321069717407227\n","Episode: 567\n","    Loss: 266.0109558105469\n","    reconstruction loss: 140.76954650878906\n","    reward loss: 2.526240587234497\n","    kl loss: 3.682300090789795\n","Episode: 568\n","    Loss: 284.15155029296875\n","    reconstruction loss: 148.02377319335938\n","    reward loss: 2.712428569793701\n","    kl loss: 4.119279384613037\n","Episode: 569\n","    Loss: 284.2815856933594\n","    reconstruction loss: 146.45143127441406\n","    reward loss: 2.856539249420166\n","    kl loss: 3.7851290702819824\n","Episode: 570\n","    Loss: 283.79656982421875\n","    reconstruction loss: 145.96295166015625\n","    reward loss: 2.8579046726226807\n","    kl loss: 3.780695676803589\n","Episode: 571\n","    Loss: 287.6293640136719\n","    reconstruction loss: 156.274169921875\n","    reward loss: 2.7229936122894287\n","    kl loss: 3.6050422191619873\n","Episode: 572\n","    Loss: 265.2278137207031\n","    reconstruction loss: 139.22531127929688\n","    reward loss: 2.577125072479248\n","    kl loss: 3.5803136825561523\n","Episode: 573\n","    Loss: 292.11474609375\n","    reconstruction loss: 142.01280212402344\n","    reward loss: 3.2241673469543457\n","    kl loss: 3.725609064102173\n","Episode: 574\n","    Loss: 252.443603515625\n","    reconstruction loss: 135.959716796875\n","    reward loss: 2.402616024017334\n","    kl loss: 3.2392325401306152\n","Episode: 575\n","    Loss: 267.1307373046875\n","    reconstruction loss: 144.93710327148438\n","    reward loss: 2.5467216968536377\n","    kl loss: 3.3058390617370605\n","Episode: 576\n","    Loss: 258.0205383300781\n","    reconstruction loss: 135.45993041992188\n","    reward loss: 2.5459649562835693\n","    kl loss: 3.3451833724975586\n","Episode: 577\n","    Loss: 275.74090576171875\n","    reconstruction loss: 148.89230346679688\n","    reward loss: 2.66385555267334\n","    kl loss: 3.36136531829834\n","Episode: 578\n","    Loss: 262.3460693359375\n","    reconstruction loss: 143.2301025390625\n","    reward loss: 2.4906845092773438\n","    kl loss: 3.194201946258545\n","Episode: 579\n","    Loss: 258.45404052734375\n","    reconstruction loss: 147.4327392578125\n","    reward loss: 2.267500638961792\n","    kl loss: 3.1658787727355957\n","Episode: 580\n","    Loss: 264.5527038574219\n","    reconstruction loss: 139.40133666992188\n","    reward loss: 2.6301403045654297\n","    kl loss: 3.3096468448638916\n","Episode: 581\n","    Loss: 268.62432861328125\n","    reconstruction loss: 139.77407836914062\n","    reward loss: 2.705540180206299\n","    kl loss: 3.415635347366333\n","Episode: 582\n","    Loss: 256.77569580078125\n","    reconstruction loss: 143.81362915039062\n","    reward loss: 2.3073971271514893\n","    kl loss: 3.220318078994751\n","Episode: 583\n","    Loss: 267.85498046875\n","    reconstruction loss: 144.83421325683594\n","    reward loss: 2.556260585784912\n","    kl loss: 3.3551642894744873\n","Episode: 584\n","    Loss: 259.59368896484375\n","    reconstruction loss: 142.45037841796875\n","    reward loss: 2.3778042793273926\n","    kl loss: 3.3920183181762695\n","Episode: 585\n","    Loss: 257.74365234375\n","    reconstruction loss: 140.353271484375\n","    reward loss: 2.4185144901275635\n","    kl loss: 3.274238348007202\n","Episode: 586\n","    Loss: 266.33843994140625\n","    reconstruction loss: 136.1627197265625\n","    reward loss: 2.8993067741394043\n","    kl loss: 2.869997024536133\n","Episode: 587\n","    Loss: 258.5851135253906\n","    reconstruction loss: 137.4910888671875\n","    reward loss: 2.5566835403442383\n","    kl loss: 3.1610095500946045\n","Episode: 588\n","    Loss: 287.8030700683594\n","    reconstruction loss: 139.351318359375\n","    reward loss: 3.286240816116333\n","    kl loss: 3.3433315753936768\n","Episode: 589\n","    Loss: 251.89593505859375\n","    reconstruction loss: 139.64422607421875\n","    reward loss: 2.2712059020996094\n","    kl loss: 3.2759511470794678\n","Episode: 590\n","    Loss: 253.33950805664062\n","    reconstruction loss: 138.21209716796875\n","    reward loss: 2.4547157287597656\n","    kl loss: 2.9212348461151123\n","Episode: 591\n","    Loss: 279.43817138671875\n","    reconstruction loss: 151.78973388671875\n","    reward loss: 2.518632650375366\n","    kl loss: 3.9496283531188965\n","Episode: 592\n","    Loss: 268.5089416503906\n","    reconstruction loss: 146.6690216064453\n","    reward loss: 2.492523670196533\n","    kl loss: 3.4601597785949707\n","Episode: 593\n","    Loss: 273.3352355957031\n","    reconstruction loss: 146.54469299316406\n","    reward loss: 2.658677339553833\n","    kl loss: 3.373685359954834\n","Episode: 594\n","    Loss: 276.73876953125\n","    reconstruction loss: 148.08409118652344\n","    reward loss: 2.7587172985076904\n","    kl loss: 3.2099568843841553\n","Episode: 595\n","    Loss: 256.87274169921875\n","    reconstruction loss: 142.1488494873047\n","    reward loss: 2.3309402465820312\n","    kl loss: 3.314100503921509\n","Episode: 596\n","    Loss: 273.5960388183594\n","    reconstruction loss: 141.88311767578125\n","    reward loss: 2.7441186904907227\n","    kl loss: 3.566875457763672\n","Episode: 597\n","    Loss: 267.5782165527344\n","    reconstruction loss: 140.60614013671875\n","    reward loss: 2.5931005477905273\n","    kl loss: 3.62135648727417\n","Episode: 598\n","    Loss: 257.79595947265625\n","    reconstruction loss: 137.379150390625\n","    reward loss: 2.547205686569214\n","    kl loss: 3.126462459564209\n","Episode: 599\n","    Loss: 257.4608459472656\n","    reconstruction loss: 137.0860595703125\n","    reward loss: 2.494904041290283\n","    kl loss: 3.305314540863037\n","Episode: 600\n","    Loss: 262.6936950683594\n","    reconstruction loss: 135.13807678222656\n","    reward loss: 2.648688316345215\n","    kl loss: 3.485152006149292\n","Episode: 601\n","    Loss: 260.15899658203125\n","    reconstruction loss: 146.54446411132812\n","    reward loss: 2.280473470687866\n","    kl loss: 3.3797972202301025\n","Episode: 602\n","    Loss: 269.9930725097656\n","    reconstruction loss: 137.1273193359375\n","    reward loss: 2.7952022552490234\n","    kl loss: 3.503366708755493\n","Episode: 603\n","    Loss: 280.71820068359375\n","    reconstruction loss: 137.55142211914062\n","    reward loss: 3.0266826152801514\n","    kl loss: 3.7232863903045654\n","Episode: 604\n","    Loss: 289.32666015625\n","    reconstruction loss: 143.5429229736328\n","    reward loss: 2.95064115524292\n","    kl loss: 4.251128673553467\n","Episode: 605\n","    Loss: 281.28656005859375\n","    reconstruction loss: 146.47021484375\n","    reward loss: 2.7679922580718994\n","    kl loss: 3.793661594390869\n","Episode: 606\n","    Loss: 286.24176025390625\n","    reconstruction loss: 138.79383850097656\n","    reward loss: 3.0805206298828125\n","    kl loss: 3.9629693031311035\n","Episode: 607\n","    Loss: 237.14895629882812\n","    reconstruction loss: 133.43289184570312\n","    reward loss: 2.0502068996429443\n","    kl loss: 3.195882797241211\n","Episode: 608\n","    Loss: 259.0655517578125\n","    reconstruction loss: 140.13262939453125\n","    reward loss: 2.4338085651397705\n","    kl loss: 3.3749635219573975\n","Episode: 609\n","    Loss: 262.1384582519531\n","    reconstruction loss: 136.09632873535156\n","    reward loss: 2.625720262527466\n","    kl loss: 3.4141926765441895\n","Episode: 610\n","    Loss: 258.6571044921875\n","    reconstruction loss: 135.91249084472656\n","    reward loss: 2.571887731552124\n","    kl loss: 3.2728562355041504\n","Episode: 611\n","    Loss: 257.63970947265625\n","    reconstruction loss: 131.36012268066406\n","    reward loss: 2.485950231552124\n","    kl loss: 3.927133798599243\n","Episode: 612\n","    Loss: 265.5534973144531\n","    reconstruction loss: 136.6688232421875\n","    reward loss: 2.6738414764404297\n","    kl loss: 3.5300230979919434\n","Episode: 613\n","    Loss: 253.41842651367188\n","    reconstruction loss: 130.85963439941406\n","    reward loss: 2.5478227138519287\n","    kl loss: 3.338498830795288\n","Episode: 614\n","    Loss: 284.5819091796875\n","    reconstruction loss: 142.9250946044922\n","    reward loss: 2.9556705951690674\n","    kl loss: 3.820835828781128\n","Episode: 615\n","    Loss: 280.3467102050781\n","    reconstruction loss: 140.55433654785156\n","    reward loss: 2.9519450664520264\n","    kl loss: 3.6474294662475586\n","Episode: 616\n","    Loss: 259.03326416015625\n","    reconstruction loss: 142.998046875\n","    reward loss: 2.327632427215576\n","    kl loss: 3.456808567047119\n","Episode: 617\n","    Loss: 260.6876525878906\n","    reconstruction loss: 135.86013793945312\n","    reward loss: 2.5633015632629395\n","    kl loss: 3.511197805404663\n","Episode: 618\n","    Loss: 261.69940185546875\n","    reconstruction loss: 129.9833526611328\n","    reward loss: 2.8285655975341797\n","    kl loss: 3.2716243267059326\n","Episode: 619\n","    Loss: 262.76129150390625\n","    reconstruction loss: 137.49822998046875\n","    reward loss: 2.5416176319122314\n","    kl loss: 3.6306445598602295\n","Episode: 620\n","    Loss: 244.11514282226562\n","    reconstruction loss: 131.65399169921875\n","    reward loss: 2.2270829677581787\n","    kl loss: 3.451324224472046\n","Episode: 621\n","    Loss: 268.0438232421875\n","    reconstruction loss: 140.9697723388672\n","    reward loss: 2.5912041664123535\n","    kl loss: 3.6381888389587402\n","Episode: 622\n","    Loss: 267.0769958496094\n","    reconstruction loss: 139.10079956054688\n","    reward loss: 2.6649069786071777\n","    kl loss: 3.4704461097717285\n","Episode: 623\n","    Loss: 271.25311279296875\n","    reconstruction loss: 145.190673828125\n","    reward loss: 2.548190116882324\n","    kl loss: 3.6875786781311035\n","Episode: 624\n","    Loss: 255.84974670410156\n","    reconstruction loss: 135.2954864501953\n","    reward loss: 2.48069429397583\n","    kl loss: 3.3729970455169678\n","Episode: 625\n","    Loss: 272.03472900390625\n","    reconstruction loss: 142.17129516601562\n","    reward loss: 2.604733943939209\n","    kl loss: 3.869774103164673\n","Episode: 626\n","    Loss: 251.41697692871094\n","    reconstruction loss: 139.89649963378906\n","    reward loss: 2.237818956375122\n","    kl loss: 3.3196823596954346\n","Episode: 627\n","    Loss: 260.1850280761719\n","    reconstruction loss: 139.8670654296875\n","    reward loss: 2.3713274002075195\n","    kl loss: 3.732150077819824\n","Episode: 628\n","    Loss: 270.3294372558594\n","    reconstruction loss: 139.0594482421875\n","    reward loss: 2.7369542121887207\n","    kl loss: 3.547658681869507\n","Episode: 629\n","    Loss: 248.23390197753906\n","    reconstruction loss: 138.42437744140625\n","    reward loss: 2.1637120246887207\n","    kl loss: 3.4079601764678955\n","Episode: 630\n","    Loss: 267.9189758300781\n","    reconstruction loss: 139.72024536132812\n","    reward loss: 2.6919703483581543\n","    kl loss: 3.397977590560913\n","Episode: 631\n","    Loss: 261.1015319824219\n","    reconstruction loss: 139.0868377685547\n","    reward loss: 2.424126148223877\n","    kl loss: 3.7170286178588867\n","Episode: 632\n","    Loss: 257.93927001953125\n","    reconstruction loss: 136.0970001220703\n","    reward loss: 2.4835205078125\n","    kl loss: 3.4919044971466064\n","Episode: 633\n","    Loss: 252.0504608154297\n","    reconstruction loss: 133.1516876220703\n","    reward loss: 2.4548606872558594\n","    kl loss: 3.297865152359009\n","Episode: 634\n","    Loss: 258.4183654785156\n","    reconstruction loss: 135.37667846679688\n","    reward loss: 2.506031036376953\n","    kl loss: 3.5330586433410645\n","Episode: 635\n","    Loss: 271.195556640625\n","    reconstruction loss: 138.14794921875\n","    reward loss: 2.7244081497192383\n","    kl loss: 3.769331455230713\n","Episode: 636\n","    Loss: 239.1437225341797\n","    reconstruction loss: 131.45266723632812\n","    reward loss: 2.1772987842559814\n","    kl loss: 3.148559331893921\n","Episode: 637\n","    Loss: 255.62364196777344\n","    reconstruction loss: 137.26422119140625\n","    reward loss: 2.377032995223999\n","    kl loss: 3.516326665878296\n","Episode: 638\n","    Loss: 276.4521484375\n","    reconstruction loss: 140.6158447265625\n","    reward loss: 2.8330636024475098\n","    kl loss: 3.6679065227508545\n","Episode: 639\n","    Loss: 253.25970458984375\n","    reconstruction loss: 138.63858032226562\n","    reward loss: 2.3217663764953613\n","    kl loss: 3.335930109024048\n","Episode: 640\n","    Loss: 233.03988647460938\n","    reconstruction loss: 125.92200469970703\n","    reward loss: 2.1091463565826416\n","    kl loss: 3.329775333404541\n","Episode: 641\n","    Loss: 260.49017333984375\n","    reconstruction loss: 142.03631591796875\n","    reward loss: 2.3412492275238037\n","    kl loss: 3.651012659072876\n","Episode: 642\n","    Loss: 238.98275756835938\n","    reconstruction loss: 132.6614532470703\n","    reward loss: 2.037247657775879\n","    kl loss: 3.501763343811035\n","Episode: 643\n","    Loss: 258.427490234375\n","    reconstruction loss: 137.05258178710938\n","    reward loss: 2.3984313011169434\n","    kl loss: 3.74298095703125\n","Episode: 644\n","    Loss: 261.6224365234375\n","    reconstruction loss: 135.48043823242188\n","    reward loss: 2.5605897903442383\n","    kl loss: 3.6521365642547607\n","Episode: 645\n","    Loss: 247.94032287597656\n","    reconstruction loss: 133.3862762451172\n","    reward loss: 2.2437243461608887\n","    kl loss: 3.602369546890259\n","Episode: 646\n","    Loss: 255.3727264404297\n","    reconstruction loss: 134.35556030273438\n","    reward loss: 2.3629844188690186\n","    kl loss: 3.831271171569824\n","Episode: 647\n","    Loss: 253.037841796875\n","    reconstruction loss: 127.28254699707031\n","    reward loss: 2.5249295234680176\n","    kl loss: 3.7382748126983643\n","Episode: 648\n","    Loss: 269.10400390625\n","    reconstruction loss: 138.9527130126953\n","    reward loss: 2.6763017177581787\n","    kl loss: 3.648073196411133\n","Episode: 649\n","    Loss: 258.5240783691406\n","    reconstruction loss: 137.23622131347656\n","    reward loss: 2.408754348754883\n","    kl loss: 3.6981453895568848\n","Episode: 650\n","    Loss: 270.2520751953125\n","    reconstruction loss: 145.11212158203125\n","    reward loss: 2.512723684310913\n","    kl loss: 3.71946382522583\n","Episode: 651\n","    Loss: 251.7674560546875\n","    reconstruction loss: 137.60415649414062\n","    reward loss: 2.2463979721069336\n","    kl loss: 3.55393648147583\n","Episode: 652\n","    Loss: 272.05926513671875\n","    reconstruction loss: 142.5098876953125\n","    reward loss: 2.5821971893310547\n","    kl loss: 3.9172468185424805\n","Episode: 653\n","    Loss: 268.9938659667969\n","    reconstruction loss: 143.90948486328125\n","    reward loss: 2.4997105598449707\n","    kl loss: 3.759450674057007\n","Episode: 654\n","    Loss: 264.06268310546875\n","    reconstruction loss: 138.10397338867188\n","    reward loss: 2.591071605682373\n","    kl loss: 3.527120351791382\n","Episode: 655\n","    Loss: 260.3196716308594\n","    reconstruction loss: 138.30621337890625\n","    reward loss: 2.3595786094665527\n","    kl loss: 3.9428207874298096\n","Episode: 656\n","    Loss: 264.87335205078125\n","    reconstruction loss: 134.1802978515625\n","    reward loss: 2.7005255222320557\n","    kl loss: 3.6174635887145996\n","Episode: 657\n","    Loss: 264.474609375\n","    reconstruction loss: 139.64932250976562\n","    reward loss: 2.498833417892456\n","    kl loss: 3.7366130352020264\n","Episode: 658\n","    Loss: 244.86337280273438\n","    reconstruction loss: 133.21914672851562\n","    reward loss: 2.187882900238037\n","    kl loss: 3.506833553314209\n","Episode: 659\n","    Loss: 240.7713623046875\n","    reconstruction loss: 127.1109619140625\n","    reward loss: 2.1808595657348633\n","    kl loss: 3.733032464981079\n","Episode: 660\n","    Loss: 259.68206787109375\n","    reconstruction loss: 135.50726318359375\n","    reward loss: 2.4210259914398193\n","    kl loss: 3.943892002105713\n","Episode: 661\n","    Loss: 250.20596313476562\n","    reconstruction loss: 127.51370239257812\n","    reward loss: 2.4855470657348633\n","    kl loss: 3.569812536239624\n","Episode: 662\n","    Loss: 257.3563537597656\n","    reconstruction loss: 141.09591674804688\n","    reward loss: 2.321676254272461\n","    kl loss: 3.500175714492798\n","Episode: 663\n","    Loss: 246.58306884765625\n","    reconstruction loss: 136.60162353515625\n","    reward loss: 2.1036925315856934\n","    kl loss: 3.635221004486084\n","Episode: 664\n","    Loss: 244.02320861816406\n","    reconstruction loss: 140.76129150390625\n","    reward loss: 2.043178081512451\n","    kl loss: 3.1750686168670654\n","Episode: 665\n","    Loss: 257.8636169433594\n","    reconstruction loss: 127.74593353271484\n","    reward loss: 2.711089611053467\n","    kl loss: 3.5229544639587402\n","Episode: 666\n","    Loss: 252.58319091796875\n","    reconstruction loss: 132.72555541992188\n","    reward loss: 2.39200496673584\n","    kl loss: 3.6137454509735107\n","Episode: 667\n","    Loss: 250.10264587402344\n","    reconstruction loss: 132.93234252929688\n","    reward loss: 2.2371761798858643\n","    kl loss: 3.8869142532348633\n","Episode: 668\n","    Loss: 267.61260986328125\n","    reconstruction loss: 136.39413452148438\n","    reward loss: 2.7005703449249268\n","    kl loss: 3.6698524951934814\n","Episode: 669\n","    Loss: 279.39306640625\n","    reconstruction loss: 141.08999633789062\n","    reward loss: 2.8593924045562744\n","    kl loss: 3.8224329948425293\n","Episode: 670\n","    Loss: 250.12979125976562\n","    reconstruction loss: 138.187744140625\n","    reward loss: 2.169367551803589\n","    kl loss: 3.601419687271118\n","Episode: 671\n","    Loss: 262.5964660644531\n","    reconstruction loss: 135.931884765625\n","    reward loss: 2.588279962539673\n","    kl loss: 3.6074790954589844\n","Episode: 672\n","    Loss: 275.4996337890625\n","    reconstruction loss: 135.98423767089844\n","    reward loss: 2.901859760284424\n","    kl loss: 3.7950289249420166\n","Episode: 673\n","    Loss: 245.29959106445312\n","    reconstruction loss: 132.3059844970703\n","    reward loss: 2.279480457305908\n","    kl loss: 3.321178674697876\n","Episode: 674\n","    Loss: 254.22120666503906\n","    reconstruction loss: 129.3089141845703\n","    reward loss: 2.5432968139648438\n","    kl loss: 3.5896899700164795\n","Episode: 675\n","    Loss: 265.1236572265625\n","    reconstruction loss: 133.948974609375\n","    reward loss: 2.6496834754943848\n","    kl loss: 3.8435773849487305\n","Episode: 676\n","    Loss: 252.8300323486328\n","    reconstruction loss: 126.34154510498047\n","    reward loss: 2.528883218765259\n","    kl loss: 3.7977585792541504\n","Episode: 677\n","    Loss: 253.01991271972656\n","    reconstruction loss: 136.03126525878906\n","    reward loss: 2.236232042312622\n","    kl loss: 3.872053623199463\n","Episode: 678\n","    Loss: 272.6119079589844\n","    reconstruction loss: 143.24171447753906\n","    reward loss: 2.5694668292999268\n","    kl loss: 3.9438862800598145\n","Episode: 679\n","    Loss: 261.56378173828125\n","    reconstruction loss: 137.72682189941406\n","    reward loss: 2.3890249729156494\n","    kl loss: 4.02210807800293\n","Episode: 680\n","    Loss: 283.5811767578125\n","    reconstruction loss: 145.05418395996094\n","    reward loss: 2.699486017227173\n","    kl loss: 4.4044976234436035\n","Episode: 681\n","    Loss: 260.6478271484375\n","    reconstruction loss: 135.50975036621094\n","    reward loss: 2.6658217906951904\n","    kl loss: 3.183432102203369\n","Episode: 682\n","    Loss: 258.3730773925781\n","    reconstruction loss: 138.51287841796875\n","    reward loss: 2.3782317638397217\n","    kl loss: 3.6622092723846436\n","Episode: 683\n","    Loss: 276.9726867675781\n","    reconstruction loss: 144.2680206298828\n","    reward loss: 2.6352407932281494\n","    kl loss: 4.047122955322266\n","Episode: 684\n","    Loss: 257.5668029785156\n","    reconstruction loss: 128.96685791015625\n","    reward loss: 2.620927572250366\n","    kl loss: 3.6867482662200928\n","Episode: 685\n","    Loss: 242.13502502441406\n","    reconstruction loss: 130.81317138671875\n","    reward loss: 2.1017138957977295\n","    kl loss: 3.776185989379883\n","Episode: 686\n","    Loss: 261.3553466796875\n","    reconstruction loss: 137.74533081054688\n","    reward loss: 2.5456786155700684\n","    kl loss: 3.451124668121338\n","Episode: 687\n","    Loss: 242.76205444335938\n","    reconstruction loss: 126.32948303222656\n","    reward loss: 2.268751859664917\n","    kl loss: 3.702625036239624\n","Episode: 688\n","    Loss: 257.587158203125\n","    reconstruction loss: 133.23526000976562\n","    reward loss: 2.4388344287872314\n","    kl loss: 3.8992674350738525\n","Episode: 689\n","    Loss: 275.8675537109375\n","    reconstruction loss: 141.73928833007812\n","    reward loss: 2.56852650642395\n","    kl loss: 4.422982215881348\n","Episode: 690\n","    Loss: 273.6947021484375\n","    reconstruction loss: 138.47796630859375\n","    reward loss: 2.7538042068481445\n","    kl loss: 3.8833580017089844\n","Episode: 691\n","    Loss: 265.3785095214844\n","    reconstruction loss: 137.1878662109375\n","    reward loss: 2.5671019554138184\n","    kl loss: 3.8342082500457764\n","Episode: 692\n","    Loss: 280.4748840332031\n","    reconstruction loss: 145.30865478515625\n","    reward loss: 2.647528648376465\n","    kl loss: 4.250271320343018\n","Episode: 693\n","    Loss: 263.6446533203125\n","    reconstruction loss: 136.53875732421875\n","    reward loss: 2.4547030925750732\n","    kl loss: 4.1191277503967285\n","Episode: 694\n","    Loss: 252.641845703125\n","    reconstruction loss: 133.98277282714844\n","    reward loss: 2.259765625\n","    kl loss: 3.956728458404541\n","Episode: 695\n","    Loss: 261.719970703125\n","    reconstruction loss: 127.73339080810547\n","    reward loss: 2.664606809616089\n","    kl loss: 4.072535514831543\n","Episode: 696\n","    Loss: 261.1900634765625\n","    reconstruction loss: 136.93035888671875\n","    reward loss: 2.306290864944458\n","    kl loss: 4.353950500488281\n","Episode: 697\n","    Loss: 270.52227783203125\n","    reconstruction loss: 131.06468200683594\n","    reward loss: 2.778097152709961\n","    kl loss: 4.222421169281006\n","Episode: 698\n","    Loss: 272.03692626953125\n","    reconstruction loss: 146.0722198486328\n","    reward loss: 2.4777369499206543\n","    kl loss: 3.924391508102417\n","Episode: 699\n","    Loss: 248.08123779296875\n","    reconstruction loss: 129.29412841796875\n","    reward loss: 2.26735520362854\n","    kl loss: 3.9429678916931152\n","Episode: 700\n","    Loss: 257.5555725097656\n","    reconstruction loss: 134.52403259277344\n","    reward loss: 2.3822972774505615\n","    kl loss: 3.965113878250122\n","Episode: 701\n","    Loss: 264.99920654296875\n","    reconstruction loss: 129.70291137695312\n","    reward loss: 2.5829906463623047\n","    kl loss: 4.48916482925415\n","Episode: 702\n","    Loss: 265.5285949707031\n","    reconstruction loss: 138.15350341796875\n","    reward loss: 2.495518684387207\n","    kl loss: 4.003192901611328\n","Episode: 703\n","    Loss: 257.4298095703125\n","    reconstruction loss: 133.33059692382812\n","    reward loss: 2.476773262023926\n","    kl loss: 3.741212844848633\n","Episode: 704\n","    Loss: 253.70242309570312\n","    reconstruction loss: 129.99246215820312\n","    reward loss: 2.448443651199341\n","    kl loss: 3.801442861557007\n","Episode: 705\n","    Loss: 255.7279510498047\n","    reconstruction loss: 132.30593872070312\n","    reward loss: 2.359602928161621\n","    kl loss: 4.083590507507324\n","Episode: 706\n","    Loss: 247.71377563476562\n","    reconstruction loss: 128.81236267089844\n","    reward loss: 2.2304913997650146\n","    kl loss: 4.0834221839904785\n","Episode: 707\n","    Loss: 246.42422485351562\n","    reconstruction loss: 128.10806274414062\n","    reward loss: 2.3451857566833496\n","    kl loss: 3.623464345932007\n","Episode: 708\n","    Loss: 246.63751220703125\n","    reconstruction loss: 127.39865112304688\n","    reward loss: 2.298868179321289\n","    kl loss: 3.877847909927368\n","Episode: 709\n","    Loss: 233.02499389648438\n","    reconstruction loss: 121.00991821289062\n","    reward loss: 2.1696226596832275\n","    kl loss: 3.6078269481658936\n","Episode: 710\n","    Loss: 250.24484252929688\n","    reconstruction loss: 123.93238830566406\n","    reward loss: 2.4438869953155518\n","    kl loss: 4.077640533447266\n","Episode: 711\n","    Loss: 250.72308349609375\n","    reconstruction loss: 129.442626953125\n","    reward loss: 2.3823158740997314\n","    kl loss: 3.7899389266967773\n","Episode: 712\n","    Loss: 262.29779052734375\n","    reconstruction loss: 134.2166748046875\n","    reward loss: 2.5700719356536865\n","    kl loss: 3.812861204147339\n","Episode: 713\n","    Loss: 253.14419555664062\n","    reconstruction loss: 130.93341064453125\n","    reward loss: 2.353844165802002\n","    kl loss: 3.982623815536499\n","Episode: 714\n","    Loss: 257.72076416015625\n","    reconstruction loss: 133.49945068359375\n","    reward loss: 2.3787500858306885\n","    kl loss: 4.096504211425781\n","Episode: 715\n","    Loss: 246.08352661132812\n","    reconstruction loss: 128.6907196044922\n","    reward loss: 2.2748196125030518\n","    kl loss: 3.7774133682250977\n","Episode: 716\n","    Loss: 244.1856689453125\n","    reconstruction loss: 133.51219177246094\n","    reward loss: 2.082801580429077\n","    kl loss: 3.77754282951355\n","Episode: 717\n","    Loss: 258.65765380859375\n","    reconstruction loss: 130.60702514648438\n","    reward loss: 2.505643606185913\n","    kl loss: 4.035311222076416\n","Episode: 718\n","    Loss: 246.5950927734375\n","    reconstruction loss: 129.87451171875\n","    reward loss: 2.255190134048462\n","    kl loss: 3.7788925170898438\n","Episode: 719\n","    Loss: 246.78970336914062\n","    reconstruction loss: 135.15335083007812\n","    reward loss: 2.1059722900390625\n","    kl loss: 3.7927331924438477\n","Episode: 720\n","    Loss: 250.36172485351562\n","    reconstruction loss: 131.54269409179688\n","    reward loss: 2.3454740047454834\n","    kl loss: 3.6727449893951416\n","Episode: 721\n","    Loss: 254.18524169921875\n","    reconstruction loss: 130.82583618164062\n","    reward loss: 2.3548929691314697\n","    kl loss: 4.093814849853516\n","Episode: 722\n","    Loss: 256.6129455566406\n","    reconstruction loss: 130.37240600585938\n","    reward loss: 2.512998580932617\n","    kl loss: 3.8285586833953857\n","Episode: 723\n","    Loss: 242.35780334472656\n","    reconstruction loss: 127.35540771484375\n","    reward loss: 2.2062110900878906\n","    kl loss: 3.778500556945801\n","Episode: 724\n","    Loss: 254.2401123046875\n","    reconstruction loss: 130.49850463867188\n","    reward loss: 2.3370516300201416\n","    kl loss: 4.194478988647461\n","Episode: 725\n","    Loss: 263.98931884765625\n","    reconstruction loss: 134.07958984375\n","    reward loss: 2.5796775817871094\n","    kl loss: 3.962102174758911\n","Episode: 726\n","    Loss: 251.25640869140625\n","    reconstruction loss: 133.1290740966797\n","    reward loss: 2.1744344234466553\n","    kl loss: 4.202211856842041\n","Episode: 727\n","    Loss: 254.7801971435547\n","    reconstruction loss: 132.2285614013672\n","    reward loss: 2.415173053741455\n","    kl loss: 3.802057981491089\n","Episode: 728\n","    Loss: 244.7464599609375\n","    reconstruction loss: 126.68899536132812\n","    reward loss: 2.3654544353485107\n","    kl loss: 3.526655912399292\n","Episode: 729\n","    Loss: 239.29794311523438\n","    reconstruction loss: 126.4022445678711\n","    reward loss: 2.19997239112854\n","    kl loss: 3.589667320251465\n","Episode: 730\n","    Loss: 238.88327026367188\n","    reconstruction loss: 125.777099609375\n","    reward loss: 2.231886148452759\n","    kl loss: 3.499016761779785\n","Episode: 731\n","    Loss: 260.25738525390625\n","    reconstruction loss: 132.59527587890625\n","    reward loss: 2.5140504837036133\n","    kl loss: 3.9670348167419434\n","Episode: 732\n","    Loss: 260.4398193359375\n","    reconstruction loss: 129.68020629882812\n","    reward loss: 2.596968412399292\n","    kl loss: 3.9865710735321045\n","Episode: 733\n","    Loss: 284.8575744628906\n","    reconstruction loss: 135.68746948242188\n","    reward loss: 3.1001293659210205\n","    kl loss: 4.066559314727783\n","Episode: 734\n","    Loss: 267.6695251464844\n","    reconstruction loss: 133.9808349609375\n","    reward loss: 2.66231632232666\n","    kl loss: 4.05076265335083\n","Episode: 735\n","    Loss: 257.93170166015625\n","    reconstruction loss: 126.59518432617188\n","    reward loss: 2.668553352355957\n","    kl loss: 3.7937145233154297\n","Episode: 736\n","    Loss: 238.93917846679688\n","    reconstruction loss: 127.98634338378906\n","    reward loss: 2.161245584487915\n","    kl loss: 3.5309226512908936\n","Episode: 737\n","    Loss: 241.31610107421875\n","    reconstruction loss: 123.62266540527344\n","    reward loss: 2.3284432888031006\n","    kl loss: 3.619791269302368\n","Episode: 738\n","    Loss: 246.0888214111328\n","    reconstruction loss: 119.80701446533203\n","    reward loss: 2.559690237045288\n","    kl loss: 3.669264078140259\n","Episode: 739\n","    Loss: 245.3308563232422\n","    reconstruction loss: 121.6926498413086\n","    reward loss: 2.4625871181488037\n","    kl loss: 3.744765043258667\n","Episode: 740\n","    Loss: 239.14794921875\n","    reconstruction loss: 124.06509399414062\n","    reward loss: 2.264152765274048\n","    kl loss: 3.583750009536743\n","Episode: 741\n","    Loss: 244.3470458984375\n","    reconstruction loss: 127.2634048461914\n","    reward loss: 2.1941049098968506\n","    kl loss: 4.028998374938965\n","Episode: 742\n","    Loss: 240.2823486328125\n","    reconstruction loss: 123.69851684570312\n","    reward loss: 2.2106986045837402\n","    kl loss: 3.9209372997283936\n","Episode: 743\n","    Loss: 238.5764923095703\n","    reconstruction loss: 118.70130920410156\n","    reward loss: 2.3772332668304443\n","    kl loss: 3.6672017574310303\n","Episode: 744\n","    Loss: 221.39218139648438\n","    reconstruction loss: 112.07461547851562\n","    reward loss: 2.1099627017974854\n","    kl loss: 3.546886682510376\n","Episode: 745\n","    Loss: 244.15684509277344\n","    reconstruction loss: 123.02415466308594\n","    reward loss: 2.4331774711608887\n","    kl loss: 3.5971477031707764\n","Episode: 746\n","    Loss: 245.51776123046875\n","    reconstruction loss: 123.55408477783203\n","    reward loss: 2.4075469970703125\n","    kl loss: 3.7699522972106934\n","Episode: 747\n","    Loss: 234.66305541992188\n","    reconstruction loss: 118.37673950195312\n","    reward loss: 2.277916431427002\n","    kl loss: 3.6559226512908936\n","Episode: 748\n","    Loss: 253.74844360351562\n","    reconstruction loss: 133.20423889160156\n","    reward loss: 2.338535785675049\n","    kl loss: 3.869544506072998\n","Episode: 749\n","    Loss: 232.413818359375\n","    reconstruction loss: 119.16996765136719\n","    reward loss: 2.168208360671997\n","    kl loss: 3.7356574535369873\n","Episode: 750\n","    Loss: 245.6538848876953\n","    reconstruction loss: 122.71495056152344\n","    reward loss: 2.3928935527801514\n","    kl loss: 3.918766498565674\n","Episode: 751\n","    Loss: 250.6723175048828\n","    reconstruction loss: 132.69198608398438\n","    reward loss: 2.235395669937134\n","    kl loss: 3.9741485118865967\n","Episode: 752\n","    Loss: 263.65679931640625\n","    reconstruction loss: 131.4668731689453\n","    reward loss: 2.515333890914917\n","    kl loss: 4.4153242111206055\n","Episode: 753\n","    Loss: 253.79910278320312\n","    reconstruction loss: 125.64250183105469\n","    reward loss: 2.5096654891967773\n","    kl loss: 4.031830787658691\n","Episode: 754\n","    Loss: 252.04150390625\n","    reconstruction loss: 124.322265625\n","    reward loss: 2.40555739402771\n","    kl loss: 4.352472305297852\n","Episode: 755\n","    Loss: 248.00723266601562\n","    reconstruction loss: 121.94122314453125\n","    reward loss: 2.548532247543335\n","    kl loss: 3.6867380142211914\n","Episode: 756\n","    Loss: 245.76535034179688\n","    reconstruction loss: 125.08119201660156\n","    reward loss: 2.3347487449645996\n","    kl loss: 3.8967950344085693\n","Episode: 757\n","    Loss: 243.10940551757812\n","    reconstruction loss: 123.65287017822266\n","    reward loss: 2.137113571166992\n","    kl loss: 4.465755939483643\n","Episode: 758\n","    Loss: 262.4729919433594\n","    reconstruction loss: 133.54132080078125\n","    reward loss: 2.5421664714813232\n","    kl loss: 3.9955835342407227\n","Episode: 759\n","    Loss: 248.1761016845703\n","    reconstruction loss: 128.6161346435547\n","    reward loss: 2.3118515014648438\n","    kl loss: 3.864516019821167\n","Episode: 760\n","    Loss: 270.18597412109375\n","    reconstruction loss: 140.4664306640625\n","    reward loss: 2.408371925354004\n","    kl loss: 4.542651176452637\n","Episode: 761\n","    Loss: 228.51210021972656\n","    reconstruction loss: 122.24949645996094\n","    reward loss: 1.9242876768112183\n","    kl loss: 3.89125394821167\n","Episode: 762\n","    Loss: 234.1790313720703\n","    reconstruction loss: 119.96672058105469\n","    reward loss: 2.245903491973877\n","    kl loss: 3.560570240020752\n","Episode: 763\n","    Loss: 233.5029296875\n","    reconstruction loss: 124.02598571777344\n","    reward loss: 2.0018067359924316\n","    kl loss: 3.941371440887451\n","Episode: 764\n","    Loss: 258.119384765625\n","    reconstruction loss: 127.78359985351562\n","    reward loss: 2.58441162109375\n","    kl loss: 3.9881374835968018\n","Episode: 765\n","    Loss: 244.518798828125\n","    reconstruction loss: 128.88433837890625\n","    reward loss: 2.1244821548461914\n","    kl loss: 4.127758979797363\n","Episode: 766\n","    Loss: 250.49159240722656\n","    reconstruction loss: 127.81198120117188\n","    reward loss: 2.388730764389038\n","    kl loss: 3.9074034690856934\n","Episode: 767\n","    Loss: 256.2762451171875\n","    reconstruction loss: 130.0799560546875\n","    reward loss: 2.3767223358154297\n","    kl loss: 4.301102161407471\n","Episode: 768\n","    Loss: 263.91619873046875\n","    reconstruction loss: 129.9145965576172\n","    reward loss: 2.583514451980591\n","    kl loss: 4.357860565185547\n","Episode: 769\n","    Loss: 237.14749145507812\n","    reconstruction loss: 119.71471405029297\n","    reward loss: 2.195676565170288\n","    kl loss: 4.058411121368408\n","Episode: 770\n","    Loss: 253.41915893554688\n","    reconstruction loss: 122.8973617553711\n","    reward loss: 2.6028971672058105\n","    kl loss: 3.9420394897460938\n","Episode: 771\n","    Loss: 243.31642150878906\n","    reconstruction loss: 125.89923095703125\n","    reward loss: 2.1204237937927246\n","    kl loss: 4.320234298706055\n","Episode: 772\n","    Loss: 242.05506896972656\n","    reconstruction loss: 125.41085052490234\n","    reward loss: 2.111619472503662\n","    kl loss: 4.273753643035889\n","Episode: 773\n","    Loss: 260.00579833984375\n","    reconstruction loss: 137.63177490234375\n","    reward loss: 2.235393762588501\n","    kl loss: 4.413522243499756\n","Episode: 774\n","    Loss: 258.43487548828125\n","    reconstruction loss: 128.01763916015625\n","    reward loss: 2.51914644241333\n","    kl loss: 4.224708557128906\n","Episode: 775\n","    Loss: 246.19891357421875\n","    reconstruction loss: 126.40376281738281\n","    reward loss: 2.2216029167175293\n","    kl loss: 4.203904151916504\n","Episode: 776\n","    Loss: 239.4757080078125\n","    reconstruction loss: 120.84806823730469\n","    reward loss: 2.289196729660034\n","    kl loss: 3.8505752086639404\n","Episode: 777\n","    Loss: 269.5799865722656\n","    reconstruction loss: 136.72128295898438\n","    reward loss: 2.5263803005218506\n","    kl loss: 4.443538188934326\n","Episode: 778\n","    Loss: 250.08615112304688\n","    reconstruction loss: 126.87066650390625\n","    reward loss: 2.3141515254974365\n","    kl loss: 4.222017765045166\n","Episode: 779\n","    Loss: 258.7861022949219\n","    reconstruction loss: 128.13157653808594\n","    reward loss: 2.446547746658325\n","    kl loss: 4.502535343170166\n","Episode: 780\n","    Loss: 256.3582763671875\n","    reconstruction loss: 126.63306427001953\n","    reward loss: 2.5604639053344727\n","    kl loss: 4.010897159576416\n","Episode: 781\n","    Loss: 243.38568115234375\n","    reconstruction loss: 128.73648071289062\n","    reward loss: 2.0433366298675537\n","    kl loss: 4.313241004943848\n","Episode: 782\n","    Loss: 241.1812744140625\n","    reconstruction loss: 115.95262145996094\n","    reward loss: 2.5087220668792725\n","    kl loss: 3.7423369884490967\n","Episode: 783\n","    Loss: 251.67169189453125\n","    reconstruction loss: 126.50498962402344\n","    reward loss: 2.388719320297241\n","    kl loss: 4.156152248382568\n","Episode: 784\n","    Loss: 238.2599334716797\n","    reconstruction loss: 124.21808624267578\n","    reward loss: 2.0534236431121826\n","    kl loss: 4.217201232910156\n","Episode: 785\n","    Loss: 240.49855041503906\n","    reconstruction loss: 125.9873046875\n","    reward loss: 2.1698782444000244\n","    kl loss: 3.8565502166748047\n","Episode: 786\n","    Loss: 229.73312377929688\n","    reconstruction loss: 118.33082580566406\n","    reward loss: 2.0814101696014404\n","    kl loss: 3.855294942855835\n","Episode: 787\n","    Loss: 234.35812377929688\n","    reconstruction loss: 124.11477661132812\n","    reward loss: 2.0787625312805176\n","    kl loss: 3.748666763305664\n","Episode: 788\n","    Loss: 233.4404754638672\n","    reconstruction loss: 121.5294418334961\n","    reward loss: 2.0474677085876465\n","    kl loss: 4.024966716766357\n","Episode: 789\n","    Loss: 240.63375854492188\n","    reconstruction loss: 122.94679260253906\n","    reward loss: 2.213793992996216\n","    kl loss: 4.020417213439941\n","Episode: 790\n","    Loss: 223.99871826171875\n","    reconstruction loss: 113.76786804199219\n","    reward loss: 2.1901462078094482\n","    kl loss: 3.357572317123413\n","Episode: 791\n","    Loss: 258.4564208984375\n","    reconstruction loss: 127.08869934082031\n","    reward loss: 2.499023914337158\n","    kl loss: 4.390190124511719\n","Episode: 792\n","    Loss: 247.8299560546875\n","    reconstruction loss: 122.581298828125\n","    reward loss: 2.3248214721679688\n","    kl loss: 4.387991905212402\n","Episode: 793\n","    Loss: 228.01809692382812\n","    reconstruction loss: 115.58901977539062\n","    reward loss: 2.111421823501587\n","    kl loss: 3.8529305458068848\n","Episode: 794\n","    Loss: 244.14907836914062\n","    reconstruction loss: 118.56312561035156\n","    reward loss: 2.512288808822632\n","    kl loss: 3.765584945678711\n","Episode: 795\n","    Loss: 220.3896942138672\n","    reconstruction loss: 114.29972839355469\n","    reward loss: 1.942044734954834\n","    kl loss: 3.8118388652801514\n","Episode: 796\n","    Loss: 252.12228393554688\n","    reconstruction loss: 121.5958251953125\n","    reward loss: 2.5382697582244873\n","    kl loss: 4.168700695037842\n","Episode: 797\n","    Loss: 240.30653381347656\n","    reconstruction loss: 120.88987731933594\n","    reward loss: 2.2435553073883057\n","    kl loss: 4.089222431182861\n","Episode: 798\n","    Loss: 246.10545349121094\n","    reconstruction loss: 120.95735168457031\n","    reward loss: 2.3919355869293213\n","    kl loss: 4.143035888671875\n","Episode: 799\n","    Loss: 236.84019470214844\n","    reconstruction loss: 118.02041625976562\n","    reward loss: 2.3263046741485596\n","    kl loss: 3.7399120330810547\n","Episode: 800\n","    Loss: 251.51010131835938\n","    reconstruction loss: 130.70272827148438\n","    reward loss: 2.2851004600524902\n","    kl loss: 4.082885265350342\n","Episode: 801\n","    Loss: 240.93402099609375\n","    reconstruction loss: 121.78172302246094\n","    reward loss: 2.264244794845581\n","    kl loss: 3.9903724193573\n","Episode: 802\n","    Loss: 248.42218017578125\n","    reconstruction loss: 118.61819458007812\n","    reward loss: 2.5636820793151855\n","    kl loss: 4.007510185241699\n","Episode: 803\n","    Loss: 241.63204956054688\n","    reconstruction loss: 125.11573791503906\n","    reward loss: 2.2841854095458984\n","    kl loss: 3.656982183456421\n","Episode: 804\n","    Loss: 230.51776123046875\n","    reconstruction loss: 122.04280090332031\n","    reward loss: 2.1162283420562744\n","    kl loss: 3.4406962394714355\n","Episode: 805\n","    Loss: 247.0098114013672\n","    reconstruction loss: 126.00820922851562\n","    reward loss: 2.3075051307678223\n","    kl loss: 4.023891925811768\n","Episode: 806\n","    Loss: 231.54205322265625\n","    reconstruction loss: 123.98406982421875\n","    reward loss: 1.9795352220535278\n","    kl loss: 3.8274264335632324\n","Episode: 807\n","    Loss: 249.11863708496094\n","    reconstruction loss: 124.5522689819336\n","    reward loss: 2.3650102615356445\n","    kl loss: 4.1790995597839355\n","Episode: 808\n","    Loss: 248.89901733398438\n","    reconstruction loss: 135.12249755859375\n","    reward loss: 2.125943899154663\n","    kl loss: 3.9368484020233154\n","Episode: 809\n","    Loss: 252.85205078125\n","    reconstruction loss: 124.20962524414062\n","    reward loss: 2.527505874633789\n","    kl loss: 4.017971992492676\n","Episode: 810\n","    Loss: 236.59173583984375\n","    reconstruction loss: 121.41060638427734\n","    reward loss: 2.1368119716644287\n","    kl loss: 4.039270401000977\n","Episode: 811\n","    Loss: 259.75714111328125\n","    reconstruction loss: 123.69709777832031\n","    reward loss: 2.59477162361145\n","    kl loss: 4.524304389953613\n","Episode: 812\n","    Loss: 253.30210876464844\n","    reconstruction loss: 129.94229125976562\n","    reward loss: 2.2461631298065186\n","    kl loss: 4.4744110107421875\n","Episode: 813\n","    Loss: 248.871826171875\n","    reconstruction loss: 123.74729919433594\n","    reward loss: 2.3052797317504883\n","    kl loss: 4.443972110748291\n","Episode: 814\n","    Loss: 251.14170837402344\n","    reconstruction loss: 123.32666778564453\n","    reward loss: 2.3428914546966553\n","    kl loss: 4.581384181976318\n","Episode: 815\n","    Loss: 278.9208068847656\n","    reconstruction loss: 126.54962921142578\n","    reward loss: 2.8936517238616943\n","    kl loss: 5.1093363761901855\n","Episode: 816\n","    Loss: 246.1357421875\n","    reconstruction loss: 119.96150970458984\n","    reward loss: 2.391866683959961\n","    kl loss: 4.245889663696289\n","Episode: 817\n","    Loss: 255.3162841796875\n","    reconstruction loss: 124.5638656616211\n","    reward loss: 2.279996633529663\n","    kl loss: 5.095255374908447\n","Episode: 818\n","    Loss: 263.64996337890625\n","    reconstruction loss: 125.76557159423828\n","    reward loss: 2.3639116287231445\n","    kl loss: 5.5147480964660645\n","Episode: 819\n","    Loss: 242.6667938232422\n","    reconstruction loss: 119.35301971435547\n","    reward loss: 2.2487149238586426\n","    kl loss: 4.460875034332275\n","Episode: 820\n","    Loss: 259.8581237792969\n","    reconstruction loss: 124.35086059570312\n","    reward loss: 2.5575289726257324\n","    kl loss: 4.5993733406066895\n","Episode: 821\n","    Loss: 255.51834106445312\n","    reconstruction loss: 124.50819396972656\n","    reward loss: 2.3648955821990967\n","    kl loss: 4.823880195617676\n","Episode: 822\n","    Loss: 222.63705444335938\n","    reconstruction loss: 116.3541030883789\n","    reward loss: 1.8617751598358154\n","    kl loss: 4.112082004547119\n","Episode: 823\n","    Loss: 241.55633544921875\n","    reconstruction loss: 114.89427947998047\n","    reward loss: 2.301616668701172\n","    kl loss: 4.610547065734863\n","Episode: 824\n","    Loss: 248.92105102539062\n","    reconstruction loss: 121.13572692871094\n","    reward loss: 2.2659850120544434\n","    kl loss: 4.847584247589111\n","Episode: 825\n","    Loss: 239.380615234375\n","    reconstruction loss: 116.76518249511719\n","    reward loss: 2.2695202827453613\n","    kl loss: 4.318222522735596\n","Episode: 826\n","    Loss: 261.0513610839844\n","    reconstruction loss: 126.61298370361328\n","    reward loss: 2.6606643199920654\n","    kl loss: 4.131511211395264\n","Episode: 827\n","    Loss: 212.9351806640625\n","    reconstruction loss: 109.46006774902344\n","    reward loss: 1.9151424169540405\n","    kl loss: 3.6445133686065674\n","Episode: 828\n","    Loss: 239.20782470703125\n","    reconstruction loss: 114.23701477050781\n","    reward loss: 2.4095773696899414\n","    kl loss: 4.06356143951416\n","Episode: 829\n","    Loss: 247.14169311523438\n","    reconstruction loss: 121.66722106933594\n","    reward loss: 2.4149932861328125\n","    kl loss: 4.094971656799316\n","Episode: 830\n","    Loss: 225.97862243652344\n","    reconstruction loss: 113.86302185058594\n","    reward loss: 2.0717315673828125\n","    kl loss: 3.9604990482330322\n","Episode: 831\n","    Loss: 242.43092346191406\n","    reconstruction loss: 122.69684600830078\n","    reward loss: 2.2875959873199463\n","    kl loss: 3.966823101043701\n","Episode: 832\n","    Loss: 251.201416015625\n","    reconstruction loss: 129.103515625\n","    reward loss: 2.312779664993286\n","    kl loss: 4.115060329437256\n","Episode: 833\n","    Loss: 259.4137268066406\n","    reconstruction loss: 129.77999877929688\n","    reward loss: 2.4535646438598633\n","    kl loss: 4.375898361206055\n","Episode: 834\n","    Loss: 228.30596923828125\n","    reconstruction loss: 116.4409408569336\n","    reward loss: 2.077645778656006\n","    kl loss: 3.914743185043335\n","Episode: 835\n","    Loss: 227.49855041503906\n","    reconstruction loss: 116.30226135253906\n","    reward loss: 2.0227835178375244\n","    kl loss: 4.039886951446533\n","Episode: 836\n","    Loss: 251.33444213867188\n","    reconstruction loss: 115.1400146484375\n","    reward loss: 2.6423370838165283\n","    kl loss: 4.3712639808654785\n","Episode: 837\n","    Loss: 261.39794921875\n","    reconstruction loss: 126.02641296386719\n","    reward loss: 2.6162850856781006\n","    kl loss: 4.380156517028809\n","Episode: 838\n","    Loss: 234.43540954589844\n","    reconstruction loss: 121.91952514648438\n","    reward loss: 1.9890285730361938\n","    kl loss: 4.2899885177612305\n","Episode: 839\n","    Loss: 232.21817016601562\n","    reconstruction loss: 122.2884521484375\n","    reward loss: 2.05922532081604\n","    kl loss: 3.7856836318969727\n","Episode: 840\n","    Loss: 226.98912048339844\n","    reconstruction loss: 113.99678039550781\n","    reward loss: 2.012483835220337\n","    kl loss: 4.25554084777832\n","Episode: 841\n","    Loss: 246.60382080078125\n","    reconstruction loss: 120.27053833007812\n","    reward loss: 2.2074759006500244\n","    kl loss: 4.907161712646484\n","Episode: 842\n","    Loss: 235.83921813964844\n","    reconstruction loss: 114.61011505126953\n","    reward loss: 2.0820202827453613\n","    kl loss: 4.835838317871094\n","Episode: 843\n","    Loss: 252.22918701171875\n","    reconstruction loss: 123.46988677978516\n","    reward loss: 2.431230306625366\n","    kl loss: 4.3666229248046875\n","Episode: 844\n","    Loss: 255.61061096191406\n","    reconstruction loss: 124.71797943115234\n","    reward loss: 2.2619681358337402\n","    kl loss: 5.172374248504639\n","Episode: 845\n","    Loss: 253.84127807617188\n","    reconstruction loss: 129.69607543945312\n","    reward loss: 2.244865894317627\n","    kl loss: 4.557488441467285\n","Episode: 846\n","    Loss: 236.99700927734375\n","    reconstruction loss: 116.12328338623047\n","    reward loss: 2.154515266418457\n","    kl loss: 4.546568870544434\n","Episode: 847\n","    Loss: 248.78366088867188\n","    reconstruction loss: 125.620849609375\n","    reward loss: 2.193430185317993\n","    kl loss: 4.639276027679443\n","Episode: 848\n","    Loss: 236.82357788085938\n","    reconstruction loss: 117.75953674316406\n","    reward loss: 2.100830554962158\n","    kl loss: 4.553496837615967\n","Episode: 849\n","    Loss: 233.400146484375\n","    reconstruction loss: 120.06178283691406\n","    reward loss: 2.0582025051116943\n","    kl loss: 4.130127429962158\n","Episode: 850\n","    Loss: 228.29031372070312\n","    reconstruction loss: 114.97671508789062\n","    reward loss: 2.023639678955078\n","    kl loss: 4.248620986938477\n","Episode: 851\n","    Loss: 212.83551025390625\n","    reconstruction loss: 108.74302673339844\n","    reward loss: 1.9052585363388062\n","    kl loss: 3.7408440113067627\n","Episode: 852\n","    Loss: 225.19366455078125\n","    reconstruction loss: 114.43441772460938\n","    reward loss: 2.0401384830474854\n","    kl loss: 3.9354398250579834\n","Episode: 853\n","    Loss: 247.6266632080078\n","    reconstruction loss: 121.59969329833984\n","    reward loss: 2.3128530979156494\n","    kl loss: 4.507709980010986\n","Episode: 854\n","    Loss: 227.776611328125\n","    reconstruction loss: 114.47529602050781\n","    reward loss: 2.1003854274749756\n","    kl loss: 3.9787826538085938\n","Episode: 855\n","    Loss: 239.51364135742188\n","    reconstruction loss: 117.54667663574219\n","    reward loss: 2.2018814086914062\n","    kl loss: 4.490111827850342\n","Episode: 856\n","    Loss: 240.02940368652344\n","    reconstruction loss: 120.86317443847656\n","    reward loss: 2.2635271549224854\n","    kl loss: 3.9942781925201416\n","Episode: 857\n","    Loss: 236.37454223632812\n","    reconstruction loss: 121.5103988647461\n","    reward loss: 2.0493059158325195\n","    kl loss: 4.3138427734375\n","Episode: 858\n","    Loss: 221.01226806640625\n","    reconstruction loss: 112.06117248535156\n","    reward loss: 1.9818795919418335\n","    kl loss: 3.958531141281128\n","Episode: 859\n","    Loss: 219.75778198242188\n","    reconstruction loss: 112.21507263183594\n","    reward loss: 1.921692132949829\n","    kl loss: 4.028347969055176\n","Episode: 860\n","    Loss: 212.1233673095703\n","    reconstruction loss: 113.0320816040039\n","    reward loss: 1.8157999515533447\n","    kl loss: 3.553828001022339\n","Episode: 861\n","    Loss: 223.11203002929688\n","    reconstruction loss: 114.968017578125\n","    reward loss: 1.9601091146469116\n","    kl loss: 3.9540188312530518\n","Episode: 862\n","    Loss: 232.8669891357422\n","    reconstruction loss: 115.47466278076172\n","    reward loss: 2.1262047290802\n","    kl loss: 4.297516345977783\n","Episode: 863\n","    Loss: 239.28619384765625\n","    reconstruction loss: 115.40716552734375\n","    reward loss: 2.2486095428466797\n","    kl loss: 4.517770290374756\n","Episode: 864\n","    Loss: 252.59898376464844\n","    reconstruction loss: 122.63492584228516\n","    reward loss: 2.4671504497528076\n","    kl loss: 4.3613786697387695\n","Episode: 865\n","    Loss: 237.54104614257812\n","    reconstruction loss: 113.00367736816406\n","    reward loss: 2.36898136138916\n","    kl loss: 4.162301063537598\n","Episode: 866\n","    Loss: 236.1158447265625\n","    reconstruction loss: 120.29934692382812\n","    reward loss: 2.081674098968506\n","    kl loss: 4.295790672302246\n","Episode: 867\n","    Loss: 257.5572204589844\n","    reconstruction loss: 135.13897705078125\n","    reward loss: 2.1990134716033936\n","    kl loss: 4.5452775955200195\n","Episode: 868\n","    Loss: 219.37075805664062\n","    reconstruction loss: 118.44624328613281\n","    reward loss: 1.7987014055252075\n","    kl loss: 3.796997308731079\n","Episode: 869\n","    Loss: 246.09945678710938\n","    reconstruction loss: 126.64213562011719\n","    reward loss: 2.0979764461517334\n","    kl loss: 4.602814197540283\n","Episode: 870\n","    Loss: 241.15896606445312\n","    reconstruction loss: 116.60609436035156\n","    reward loss: 2.36582612991333\n","    kl loss: 4.174896717071533\n","Episode: 871\n","    Loss: 251.0471954345703\n","    reconstruction loss: 126.07829284667969\n","    reward loss: 2.3165204524993896\n","    kl loss: 4.389069080352783\n","Episode: 872\n","    Loss: 210.12136840820312\n","    reconstruction loss: 114.25274658203125\n","    reward loss: 1.6431059837341309\n","    kl loss: 3.8359909057617188\n","Episode: 873\n","    Loss: 228.21043395996094\n","    reconstruction loss: 113.04133605957031\n","    reward loss: 2.1217665672302246\n","    kl loss: 4.09072732925415\n","Episode: 874\n","    Loss: 241.9790802001953\n","    reconstruction loss: 122.9946060180664\n","    reward loss: 2.2267441749572754\n","    kl loss: 4.1048431396484375\n","Episode: 875\n","    Loss: 222.3226318359375\n","    reconstruction loss: 111.75111389160156\n","    reward loss: 1.987843632698059\n","    kl loss: 4.0996994972229\n","Episode: 876\n","    Loss: 240.08856201171875\n","    reconstruction loss: 120.34773254394531\n","    reward loss: 2.236409902572632\n","    kl loss: 4.14664888381958\n","Episode: 877\n","    Loss: 228.40611267089844\n","    reconstruction loss: 111.98274230957031\n","    reward loss: 2.201664924621582\n","    kl loss: 3.9365100860595703\n","Episode: 878\n","    Loss: 239.1085662841797\n","    reconstruction loss: 125.55209350585938\n","    reward loss: 2.046466827392578\n","    kl loss: 4.193012714385986\n","Episode: 879\n","    Loss: 243.50369262695312\n","    reconstruction loss: 120.97966766357422\n","    reward loss: 2.2632484436035156\n","    kl loss: 4.331033229827881\n","Episode: 880\n","    Loss: 235.52865600585938\n","    reconstruction loss: 121.21853637695312\n","    reward loss: 2.1454951763153076\n","    kl loss: 3.921778917312622\n","Episode: 881\n","    Loss: 226.7545623779297\n","    reconstruction loss: 114.81039428710938\n","    reward loss: 1.9805761575698853\n","    kl loss: 4.2624006271362305\n","Episode: 882\n","    Loss: 216.1383514404297\n","    reconstruction loss: 110.46371459960938\n","    reward loss: 1.8806638717651367\n","    kl loss: 3.9851391315460205\n","Episode: 883\n","    Loss: 232.69866943359375\n","    reconstruction loss: 116.59317016601562\n","    reward loss: 2.0387444496154785\n","    kl loss: 4.474946022033691\n","Episode: 884\n","    Loss: 215.49656677246094\n","    reconstruction loss: 112.64201354980469\n","    reward loss: 1.9160003662109375\n","    kl loss: 3.579453706741333\n","Episode: 885\n","    Loss: 238.67489624023438\n","    reconstruction loss: 122.61079406738281\n","    reward loss: 2.0808498859405518\n","    kl loss: 4.323436737060547\n","Episode: 886\n","    Loss: 232.8377685546875\n","    reconstruction loss: 115.96249389648438\n","    reward loss: 2.169935464859009\n","    kl loss: 4.092752933502197\n","Episode: 887\n","    Loss: 248.59429931640625\n","    reconstruction loss: 123.09382629394531\n","    reward loss: 2.3083202838897705\n","    kl loss: 4.470926761627197\n","Episode: 888\n","    Loss: 226.0699920654297\n","    reconstruction loss: 118.549072265625\n","    reward loss: 1.9030349254608154\n","    kl loss: 4.091468334197998\n","Episode: 889\n","    Loss: 258.2878723144531\n","    reconstruction loss: 124.84934997558594\n","    reward loss: 2.5727453231811523\n","    kl loss: 4.3392415046691895\n","Episode: 890\n","    Loss: 223.4351348876953\n","    reconstruction loss: 115.07095336914062\n","    reward loss: 1.8627643585205078\n","    kl loss: 4.316743850708008\n","Episode: 891\n","    Loss: 230.63885498046875\n","    reconstruction loss: 119.42268371582031\n","    reward loss: 2.0158324241638184\n","    kl loss: 4.066202640533447\n","Episode: 892\n","    Loss: 232.39366149902344\n","    reconstruction loss: 119.65913391113281\n","    reward loss: 1.9517501592636108\n","    kl loss: 4.442326545715332\n","Episode: 893\n","    Loss: 236.16558837890625\n","    reconstruction loss: 115.48111724853516\n","    reward loss: 2.105083465576172\n","    kl loss: 4.700655460357666\n","Episode: 894\n","    Loss: 256.5370788574219\n","    reconstruction loss: 121.50586700439453\n","    reward loss: 2.3876917362213135\n","    kl loss: 5.146201133728027\n","Episode: 895\n","    Loss: 232.96153259277344\n","    reconstruction loss: 119.55744171142578\n","    reward loss: 1.9628223180770874\n","    kl loss: 4.4705305099487305\n","Episode: 896\n","    Loss: 247.56475830078125\n","    reconstruction loss: 117.67044067382812\n","    reward loss: 2.4158036708831787\n","    kl loss: 4.53411865234375\n","Episode: 897\n","    Loss: 235.50128173828125\n","    reconstruction loss: 115.35382080078125\n","    reward loss: 2.1224687099456787\n","    kl loss: 4.586106300354004\n","Episode: 898\n","    Loss: 224.44192504882812\n","    reconstruction loss: 114.52165222167969\n","    reward loss: 1.8059459924697876\n","    kl loss: 4.6712164878845215\n","Episode: 899\n","    Loss: 233.3064422607422\n","    reconstruction loss: 111.62956237792969\n","    reward loss: 2.1610381603240967\n","    kl loss: 4.604053974151611\n","Episode: 900\n","    Loss: 230.50186157226562\n","    reconstruction loss: 112.94981384277344\n","    reward loss: 2.103585958480835\n","    kl loss: 4.392653942108154\n","Episode: 901\n","    Loss: 217.6076202392578\n","    reconstruction loss: 108.99124145507812\n","    reward loss: 1.938228726387024\n","    kl loss: 4.077836990356445\n","Episode: 902\n","    Loss: 221.03665161132812\n","    reconstruction loss: 107.90620422363281\n","    reward loss: 2.1311821937561035\n","    kl loss: 3.8539068698883057\n","Episode: 903\n","    Loss: 245.57704162597656\n","    reconstruction loss: 116.96773529052734\n","    reward loss: 2.3521084785461426\n","    kl loss: 4.628551006317139\n","Episode: 904\n","    Loss: 223.01551818847656\n","    reconstruction loss: 116.30599975585938\n","    reward loss: 1.9079946279525757\n","    kl loss: 3.9929704666137695\n","Episode: 905\n","    Loss: 224.45562744140625\n","    reconstruction loss: 109.49063110351562\n","    reward loss: 2.091346025466919\n","    kl loss: 4.176789283752441\n","Episode: 906\n","    Loss: 217.2454833984375\n","    reconstruction loss: 103.90857696533203\n","    reward loss: 2.1527249813079834\n","    kl loss: 3.799152374267578\n","Episode: 907\n","    Loss: 229.27427673339844\n","    reconstruction loss: 109.10881042480469\n","    reward loss: 2.2432146072387695\n","    kl loss: 4.165295124053955\n","Episode: 908\n","    Loss: 245.3706817626953\n","    reconstruction loss: 126.79981994628906\n","    reward loss: 2.1788330078125\n","    kl loss: 4.231171131134033\n","Episode: 909\n","    Loss: 231.27442932128906\n","    reconstruction loss: 111.13693237304688\n","    reward loss: 2.264735221862793\n","    kl loss: 4.08717679977417\n","Episode: 910\n","    Loss: 226.69927978515625\n","    reconstruction loss: 114.07096099853516\n","    reward loss: 2.0359387397766113\n","    kl loss: 4.137044429779053\n","Episode: 911\n","    Loss: 229.65863037109375\n","    reconstruction loss: 109.889404296875\n","    reward loss: 2.2241742610931396\n","    kl loss: 4.192311763763428\n","Episode: 912\n","    Loss: 257.54791259765625\n","    reconstruction loss: 130.47677612304688\n","    reward loss: 2.2481935024261475\n","    kl loss: 4.838436603546143\n","Episode: 913\n","    Loss: 250.6470947265625\n","    reconstruction loss: 124.05936431884766\n","    reward loss: 2.2530391216278076\n","    kl loss: 4.773134708404541\n","Episode: 914\n","    Loss: 250.35147094726562\n","    reconstruction loss: 122.07695007324219\n","    reward loss: 2.4219353199005127\n","    kl loss: 4.350677967071533\n","Episode: 915\n","    Loss: 240.84657287597656\n","    reconstruction loss: 121.38957977294922\n","    reward loss: 2.139402151107788\n","    kl loss: 4.45779275894165\n","Episode: 916\n","    Loss: 234.89126586914062\n","    reconstruction loss: 112.11056518554688\n","    reward loss: 2.295809030532837\n","    kl loss: 4.242738246917725\n","Episode: 917\n","    Loss: 229.63076782226562\n","    reconstruction loss: 110.28727722167969\n","    reward loss: 2.1475584506988525\n","    kl loss: 4.4178948402404785\n","Episode: 918\n","    Loss: 244.72006225585938\n","    reconstruction loss: 124.09188842773438\n","    reward loss: 2.158006191253662\n","    kl loss: 4.509795188903809\n","Episode: 919\n","    Loss: 233.99581909179688\n","    reconstruction loss: 115.08586120605469\n","    reward loss: 2.1585066318511963\n","    kl loss: 4.336223125457764\n","Episode: 920\n","    Loss: 257.8219909667969\n","    reconstruction loss: 128.66067504882812\n","    reward loss: 2.399338722229004\n","    kl loss: 4.51844596862793\n","Episode: 921\n","    Loss: 232.56527709960938\n","    reconstruction loss: 118.3414306640625\n","    reward loss: 2.019162654876709\n","    kl loss: 4.355316638946533\n","Episode: 922\n","    Loss: 231.6531982421875\n","    reconstruction loss: 115.76070404052734\n","    reward loss: 1.9184190034866333\n","    kl loss: 4.874783039093018\n","Episode: 923\n","    Loss: 234.9404296875\n","    reconstruction loss: 116.79232788085938\n","    reward loss: 2.0076093673706055\n","    kl loss: 4.788177967071533\n","Episode: 924\n","    Loss: 224.8210906982422\n","    reconstruction loss: 113.37046813964844\n","    reward loss: 2.0035173892974854\n","    kl loss: 4.13275146484375\n","Episode: 925\n","    Loss: 228.3490447998047\n","    reconstruction loss: 114.5060806274414\n","    reward loss: 1.9671367406845093\n","    kl loss: 4.499317646026611\n","Episode: 926\n","    Loss: 234.0322265625\n","    reconstruction loss: 117.46528625488281\n","    reward loss: 2.0421299934387207\n","    kl loss: 4.50924015045166\n","Episode: 927\n","    Loss: 230.70509338378906\n","    reconstruction loss: 116.97695922851562\n","    reward loss: 2.056912422180176\n","    kl loss: 4.173619270324707\n","Episode: 928\n","    Loss: 249.62586975097656\n","    reconstruction loss: 118.6708984375\n","    reward loss: 2.4509496688842773\n","    kl loss: 4.517173767089844\n","Episode: 929\n","    Loss: 230.9901580810547\n","    reconstruction loss: 121.04885864257812\n","    reward loss: 1.8176772594451904\n","    kl loss: 4.632258892059326\n","Episode: 930\n","    Loss: 229.4801483154297\n","    reconstruction loss: 114.65208435058594\n","    reward loss: 2.1208579540252686\n","    kl loss: 4.059803485870361\n","Episode: 931\n","    Loss: 238.06732177734375\n","    reconstruction loss: 114.77963256835938\n","    reward loss: 2.229557991027832\n","    kl loss: 4.5253167152404785\n","Episode: 932\n","    Loss: 221.33587646484375\n","    reconstruction loss: 114.615966796875\n","    reward loss: 1.8600127696990967\n","    kl loss: 4.161946773529053\n","Episode: 933\n","    Loss: 211.44076538085938\n","    reconstruction loss: 101.84382629394531\n","    reward loss: 2.0793750286102295\n","    kl loss: 3.681882381439209\n","Episode: 934\n","    Loss: 240.29270935058594\n","    reconstruction loss: 108.76025390625\n","    reward loss: 2.480839252471924\n","    kl loss: 4.470308303833008\n","Episode: 935\n","    Loss: 207.49310302734375\n","    reconstruction loss: 105.29641723632812\n","    reward loss: 1.7253938913345337\n","    kl loss: 4.180788993835449\n","Episode: 936\n","    Loss: 220.4042205810547\n","    reconstruction loss: 112.90068054199219\n","    reward loss: 1.9012653827667236\n","    kl loss: 4.095924377441406\n","Episode: 937\n","    Loss: 239.39981079101562\n","    reconstruction loss: 122.20552062988281\n","    reward loss: 2.0852699279785156\n","    kl loss: 4.420984745025635\n","Episode: 938\n","    Loss: 232.61863708496094\n","    reconstruction loss: 110.8838119506836\n","    reward loss: 2.2799582481384277\n","    kl loss: 4.193629264831543\n","Episode: 939\n","    Loss: 248.65879821777344\n","    reconstruction loss: 120.80722045898438\n","    reward loss: 2.3460423946380615\n","    kl loss: 4.574009895324707\n","Episode: 940\n","    Loss: 226.75155639648438\n","    reconstruction loss: 116.42424774169922\n","    reward loss: 1.8658013343811035\n","    kl loss: 4.502426624298096\n","Episode: 941\n","    Loss: 229.138427734375\n","    reconstruction loss: 113.77207946777344\n","    reward loss: 2.1236941814422607\n","    kl loss: 4.103705406188965\n","Episode: 942\n","    Loss: 226.31625366210938\n","    reconstruction loss: 110.49978637695312\n","    reward loss: 2.1205363273620605\n","    kl loss: 4.159769058227539\n","Episode: 943\n","    Loss: 232.0198974609375\n","    reconstruction loss: 112.90406036376953\n","    reward loss: 2.1221611499786377\n","    kl loss: 4.484018802642822\n","Episode: 944\n","    Loss: 223.46365356445312\n","    reconstruction loss: 112.90585327148438\n","    reward loss: 1.9083797931671143\n","    kl loss: 4.376450538635254\n","Episode: 945\n","    Loss: 240.46255493164062\n","    reconstruction loss: 119.9146728515625\n","    reward loss: 2.1994073390960693\n","    kl loss: 4.356863498687744\n","Episode: 946\n","    Loss: 238.70889282226562\n","    reconstruction loss: 123.37996673583984\n","    reward loss: 1.9955435991287231\n","    kl loss: 4.548489570617676\n","Episode: 947\n","    Loss: 226.61666870117188\n","    reconstruction loss: 108.984619140625\n","    reward loss: 2.2738840579986572\n","    kl loss: 3.80461049079895\n","Episode: 948\n","    Loss: 234.17352294921875\n","    reconstruction loss: 120.51481628417969\n","    reward loss: 1.9299606084823608\n","    kl loss: 4.611007213592529\n","Episode: 949\n","    Loss: 235.87261962890625\n","    reconstruction loss: 113.83755493164062\n","    reward loss: 2.1872243881225586\n","    kl loss: 4.548220634460449\n","Episode: 950\n","    Loss: 218.82864379882812\n","    reconstruction loss: 109.72176361083984\n","    reward loss: 1.951182246208191\n","    kl loss: 4.081549644470215\n","Episode: 951\n","    Loss: 246.58203125\n","    reconstruction loss: 116.79362487792969\n","    reward loss: 2.4214491844177246\n","    kl loss: 4.5037689208984375\n","Episode: 952\n","    Loss: 226.65591430664062\n","    reconstruction loss: 107.88351440429688\n","    reward loss: 2.1411261558532715\n","    kl loss: 4.383297920227051\n","Episode: 953\n","    Loss: 226.99081420898438\n","    reconstruction loss: 105.95906066894531\n","    reward loss: 2.274340867996216\n","    kl loss: 4.1429829597473145\n","Episode: 954\n","    Loss: 210.5962371826172\n","    reconstruction loss: 105.70170593261719\n","    reward loss: 1.8795207738876343\n","    kl loss: 3.9111316204071045\n","Episode: 955\n","    Loss: 231.03981018066406\n","    reconstruction loss: 111.50658416748047\n","    reward loss: 2.220345973968506\n","    kl loss: 4.182112216949463\n","Episode: 956\n","    Loss: 216.05380249023438\n","    reconstruction loss: 107.14784240722656\n","    reward loss: 1.9369932413101196\n","    kl loss: 4.111119747161865\n","Episode: 957\n","    Loss: 239.56320190429688\n","    reconstruction loss: 124.03569030761719\n","    reward loss: 1.9426690340042114\n","    kl loss: 4.753407955169678\n","Episode: 958\n","    Loss: 210.65579223632812\n","    reconstruction loss: 105.7210693359375\n","    reward loss: 1.911243200302124\n","    kl loss: 3.8041210174560547\n","Episode: 959\n","    Loss: 242.95545959472656\n","    reconstruction loss: 112.97803497314453\n","    reward loss: 2.4425783157348633\n","    kl loss: 4.448718547821045\n","Episode: 960\n","    Loss: 227.19329833984375\n","    reconstruction loss: 114.92328643798828\n","    reward loss: 1.9929572343826294\n","    kl loss: 4.251650810241699\n","Episode: 961\n","    Loss: 236.43405151367188\n","    reconstruction loss: 119.85462188720703\n","    reward loss: 2.054196357727051\n","    kl loss: 4.468255519866943\n","Episode: 962\n","    Loss: 257.1168212890625\n","    reconstruction loss: 125.91863250732422\n","    reward loss: 2.344639301300049\n","    kl loss: 4.913581848144531\n","Episode: 963\n","    Loss: 215.4208526611328\n","    reconstruction loss: 105.88401794433594\n","    reward loss: 1.9303354024887085\n","    kl loss: 4.197509765625\n","Episode: 964\n","    Loss: 229.65072631835938\n","    reconstruction loss: 116.67237854003906\n","    reward loss: 1.9215636253356934\n","    kl loss: 4.572360992431641\n","Episode: 965\n","    Loss: 240.31748962402344\n","    reconstruction loss: 114.50992584228516\n","    reward loss: 2.157104253768921\n","    kl loss: 5.030890941619873\n","Episode: 966\n","    Loss: 213.04318237304688\n","    reconstruction loss: 105.39144897460938\n","    reward loss: 1.8289414644241333\n","    kl loss: 4.3638787269592285\n","Episode: 967\n","    Loss: 234.41371154785156\n","    reconstruction loss: 107.56421661376953\n","    reward loss: 2.365870952606201\n","    kl loss: 4.404401779174805\n","Episode: 968\n","    Loss: 231.0113525390625\n","    reconstruction loss: 112.50894165039062\n","    reward loss: 2.0789434909820557\n","    kl loss: 4.573938846588135\n","Episode: 969\n","    Loss: 229.99932861328125\n","    reconstruction loss: 109.61503601074219\n","    reward loss: 2.066577911376953\n","    kl loss: 4.8054070472717285\n","Episode: 970\n","    Loss: 235.66888427734375\n","    reconstruction loss: 117.95936584472656\n","    reward loss: 2.0559582710266113\n","    kl loss: 4.575098037719727\n","Episode: 971\n","    Loss: 226.83509826660156\n","    reconstruction loss: 113.42677307128906\n","    reward loss: 2.0160927772521973\n","    kl loss: 4.2845072746276855\n","Episode: 972\n","    Loss: 228.68589782714844\n","    reconstruction loss: 117.2625732421875\n","    reward loss: 1.9401580095291138\n","    kl loss: 4.351778984069824\n","Episode: 973\n","    Loss: 232.5703582763672\n","    reconstruction loss: 114.1253662109375\n","    reward loss: 2.056769609451294\n","    kl loss: 4.6458048820495605\n","Episode: 974\n","    Loss: 242.650146484375\n","    reconstruction loss: 114.07295227050781\n","    reward loss: 2.4014153480529785\n","    kl loss: 4.452764511108398\n","Episode: 975\n","    Loss: 213.22816467285156\n","    reconstruction loss: 108.4612045288086\n","    reward loss: 1.788572907447815\n","    kl loss: 4.216691493988037\n","Episode: 976\n","    Loss: 224.54196166992188\n","    reconstruction loss: 109.52593994140625\n","    reward loss: 1.9813621044158936\n","    kl loss: 4.566834926605225\n","Episode: 977\n","    Loss: 217.18495178222656\n","    reconstruction loss: 108.00480651855469\n","    reward loss: 1.8576940298080444\n","    kl loss: 4.4160847663879395\n","Episode: 978\n","    Loss: 242.3742218017578\n","    reconstruction loss: 124.35910034179688\n","    reward loss: 2.042158842086792\n","    kl loss: 4.653956413269043\n","Episode: 979\n","    Loss: 235.3572540283203\n","    reconstruction loss: 113.78125\n","    reward loss: 2.1797258853912354\n","    kl loss: 4.528560161590576\n","Episode: 980\n","    Loss: 226.61599731445312\n","    reconstruction loss: 114.06668853759766\n","    reward loss: 1.96394681930542\n","    kl loss: 4.381117343902588\n","Episode: 981\n","    Loss: 245.45953369140625\n","    reconstruction loss: 126.33988952636719\n","    reward loss: 2.014333486557007\n","    kl loss: 4.861797332763672\n","Episode: 982\n","    Loss: 224.39117431640625\n","    reconstruction loss: 115.43365478515625\n","    reward loss: 1.7848782539367676\n","    kl loss: 4.648679256439209\n","Episode: 983\n","    Loss: 227.755859375\n","    reconstruction loss: 112.86344909667969\n","    reward loss: 1.8399748802185059\n","    kl loss: 5.0493292808532715\n","Episode: 984\n","    Loss: 220.8214569091797\n","    reconstruction loss: 108.39888763427734\n","    reward loss: 2.0060513019561768\n","    kl loss: 4.221076965332031\n","Episode: 985\n","    Loss: 242.29852294921875\n","    reconstruction loss: 115.60800170898438\n","    reward loss: 2.1987874507904053\n","    kl loss: 4.973297119140625\n","Episode: 986\n","    Loss: 237.69281005859375\n","    reconstruction loss: 111.98751831054688\n","    reward loss: 2.199326276779175\n","    kl loss: 4.87288761138916\n","Episode: 987\n","    Loss: 243.11892700195312\n","    reconstruction loss: 112.6974105834961\n","    reward loss: 2.320261240005493\n","    kl loss: 4.921236991882324\n","Episode: 988\n","    Loss: 241.08700561523438\n","    reconstruction loss: 123.19405364990234\n","    reward loss: 1.9289889335632324\n","    kl loss: 5.037833213806152\n","Episode: 989\n","    Loss: 229.57363891601562\n","    reconstruction loss: 116.78472900390625\n","    reward loss: 1.9615347385406494\n","    kl loss: 4.41351842880249\n","Episode: 990\n","    Loss: 225.26634216308594\n","    reconstruction loss: 115.07896423339844\n","    reward loss: 1.7786210775375366\n","    kl loss: 4.793564319610596\n","Episode: 991\n","    Loss: 236.79991149902344\n","    reconstruction loss: 117.30542755126953\n","    reward loss: 1.9956411123275757\n","    kl loss: 4.964704513549805\n","Episode: 992\n","    Loss: 243.8575897216797\n","    reconstruction loss: 123.03593444824219\n","    reward loss: 2.0761048793792725\n","    kl loss: 4.815797805786133\n","Episode: 993\n","    Loss: 224.41714477539062\n","    reconstruction loss: 103.48593139648438\n","    reward loss: 2.1553566455841064\n","    kl loss: 4.549373149871826\n","Episode: 994\n","    Loss: 204.37339782714844\n","    reconstruction loss: 103.36729431152344\n","    reward loss: 1.7557483911514282\n","    kl loss: 3.955491781234741\n","Episode: 995\n","    Loss: 229.90213012695312\n","    reconstruction loss: 109.59597778320312\n","    reward loss: 2.085845708847046\n","    kl loss: 4.730154991149902\n","Episode: 996\n","    Loss: 231.5831756591797\n","    reconstruction loss: 115.47932434082031\n","    reward loss: 1.9409966468811035\n","    kl loss: 4.816896438598633\n","Episode: 997\n","    Loss: 221.8134002685547\n","    reconstruction loss: 110.5561752319336\n","    reward loss: 1.8582910299301147\n","    kl loss: 4.621704578399658\n","Episode: 998\n","    Loss: 239.7049102783203\n","    reconstruction loss: 119.79354858398438\n","    reward loss: 2.041429042816162\n","    kl loss: 4.846134662628174\n","Episode: 999\n","    Loss: 220.6201934814453\n","    reconstruction loss: 112.05944061279297\n","    reward loss: 1.8760994672775269\n","    kl loss: 4.289726257324219\n"]}],"source":["# Training Loop\n","\n","env = make_env()\n","# Hyperparameters\n","grayscale = False\n","category_size = 32\n","class_size = 32\n","z_dim = category_size * class_size\n","history_dim = 512\n","gMLP_dim = 1000\n","S4_mlp_dim = 2048\n","zMLP_dim = 1000\n","dropout = 0.2\n","rewardMLP_dim = 1024\n","alpha = 0.8\n","beta_recon = 1.0\n","beta_rew = 35.0\n","beta_kl = 10.0\n","free_bits = 1.0\n","model_lr = 1e-3\n","weight_decay = 1e-2\n","\n","\n","\n","start_epoch = 0\n","restart_training = False\n","\n","if not restart_training:\n","    encoder = Encoder(\n","        input_dim=(1 if grayscale else 3),\n","        category_size=category_size,\n","        class_size=class_size\n","    ).to(device)\n","\n","    decoder = Decoder(\n","        output_dim=(1 if grayscale else 3),\n","        z_dim=z_dim,\n","        history_dim=history_dim\n","    ).to(device)\n","\n","    historyEncoder = HistoryEncoder(\n","        z_dim=z_dim,\n","        action_dim=env.action_space.shape[0],\n","        gMLP_dim=gMLP_dim,\n","        history_dim=history_dim,\n","        S4_mlp_dim=S4_mlp_dim,\n","        S4_n_layers=6,\n","        dropout=dropout,\n","        prenorm=True,\n","        zMLP_dim=zMLP_dim,\n","        class_size=class_size,\n","        category_size=category_size,\n","        lr=model_lr\n","    ).to(device)\n","\n","    rewardModel = RewardModel(\n","        history_dim=history_dim,\n","        z_dim=z_dim,\n","        mlp_dim=rewardMLP_dim\n","    ).to(device)\n","\n","EPOCHS = 1000\n","# 世界モデルのパラメータたち\n","model_params = (list(encoder.parameters()) + list(decoder.parameters())  + list(historyEncoder.parameters()) + list(rewardModel.parameters()))\n","\n","model_optimizer, scheduler = setup_optimizer(\n","    model_params, lr=model_lr, weight_decay=weight_decay, epochs=EPOCHS\n",")\n","\n","\n","batch_size = 8\n","chunk_length = 100\n","clip_grad_norm = 1000\n","\n","encoder.train(); decoder.train(); historyEncoder.train(); rewardModel.train()\n","\n","\n","for episode in range(start_epoch, start_epoch + EPOCHS):\n","    done = False\n","    if len(replay_buffer) > batch_size * chunk_length:\n","        observations, actions, rewards, dones = replay_buffer.sample(batch_size, chunk_length)\n","        observations = preprocess_obs(observations)\n","        observations = torch.tensor(observations, dtype=torch.float32).to(device)\n","        observations = rearrange(observations, \"b t h w c -> b t c h w\")\n","        actions = torch.tensor(actions, dtype=torch.float32).to(device)\n","        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n","        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n","\n","        z_cur, posterior_dist, posterior_dist_no_grad = encoder(observations)\n","        h, z_next, prior_dist, prior_dist_no_grad = historyEncoder(z_cur, actions)\n","        recon_obs = decoder(h, z_next)\n","        rewards_pred = rewardModel(h, z_next)\n","\n","        recon_loss = F.mse_loss(recon_obs[:, :-1, :, :, :], observations[:, 1:, :, :, :], reduction=\"none\").mean([0, 1]).sum()\n","        reward_loss = F.mse_loss(rewards, rewards_pred, reduction=\"none\").mean([0, 1]).sum() # 報酬はインデックスずらさない\n","\n","        # KLダイバージェンスの重み付けスケジューリング\n","        kl_anneal_steps = 1000\n","        kl_weight = min(1.0, (episode + 1) / kl_anneal_steps)\n","\n","        # Free bitsの適用\n","        kl_loss = (alpha * kl_divergence(posterior_dist_no_grad, prior_dist) + (1 - alpha) * kl_divergence(posterior_dist, prior_dist_no_grad)).sum(dim=-1).clamp(min=free_bits).mean([0,1]).sum()\n","\n","        loss = beta_recon * recon_loss + beta_rew * reward_loss + beta_kl * kl_loss\n","\n","        model_optimizer.zero_grad()\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model_params, clip_grad_norm)\n","        model_optimizer.step()\n","\n","        # writer.add_scalar('Loss/loss', loss.item(), episode)\n","        # writer.add_scalar('Loss/recon_loss', recon_loss.item(), episode)\n","        # writer.add_scalar('Loss/reward_loss', reward_loss.item(), episode)\n","        # writer.add_scalar('Loss/kl_loss', kl_loss.item(), episode)\n","        print(f\"Episode: {episode}\\n    Loss: {loss.item()}\")\n","        print(f\"    reconstruction loss: {recon_loss.item()}\")\n","        print(f\"    reward loss: {reward_loss.item()}\")\n","        print(f\"    kl loss: {kl_loss.item()}\")\n","\n","        # Evaluation(Imaginationによる評価ではないので正確とは言えない。2個下にあるコードセルにImaginatinoの実装を作った)\n","        if (episode+1) % 5000 == 0:\n","            encoder.eval(); decoder.eval(); historyEncoder.eval(); rewardModel.eval()\n","            env = make_env()\n","            obs = env.reset()\n","            done = False\n","            recon_error = 0\n","            rew_error = 0\n","            while not done:\n","                action = env.action_space.sample()\n","                next_obs, reward, done, _ = env.step(action)\n","                replay_buffer.push(obs, action, reward, done)\n","                obs_ = preprocess_obs(obs)\n","                obs_ = torch.tensor(obs_, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n","                obs_ = rearrange(obs_, \"b t h w c -> b t c h w\")\n","                next_obs_ = preprocess_obs(next_obs)\n","                next_obs_ = torch.tensor(next_obs_, dtype=torch.float32).to(device).reshape(1, 1, *next_obs.shape)\n","                next_obs_ = rearrange(next_obs_, \"b t h w c -> b t c h w\")\n","                action = torch.tensor(action, dtype=torch.float32).to(device).reshape(1, 1, *action.shape)\n","                reward = torch.tensor(reward, dtype=torch.float32).to(device).reshape(1, 1, 1)\n","                with torch.no_grad():\n","                    z_cur, posterior_dist, posterior_dist_no_grad = encoder(obs_)\n","                    h, z_next, prior_dist, prior_dist_no_grad = historyEncoder(z_cur, action)\n","                    recon_obs = decoder(h, z_next)\n","                    rewards_pred = rewardModel(h, z_next)\n","\n","                recon_error += F.mse_loss(recon_obs, next_obs_).sum()\n","                rew_error += F.mse_loss(rewards_pred, reward).sum()\n","\n","                obs = next_obs\n","\n","            recon_error /= 500\n","            rew_error /= 500\n","            print(f\"Evaluation (episode: {episode})\\n Recon error: {recon_error.item()}, Reward error: {rew_error.item()}\")\n","            # writer.add_scalar('Eval/Recon_error', recon_error.item(), episode)\n","            # writer.add_scalar('Eval/Reward_error', rew_error.item(), episode)\n","\n","\n","            encoder.train(); decoder.train(); historyEncoder.train(); rewardModel.train()\n","            del env\n","            gc.collect()\n","\n","    scheduler.step(episode)\n","\n","#writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwbaqeuE2D4v","outputId":"156f20b1-d224-4550-ac35-2b5652f1f1dd","executionInfo":{"status":"ok","timestamp":1737339230289,"user_tz":-540,"elapsed":15,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0015, 0.0014, 0.0022, 0.7542, 0.0014, 0.0023, 0.0015, 0.0996, 0.0039,\n","        0.0015, 0.0014, 0.0014, 0.0016, 0.0025, 0.0977, 0.0017, 0.0014, 0.0014,\n","        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0025, 0.0017, 0.0016,\n","        0.0015, 0.0014, 0.0014, 0.0017, 0.0014], device='cuda:0',\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":22}],"source":["posterior_dist.probs[0, 2, 2]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJhuNu8J2QY-","outputId":"f5cc7cfc-6600-4784-c21c-82aa75c656ac","executionInfo":{"status":"ok","timestamp":1737339230289,"user_tz":-540,"elapsed":13,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0014, 0.0013, 0.0013, 0.6807, 0.0011, 0.0014, 0.0009, 0.1518, 0.0030,\n","        0.0013, 0.0017, 0.0012, 0.0016, 0.0016, 0.1144, 0.0017, 0.0013, 0.0012,\n","        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0020, 0.0015, 0.0127,\n","        0.0016, 0.0014, 0.0014, 0.0017, 0.0013], device='cuda:0',\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":23}],"source":["prior_dist.probs[0, 2, 2]"]},{"cell_type":"markdown","metadata":{"id":"zCPIHoBO-07D"},"source":["### 画像を再構成"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPCZbJ6Gdgd_"},"outputs":[],"source":["# 結果を動画で観てみるための関数\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","from matplotlib import animation\n","\n","\n","def display_video(frames: List[np.ndarray]) -> None:\n","    \"\"\"\n","    結果を動画にするための関数．\n","\n","    frames : List[np.ndarray]\n","        観測画像をリスト化したもの．\n","    \"\"\"\n","    plt.figure(figsize=(8, 8), dpi=50)\n","    patch = plt.imshow(frames[0])\n","    plt.axis(\"off\")\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","        plt.title(\"Step %d\" % (i))\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n","    display(HTML(anim.to_jshtml(default_mode=\"once\")))\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":951,"output_embedded_package_id":"1nB41rUNZAatFUFa8-kBI6XBbVAmdZyML"},"executionInfo":{"elapsed":10531,"status":"ok","timestamp":1737340485953,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"},"user_tz":-540},"id":"k9yWUnZ3dnWn","outputId":"cf61d9db-0c58-4f08-b64e-b7021054c076"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["encoder.eval(); decoder.eval(); historyEncoder.eval(); rewardModel.eval()\n","env = make_env()\n","obs = env.reset()\n","if True:\n","    observations, actions, rewards, dones = replay_buffer.sample(1, chunk_length)\n","    observations = preprocess_obs(observations)\n","    observations = torch.tensor(observations, dtype=torch.float32).to(device)\n","    observations = rearrange(observations, \"b t h w c -> b t c h w\")\n","    actions = torch.tensor(actions, dtype=torch.float32).to(device)\n","\n","    z_cur, posterior_dist, posterior_dist_no_grad = encoder(observations)\n","    h, z_next, prior_dist, prior_dist_no_grad = historyEncoder(z_cur, actions)\n","    recon_obs = decoder(h, z_next)\n","    pred_rewards = rewardModel(h, z_next)\n","\n","    recon_images = recon_obs[0][:-1].cpu().detach().numpy().transpose(0, 2, 3, 1)\n","    recon_images = (recon_images + 0.5).clip(0.0, 1.0)\n","    original_images = observations[0][1:].cpu().detach().numpy().transpose(0, 2, 3, 1)\n","    original_images = (original_images + 0.5).clip(0.0, 1.0)\n","    images = np.concatenate([original_images, recon_images], axis=-2)\n","    print(images.shape)\n","    frames = []\n","    for i in range(chunk_length-1):\n","        frames.append(images[i])\n","\n","    display_video(frames)\n","\n","    rewards = rewards[0, :, 0]\n","    pred_rewards = pred_rewards[0, :, 0].cpu().detach().numpy()\n","    plt.plot(np.arange(len(rewards)), rewards, label=\"real\")\n","    plt.plot(np.arange(len(pred_rewards)), pred_rewards, label=\"pred\")\n","    plt.legend()\n","    plt.show()\n","\n","    # ピアソンの相関係数を計算\n","    #correlation = np.corrcoef(rewards, pred_rewards)[0, 1]\n","\n","    #print(f\"ピアソンの相関係数: {correlation}\")\n","\n","del env\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zi1OwA2zRnF7","outputId":"57bf6c60-dadc-482b-f958-979d558cf244","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737340486920,"user_tz":-540,"elapsed":5,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0., device='cuda:0', grad_fn=<DivBackward0>)"]},"metadata":{},"execution_count":24}],"source":["torch.sum(torch.abs(z_next[:-1] - z_cur[1:])) / 99"]},{"cell_type":"markdown","metadata":{"id":"zJmLplS2RnF8"},"source":["### 複数ステップ先の予測（Imagination part）"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1A6zpkbrybZxTKnj-bFLtqaRzlZEAVr2Q"},"executionInfo":{"elapsed":7128,"status":"ok","timestamp":1737340494044,"user":{"displayName":"大蔵春日","userId":"03754790342905464126"},"user_tz":-540},"id":"ZNmzRNjrRnF8","outputId":"938d9517-0210-4472-aba4-75d79a3bfb7c"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["encoder.eval(); decoder.eval(); historyEncoder.eval(); rewardModel.eval()\n","env = make_env()\n","done = False\n","obs = env.reset()\n","obs = preprocess_obs(obs)\n","obs = torch.tensor(obs, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n","obs = rearrange(obs, \"b t h w c -> b t c h w\")\n","frames = []\n","reward_real_list = []\n","reward_pred_list = []\n","S4_hiddens = None\n","with torch.no_grad():\n","    z_cur, posterior_dist, posterior_dist_no_grad = encoder(obs)\n","assert z_cur.shape == (1, 1, z_dim), \"z_curs.shapeは(1, 1, z_dim)を想定しています\"\n","# action = env.action_space.sample()\n","# actions = torch.tensor(action, dtype=torch.float32).to(device).reshape(1, 1, *action.shape)\n","\n","# S4には今までの行動とz_curをすべて入れないと隠れ状態を0として計算してしまいます。\n","# なので、毎回concateして、actionsとz_cursを作っています。（もっと効率的なやり方があれ変えてください）\n","# imagineを使用する方法に書き換え\n","for _ in range(50):\n","    action = env.action_space.sample()\n","    next_obs, reward, done, _ = env.step(action)\n","    reward_real_list.append(reward)\n","\n","    next_obs = preprocess_obs(next_obs).reshape(1, 1, *next_obs.shape)\n","    next_obs = torch.tensor(next_obs, dtype=torch.float32).to(device)\n","    next_obs = rearrange(next_obs, \"b t h w c -> b t c h w\")\n","    reward = torch.tensor(reward, dtype=torch.float32).to(device)\n","\n","    action = torch.tensor(action, dtype=torch.float32).to(device).reshape(1, 1, *action.shape) # (6,) -> [1, 1, 6]\n","\n","    with torch.no_grad():\n","        #print(z_cur.shape)\n","        #print(action.shape)\n","        h, z_cur, S4_hiddens = historyEncoder.imagine(z_cur, action, S4_hiddens) # hs, z_nexts, _, _ = historyEncoder(z_curs, actions)\n","        h, z_cur = h[:, -1].reshape(1, 1, h.size(-1)), z_cur[:, -1].reshape(1, 1, z_cur.size(-1))\n","        recon_obs = decoder(h, z_cur)\n","        pred_reward = rewardModel(h, z_cur)\n","        print(S4_hiddens)\n","\n","    recon_image = recon_obs[0][0].cpu().detach().numpy().transpose(1, 2, 0)\n","    recon_image = (recon_image + 0.5).clip(0.0, 1.0)\n","    original_image = next_obs[0][0].cpu().detach().numpy().transpose(1, 2, 0)\n","    original_image = (original_image + 0.5).clip(0.0, 1.0)\n","    image = np.concatenate([original_image, recon_image], axis=-2)\n","    frames.append(image)\n","\n","    pred_reward = pred_reward[0][0].cpu().detach().numpy()\n","    reward_pred_list.append(pred_reward.item())\n","\n","# 動画を生成\n","display_video(frames)\n","\n","# 報酬を描画\n","plt.plot(np.arange(len(reward_real_list)), reward_real_list, label=\"real\")\n","plt.plot(np.arange(len(reward_pred_list)), reward_pred_list, label=\"pred\")\n","plt.legend()\n","plt.show()\n","\n","# ピアソンの相関係数を計算\n","correlation = np.corrcoef(reward_real_list, reward_pred_list)[0, 1]\n","\n","print(f\"ピアソンの相関係数: {correlation}\")\n","\n","del env\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNtlmaVsRnF8"},"outputs":[],"source":["torch.sum(torch.abs(z_nexts[0, :-1] - z_curs[0, 1:-1])) / (2*49)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2s-dYSomL7O"},"outputs":[],"source":["torch.abs(z_cur[0, 1:] - z_next[0, :-1]).sum() / (2 * (chunk_length-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3T2G_BesYmj"},"outputs":[],"source":["torch.abs(z_cur[0, 1:] - z_next[0, :-1]).sum() / (2 * (chunk_length-1))"]},{"cell_type":"markdown","metadata":{"id":"QJ13nxcPoV20"},"source":["### 学習の実装（方策モデルも含めてタスクを解く学習）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HfiOqKxJoV21"},"outputs":[],"source":["# Training Loop\n","save_model_dir = \"/workspace/assets/params\"\n","env = make_env()\n","\n","# Hyperparameters\n","grayscale = False\n","category_size = 32\n","class_size = 32\n","z_dim = category_size * class_size\n","history_dim = 512\n","gMLP_dim = 1000\n","S4_mlp_dim = 2048\n","zMLP_dim = 1000\n","dropout = 0.2\n","rewardMLP_dim = 1024\n","alpha = 0.8\n","beta_recon = 1.0\n","beta_rew = 35.0\n","beta_kl = 10.0\n","free_bits = 1.0\n","worldModel_lr = 1e-3\n","weight_decay = 1e-2\n","gamma = 0.9\n","lambda_ = 0.95\n","\n","\n","\n","start_episode = 0\n","restart_training = False\n","\n","if not restart_training:\n","    encoder = Encoder(\n","        input_dim=(1 if grayscale else 3),\n","        category_size=category_size,\n","        class_size=class_size\n","    ).to(device)\n","\n","    decoder = Decoder(\n","        output_dim=(1 if grayscale else 3),\n","        z_dim=z_dim,\n","        history_dim=history_dim\n","    ).to(device)\n","\n","    historyEncoder = HistoryEncoder(\n","        z_dim=z_dim,\n","        action_dim=env.action_space.shape[0],\n","        gMLP_dim=gMLP_dim,\n","        history_dim=history_dim,\n","        S4_mlp_dim=S4_mlp_dim,\n","        S4_n_layers=6,\n","        dropout=dropout,\n","        prenorm=True,\n","        zMLP_dim=zMLP_dim,\n","        class_size=class_size,\n","        category_size=category_size,\n","        lr=worldModel_lr\n","    ).to(device)\n","\n","    rewardModel = RewardModel(\n","        history_dim=history_dim,\n","        z_dim=z_dim,\n","        mlp_dim=rewardMLP_dim\n","    ).to(device)\n","\n","    policyModel = PolicyModel(\n","        z_dim=z_dim,\n","        history_dim=history_dim,\n","        action_dim=env.action_space.shape[0],\n","        mlp_dim=512\n","    ).to(device)\n","\n","    valueModel = ValueModel(\n","        z_dim=z_dim,\n","        history_dim=history_dim,\n","        mlp_dim=512\n","    ).to(device)\n","\n","EPISODE = 100\n","update_steps = 100\n","imagination_horizon = 15\n","test_interval = 10\n","model_save_interval = 20\n","\n","# 世界モデルのパラメータたち\n","world_model_params = (list(encoder.parameters()) + list(decoder.parameters())  + list(historyEncoder.parameters()) + list(rewardModel.parameters()))\n","\n","worldModel_optimizer, scheduler = setup_optimizer(\n","    world_model_params, lr=worldModel_lr, weight_decay=weight_decay, epochs=EPISODE * update_steps\n",")\n","policy_optimizer = optim.AdamW(policyModel.parameters(), lr=5e-4)\n","value_optimizer = optim.AdamW(valueModel.parameters(), lr=5e-4)\n","\n","batch_size = 8\n","chunk_length = 100\n","clip_grad_norm = 1000\n","\n","encoder.train(); decoder.train(); historyEncoder.train(); rewardModel.train()\n","\n","\n","for episode in range(start_episode, start_episode + EPISODE):\n","\n","    # 実環境とinteraction\n","    env = make_env()\n","    obs = env.reset()\n","    obs_ = preprocess_obs(obs)\n","    obs_ = torch.tensor(obs_, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n","    obs_ = rearrange(obs_, \"b t h w c -> b t c h w\")\n","    z_cur, _, _ = encoder(obs_)\n","    z_curs, actions = None, None\n","    h = torch.zeros(1, 1, history_dim).to(device) # 疑似的なhistory\n","    done = False\n","    while not done:\n","        with torch.no_grad():\n","            action = policyModel(z_cur, h)\n","            action = ((torch.rand_like(action) - 0.5) / 5  + action)[0][0].cpu().detach().numpy()\n","            obs, reward, done, _ = env.step(action)\n","            replay_buffer.push(obs, action, reward, done)\n","\n","            obs_ = preprocess_obs(obs)\n","            obs_ = torch.tensor(obs_, dtype=torch.float32).to(device).reshape(1, 1, *next_obs.shape)\n","            obs_ = rearrange(obs_, \"b t h w c -> b t c h w\")\n","            action_ = torch.tensor(action, dtype=torch.float32).to(device).reshape(1, 1, *action.shape)\n","            z_cur, _, _ = encoder(obs_)\n","            if z_curs == None and actions == None:\n","                z_curs, actions = z_cur, action_\n","            else:\n","                z_curs, actions = torch.cat([z_curs, z_cur]), torch.cat([actions, action_])\n","\n","            hs, z_nexts, _, _ = historyEncoder(z_curs, actions)\n","            z_cur, h = z_nexts[:, -1].unsqueeze(1), hs[:, -1].unsqueeze(1)\n","\n","    # 世界モデル更新部分\n","    if len(replay_buffer) > batch_size * chunk_length:\n","        start = time.time()\n","        for step in range(update_steps):\n","            observations, actions, rewards, dones = replay_buffer.sample(batch_size, chunk_length)\n","            observations = preprocess_obs(observations)\n","            observations = torch.tensor(observations, dtype=torch.float32).to(device)\n","            observations = rearrange(observations, \"b t h w c -> b t c h w\")\n","            actions = torch.tensor(actions, dtype=torch.float32).to(device)\n","            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n","            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n","\n","            z_cur, posterior_dist, posterior_dist_no_grad = encoder(observations)\n","            h, z_next, prior_dist, prior_dist_no_grad = historyEncoder(z_cur, actions)\n","            recon_obs = decoder(h, z_next)\n","            rewards_pred = rewardModel(h, z_next)\n","\n","            recon_loss = F.mse_loss(recon_obs[:, :-1, :, :, :], observations[:, 1:, :, :, :], reduction=\"none\").mean([0, 1]).sum()\n","            reward_loss = F.mse_loss(rewards, rewards_pred, reduction=\"none\").mean([0, 1]).sum() # 報酬はインデックスずらさない\n","\n","            # KLダイバージェンスの重み付けスケジューリング\n","            # kl_anneal_steps = 1000\n","            # kl_weight = min(1.0, (episode + 1) / kl_anneal_steps)\n","\n","            # Free bitsの適用\n","            kl_loss = (alpha * kl_divergence(posterior_dist_no_grad, prior_dist) + (1 - alpha) * kl_divergence(posterior_dist, prior_dist_no_grad)).sum(dim=-1).clamp(min=free_bits).mean([0,1]).sum()\n","\n","            loss = beta_recon * recon_loss + beta_rew * reward_loss + beta_kl * kl_loss\n","\n","            worldModel_optimizer.zero_grad()\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(world_model_params, clip_grad_norm)\n","            worldModel_optimizer.step()\n","\n","            print(f\"Step: {episode * update_steps + step}\\n    Loss: {loss.item()}\")\n","            print(f\"    reconstruction loss: {recon_loss.item()}\")\n","            print(f\"    reward loss: {reward_loss.item()}\")\n","            print(f\"    kl loss: {kl_loss.item()}\")\n","\n","            # 方策モデル、価値モデルの学習\n","            zs, hs = z_next[:, -1].unsqueeze(1), h[:, -1].unsqueeze(1)\n","            actions = None\n","            for _ in range(imagination_horizon+1):\n","                z_, h_ = zs[:, -1].unsqueeze(1), hs[:, -1].unsqueeze(1)\n","                action = policyModel(z_, h_)\n","                if actions == None:\n","                    actions = action\n","                else:\n","                    actions = torch.cat([actions, action], dim=1)\n","                hs_, z_nexts_, _, _ = historyEncoder(zs, actions)\n","                h, z_next = hs_[:, -1].unsqueeze(1), z_nexts_[:, -1].unsqueeze(1)\n","                zs = torch.cat([zs, z_next], dim=1)\n","                hs = torch.cat([hs, h], dim=1)\n","\n","            imagined_rewards = rewardModel(hs, zs)\n","            imagined_values = valueModel(hs, zs)\n","            lambda_target_values = lambda_target(imagined_rewards, imagined_values, gamma, lambda_)\n","            action_loss = -lambda_target_values.mean()\n","            policy_optimizer.zero_grad()\n","            action_loss.backward()\n","            nn.utils.clip_grad_norm_(policyModel.parameters(), clip_grad_norm)\n","            policy_optimizer.step()\n","\n","            imagined_values = valueModel(hs.detach(), zs.detach())\n","            value_loss = 0.5 * F.mse_loss(imagined_values, lambda_target_values.detach())\n","            value_optimizer.zero_grad()\n","            value_loss.backward()\n","            nn.utils.clip_grad_norm_(valueModel.parameters(), clip_grad_norm)\n","            value_optimizer.step()\n","\n","            print(f\"Episode: {episode}  Policy loss: {action_loss}, Value loss: {value_loss}\")\n","        print('elasped time for update: %.2fs' % (time.time() - start))\n","\n","    # --------------------------------------------------------------\n","    #    テストフェーズ．探索ノイズなしでの性能を評価する\n","    # --------------------------------------------------------------\n","    if (episode + 1) % test_interval == 0:\n","        start = time.time()\n","        obs = env.reset()\n","        obs = preprocess_obs(obs)\n","        obs = torch.tensor(obs, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n","        obs = rearrange(obs, \"b t h w c -> b t c h w\")\n","        with torch.no_grad():\n","            z, _, _ = encoder(obs)\n","        h = torch.zeros(1, 1, history_dim).to(device)\n","        zs, actions = None, None\n","        done = False\n","        total_reward = 0\n","        while not done:\n","            action = policyModel.deter(h, z).cpu().detach().numpy()\n","            obs, reward, done, _ = env.step(action)\n","            action_ = torch.tensor(action, dtype=torch.float32).to(device)\n","            total_reward += reward\n","            obs = preprocess_obs(obs)\n","            obs = torch.tensor(obs, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n","            obs = rearrange(obs, \"b t h w c -> b t c h w\")\n","            with torch.no_grad():\n","                z_, _, _ = encoder(obs)\n","            if zs == None and actions == None:\n","                zs, actions = z_, action_\n","            else:\n","                zs = torch.cat([zs, z_], dim=-1)\n","                actions = torch.cat([actions, action_], dim=-1)\n","            h, z, _, _ = historyEncoder(zs, actions)\n","\n","        writer.add_scalar('total reward at test', total_reward, episode)\n","        print('Total test reward at episode [%4d/%4d] is %f' %\n","                (episode+1, EPISODE, total_reward))\n","        print('elasped time for test: %.2fs' % (time.time() - start))\n","\n","    if (episode + 1) % model_save_interval == 0:\n","        # 定期的に学習済みモデルのパラメータを保存する\n","        model_log_dir = os.path.join(save_model_dir, 'episode_%04d' % (episode + 1))\n","        os.makedirs(model_log_dir, exist_ok=True)\n","        torch.save(encoder.state_dict(), os.path.join(model_log_dir, 'encoder.pth'))\n","        torch.save(decoder.state_dict(), os.path.join(model_log_dir, 'decoder.pth'))\n","        torch.save(historyEncoder.state_dict(), os.path.join(model_log_dir, 'historyEncoder.pth'))\n","        torch.save(rewardModel.state_dict(), os.path.join(model_log_dir, 'rewardModel.pth'))\n","        torch.save(valueModel.state_dict(), os.path.join(model_log_dir, 'valueModel.pth'))\n","        torch.save(policyModel.state_dict(), os.path.join(model_log_dir, 'policyModel.pth'))\n","    del env # envをそのまま残しておくとエラーが出たかららしい\n","    gc.collect()\n","\n","writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHyycC0ARnF9"},"outputs":[],"source":["%tensorboard --logdir='./logs'"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}