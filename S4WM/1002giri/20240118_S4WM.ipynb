{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOsrfG4NoV2i",
    "outputId": "ec6abd36-dcd5-44e6-e3af-b7e8a8eab217",
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/DMwr2023/S4Dmodification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "939d0OzWoV2l",
    "outputId": "626dab5c-e4c9-458f-caf9-7536e95e7cf0",
    "scrolled": true,
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "# Structured Kernels（restart kernelをしてからもう一回する必要があるかもしれない。その場合だいぶ時間がかかる。逆にだいぶ時間がかかんなかったらうまくいってない）\n",
    "%cd /workspace/S4Dmodification\n",
    "# ============= Set Up =============\n",
    "# Requirements\n",
    "# 入れなくても動く\n",
    "# !conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n",
    "!pip install -r requirements.txt\n",
    "# Structured Kernels\n",
    "%cd extensions/kernels/\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kw_9oXg0RnFs",
    "outputId": "c08a41e3-560c-4af8-b16d-4f23abe74300"
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFrG-uLkRnFt",
    "outputId": "fe92d532-7027-4ccf-fb2d-baffc8146465"
   },
   "outputs": [],
   "source": [
    "# うまくいかなければrestart kernelをする（もしくは上を再実行する）\n",
    "import importlib\n",
    "import structured_kernels\n",
    "importlib.reload(structured_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZ_AGAfNRnFt",
    "outputId": "c7fe8fa3-5ac8-4aaf-da32-f81c408bfac8"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspace/S4Dmodification/extensions/kernels\")\n",
    "import structured_kernels\n",
    "print(\"Structured kernels module loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3ACROoYRnFu",
    "outputId": "5478077f-9929-4a2e-a9e8-7ecb669cbdba"
   },
   "outputs": [],
   "source": [
    "!pip show structured_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12379,
     "status": "ok",
     "timestamp": 1737217335887,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "TcncgEj_Rr3Q",
    "outputId": "c705b95d-a7a2-4196-a12a-469804e57978"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "#%cd /content/drive/MyDrive/Colab Notebooks/WorldModel2024/最終課題/\n",
    "#condaをインストール\n",
    "#!pip install -q condacolab\n",
    "#import condacolab\n",
    "#condacolab.install()\n",
    "%cd /content/drive/MyDrive/Colab Notebooks/WorldModel2024/最終課題/S4Dmodification/\n",
    "#!conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n",
    "!pip install pytorch torchvision torchaudio pytorch-cuda -c pytorch -c nvidia\n",
    "!pip install -r requirements.txt\n",
    "# Structured Kernels\n",
    "%cd /content/drive/MyDrive/Colab Notebooks/WorldModel2024/最終課題/S4Dmodification/extensions/kernels/\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1737217335888,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "gFhlQiETRwRH",
    "outputId": "fc3a9a08-a4b4-4693-ca2b-0637e0abc2da"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Colab Notebooks/WorldModel2024/最終課題/S4Dmodification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16816,
     "status": "ok",
     "timestamp": 1737212706012,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "Y6bztFv3pmuc",
    "outputId": "bc75c09a-86ae-40a3-aaf7-c85029d988eb"
   },
   "outputs": [],
   "source": [
    "!pip install pybullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDkoT6sbpM5j",
    "outputId": "812f1ae5-81c9-458e-b81e-9c900cdf88f1"
   },
   "outputs": [],
   "source": [
    "%cd /workspace/S4Dmodification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21048,
     "status": "ok",
     "timestamp": 1737217356928,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "g08VOGf8oV2m",
    "outputId": "69f8fbf4-e81d-4377-d9b1-564413374ed1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from models.s4.s4 import S4Block as S4  # Can use full version instead of minimal S4D standalone below\n",
    "from models.s4.s4d import S4D\n",
    "#from models.s4.s4d import S4D\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from typing import Any, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import pybullet_envs  # PyBulletの環境をgymに登録する\n",
    "from einops import rearrange\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Dropout broke in PyTorch 1.11\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) == (1, 11):\n",
    "    print(\"WARNING: Dropout is bugged in PyTorch 1.11. Results may be worse.\")\n",
    "    dropout_fn = nn.Dropout\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 12):\n",
    "    dropout_fn = nn.Dropout1d\n",
    "else:\n",
    "    dropout_fn = nn.Dropout2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1737217356930,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "hmSpRiG-oV2o",
    "outputId": "ab36b547-56e5-414b-d534-71d8522a7d46"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LO_awH5qoV2r"
   },
   "source": [
    "### 環境のWrapper（カメラに関する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1737217356930,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "LeUt9xnPoV2r"
   },
   "outputs": [],
   "source": [
    "class GymWrapper_PyBullet(object):\n",
    "    \"\"\"\n",
    "    PyBullet環境のためのラッパー\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"]}\n",
    "    reward_range = (-np.inf, np.inf)\n",
    "\n",
    "    # __init__でカメラ位置に関するパラメータ（ cam_dist:カメラ距離，cam_yaw：カメラの水平面での回転，cam_pitch:カメラの縦方向での回転）を受け取り，カメラの位置を調整できるようにします.\n",
    "    # 　同時に画像の大きさも変更できるようにします\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        cam_dist: int = 3,\n",
    "        cam_yaw: int = 0,\n",
    "        cam_pitch: int = -30,\n",
    "        render_width: int = 320,\n",
    "        render_height: int = 240,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        コンストラクタ．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : gym.Env\n",
    "            gymで提供されている環境のインスタンス．\n",
    "        cam_dist : int\n",
    "            カメラの距離．\n",
    "        cam_yaw : int\n",
    "            カメラの水平面での回転．\n",
    "        cam_pitch : int\n",
    "            カメラの縦方向での回転．\n",
    "        render_width : int\n",
    "            観測画像の幅．\n",
    "        render_height : int\n",
    "            観測画像の高さ．\n",
    "        \"\"\"\n",
    "        self._env = env\n",
    "\n",
    "        self._render_width = render_width\n",
    "        self._render_height = render_height\n",
    "        self._set_nested_attr(self._env, cam_dist, \"_cam_dist\")\n",
    "        self._set_nested_attr(self._env, cam_yaw, \"_cam_yaw\")\n",
    "        self._set_nested_attr(self._env, cam_pitch, \"_cam_pitch\")\n",
    "        self._set_nested_attr(self._env, render_width, \"_render_width\")\n",
    "        self._set_nested_attr(self._env, render_height, \"_render_height\")\n",
    "\n",
    "    def _set_nested_attr(self, env: gym.Env, value: int, attr: str) -> None:\n",
    "        \"\"\"\n",
    "        多重継承の属性に再帰的にアクセスして値を変更する．\n",
    "        カメラの設定に利用．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : int\n",
    "            設定したい値．\n",
    "        attr : str\n",
    "            変更したい属性の名前．\n",
    "        \"\"\"\n",
    "        if hasattr(env, attr):\n",
    "            setattr(env, attr, value)\n",
    "        else:\n",
    "            self._set_nested_attr(env.env, value, attr)\n",
    "\n",
    "    def __getattr(self, name: str) -> Any:\n",
    "        \"\"\"\n",
    "        環境が保持している属性値を取得するメソッド．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            取得したい属性値の名前．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _env.name : Any\n",
    "            環境が保持している属性値．\n",
    "        \"\"\"\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    @property\n",
    "    def observation_space(self) -> gym.spaces.Box:\n",
    "        \"\"\"\n",
    "        観測空間に関する情報を取得するメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        space : gym.spaces.Box\n",
    "            観測空間に関する情報（各画素値の最小値，各画素値の最大値，観測データの形状， データの型）．\n",
    "        \"\"\"\n",
    "        width = self._render_width\n",
    "        height = self._render_height\n",
    "        return gym.spaces.Box(0, 255, (height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    @property\n",
    "    def action_space(self) -> gym.spaces.Box:\n",
    "        \"\"\"\n",
    "        行動空間に関する情報を取得するメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        space : gym.spaces.Box\n",
    "            行動空間に関する情報（各行動の最小値，各行動の最大値，行動空間の次元， データの型） ．\n",
    "        \"\"\"\n",
    "        return self._env.action_space\n",
    "\n",
    "    # 　元の観測（低次元の状態）は今回は捨てて，env.render()で取得した画像を観測とします.\n",
    "    #  画像，報酬，終了シグナルが得られます.\n",
    "    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n",
    "        \"\"\"\n",
    "        環境に行動を与え次の観測，報酬，終了フラグを取得するメソッド．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : np.dnarray (action_dim, )\n",
    "            与える行動．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (height, width, 3)\n",
    "            行動を与えたときの次の観測．\n",
    "        reward : float\n",
    "            行動を与えたときに得られる報酬．\n",
    "        done : bool\n",
    "            エピソードが終了したかどうか表すフラグ．\n",
    "        info : dict\n",
    "            その他の環境に関する情報．\n",
    "        \"\"\"\n",
    "        _, reward, done, info = self._env.step(action)\n",
    "        obs = self._env.render(mode=\"rgb_array\") # 今回状態として画像を扱いたいため\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        環境をリセットするためのメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (height, width, 3)\n",
    "            環境をリセットしたときの初期の観測．\n",
    "        \"\"\"\n",
    "        self._env.reset()\n",
    "        obs = self._env.render(mode=\"rgb_array\")\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode=\"human\", **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        観測をレンダリングするためのメソッド．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mode : str\n",
    "            レンダリング方法に関するオプション． (default='human')\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (height, width, 3)\n",
    "            観測をレンダリングした結果．\n",
    "        \"\"\"\n",
    "        return self._env.render(mode, **kwargs)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        環境を閉じるためのメソッド．\n",
    "        \"\"\"\n",
    "        self._env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mu7zty7oV2s"
   },
   "source": [
    "#### カメラに関するWrapperのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1737217357237,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "bssBjMXHoV2s",
    "outputId": "13bb111f-0320-4ace-b8ce-2c71072a535e"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetahBulletEnv-v0\")\n",
    "# カメラのパラメータを与えてカメラの位置と角度，画像の大きさを調整\n",
    "env = GymWrapper_PyBullet(\n",
    "    env, cam_dist=2, cam_pitch=0, render_width=64, render_height=64\n",
    ")\n",
    "\n",
    "env.reset()\n",
    "image = env.render(mode=\"rgb_array\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "env.close()\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKpfbv9loV2s"
   },
   "source": [
    "### 環境のWrapper（行動の連続入力に関する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1737217357237,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "9pVE1R8koV2t"
   },
   "outputs": [],
   "source": [
    "class RepeatAction(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    同じ行動を指定された回数自動的に繰り返すラッパー．観測は最後の行動に対応するものになる\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: GymWrapper_PyBullet, skip: int = 4) -> None:\n",
    "        \"\"\"\n",
    "        コンストラクタ．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : GymWrapper_PyBullet\n",
    "            環境のインスタンス．今回は先程定義したラッパーでラップした環境を利用する．\n",
    "        skip : int\n",
    "            同じ行動を繰り返す回数．\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        環境をリセットするためのメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (width, height, 3)\n",
    "            環境をリセットしたときの初期の観測．\n",
    "        \"\"\"\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n",
    "        \"\"\"\n",
    "        環境に行動を与え次の観測，報酬，終了フラグを取得するメソッド．\n",
    "        与えられた行動をskipの回数だけ繰り返した結果を返す．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : np.ndarray (action_dim, )\n",
    "            与える行動．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (width, height, 3)\n",
    "            行動をskipの回数だけ繰り返したあとの観測．\n",
    "        total_reawrd : float\n",
    "            行動をskipの回数だけ繰り返したときの報酬和．\n",
    "        done : bool\n",
    "            エピソードが終了したかどうか表すフラグ．\n",
    "        info : dict\n",
    "            その他の環境に関する情報．\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJzUltaMoV2t"
   },
   "source": [
    "#### Wrapperを通した環境を作る関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1737217357239,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "AxP0b5VaoV2t"
   },
   "outputs": [],
   "source": [
    "def make_env() -> RepeatAction:\n",
    "    \"\"\"\n",
    "    作成たラッパーをまとめて適用して環境を作成する関数．\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    env : RepeatAction\n",
    "        ラッパーを適用した環境．\n",
    "    \"\"\"\n",
    "    env = gym.make(\"HalfCheetahBulletEnv-v0\")  # 環境を読み込む．今回はHalfCheetah\n",
    "    # Dreamerでは観測は64x64のRGB画像\n",
    "    env = GymWrapper_PyBullet(\n",
    "        env, cam_dist=2, cam_pitch=0, render_width=64, render_height=64\n",
    "    )\n",
    "    env = RepeatAction(env, skip=2)  # DreamerではActionRepeatは2\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTf30JzooV2u"
   },
   "source": [
    "### Replay Buffer\n",
    "連続した経験をとってくるのでDQNとは少し違う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1737217357240,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "PKX042qhoV2u"
   },
   "outputs": [],
   "source": [
    "# 　今回のReplayBuffer\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    RNNを用いて訓練するのに適したリプレイバッファ．\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, capacity: int, observation_shape: List[int], action_dim: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        コンストラクタ．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            リプレイバッファにためておくことができる経験の上限．\n",
    "        observation_shape : List[int]\n",
    "            環境から与えられる観測の形状．\n",
    "        action_dim : int\n",
    "            行動空間の次元数．\n",
    "        \"\"\"\n",
    "\n",
    "        self.observations = np.zeros((capacity, *observation_shape), dtype=np.uint8)\n",
    "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.done = np.zeros((capacity, 1), dtype=bool)\n",
    "        # self.done = np.zeros((capacity, 1), dtype=np.bool)\n",
    "\n",
    "        self.index = 0\n",
    "        self.is_filled = False\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(\n",
    "        self, observation: np.ndarray, action: np.ndarray, reward: float, done: bool\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        リプレイバッファに経験を追加するメソッド．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : np.ndarray (64, 64, 3)\n",
    "            環境から得られた観測．\n",
    "        action : np.ndarray (action_dim, )\n",
    "            エージェントがとった（もしくは経験を貯める際のランダムな）行動．\n",
    "        reward : float\n",
    "            観測に対して行動をとったときに得られる報酬．\n",
    "        done : bool\n",
    "            エピソードが終了するかどうかのフラグ．\n",
    "        \"\"\"\n",
    "        self.observations[self.index] = observation\n",
    "        self.actions[self.index] = action\n",
    "        self.rewards[self.index] = reward\n",
    "        self.done[self.index] = done\n",
    "\n",
    "        # indexは巡回し，最も古い経験を上書きする\n",
    "        if self.index == self.capacity - 1:\n",
    "            self.is_filled = True\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size: int, chunk_length: int) -> Tuple[np.ndarray]:\n",
    "        \"\"\"\n",
    "        経験をリプレイバッファからサンプルします．（ほぼ）一様なサンプルです．\n",
    "        結果として返ってくるのは観測（画像），行動，報酬，終了シグナルについての(batch_size, chunk_length, 各要素の次元)の配列です．\n",
    "        各バッチは連続した経験になっています．\n",
    "        注意: chunk_lengthをあまり大きな値にすると問題が発生する場合があります．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            バッチサイズ．\n",
    "        chunk_length : int\n",
    "            バッチあたりの系列長．\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sampled_observations : np.ndarray (batch size, chunk length, 3, 64, 64)\n",
    "            バッファからサンプリングされた観測．\n",
    "        sampled_actions : np.ndarray (batch size, chunk length, action dim)\n",
    "            バッファからサンプリングされた行動．\n",
    "        sampled_rewards : np.ndarray (batch size, chunk length, 1)\n",
    "            バッファからサンプリングされた報酬．\n",
    "        sampled_done : np.ndarray (batch size, chunk length, 1)\n",
    "            バッファからサンプリングされたエピソードの終了フラグ．\n",
    "        \"\"\"\n",
    "        episode_borders = np.where(self.done)[0]\n",
    "        sampled_indexes = []\n",
    "        for _ in range(batch_size):\n",
    "            cross_border = True\n",
    "            while cross_border:\n",
    "                initial_index = np.random.randint(len(self) - chunk_length + 1)\n",
    "                final_index = initial_index + chunk_length - 1\n",
    "                cross_border = np.logical_and(\n",
    "                    initial_index <= episode_borders, episode_borders < final_index\n",
    "                ).any()  # 論理積\n",
    "            sampled_indexes += list(range(initial_index, final_index + 1))\n",
    "\n",
    "        sampled_observations = self.observations[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, *self.observations.shape[1:]\n",
    "        )\n",
    "        sampled_actions = self.actions[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, self.actions.shape[1]\n",
    "        )\n",
    "        sampled_rewards = self.rewards[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, 1\n",
    "        )\n",
    "        sampled_done = self.done[sampled_indexes].reshape(batch_size, chunk_length, 1)\n",
    "        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        バッファに貯められている経験の数を返すメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        length : int\n",
    "            バッファに貯められている経験の数．\n",
    "        \"\"\"\n",
    "        return self.capacity if self.is_filled else self.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IeElUxMoV2u"
   },
   "source": [
    "#### 観測の前処理を行う関数\n",
    "ラッパーとして最初から適用してしまわないのは，リプレイバッファにはより容量の小さなnp．uint8の形式で保存しておきたいためです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1737217357240,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "YwMN-_OOoV2u"
   },
   "outputs": [],
   "source": [
    "def preprocess_obs(obs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    画像を正規化する．[0, 255] -> [-0.5, 0.5]．\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obs : np.ndarray (64, 64, 3) or (chank length, batch size, 64, 64, 3)\n",
    "        環境から得られた観測．画素値は[0, 255]．\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    normalized_obs : np.ndarray (64, 64, 3) or (chank length, batch size, 64, 64, 3)\n",
    "        画素値を[-0.5, 0.5]で正規化した観測．\n",
    "    \"\"\"\n",
    "    obs = obs.astype(np.float32)\n",
    "    normalized_obs = obs / 255.0 - 0.5\n",
    "    return normalized_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOqUE9TCoV2v"
   },
   "source": [
    "#### λ-returnを計算する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1737217357241,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "SW_a1HQsoV2v"
   },
   "outputs": [],
   "source": [
    "def lambda_target(\n",
    "    rewards: torch.Tensor, values: torch.Tensor, gamma: float, lambda_: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    価値関数の学習のためのλ-returnを計算する関数．\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rewards : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n",
    "        報酬モデルによる報酬の推定値．\n",
    "    values : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n",
    "        価値関数を近似するValueモデルによる状態価値観数の推定値．\n",
    "    gamma : float\n",
    "        割引率．\n",
    "    lambda_ : float\n",
    "        λ-returnのパラメータλ．\n",
    "\n",
    "    V_lambda : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n",
    "        各状態に対するλ-returnの値．\n",
    "    \"\"\"\n",
    "    V_lambda = torch.zeros_like(rewards, device=rewards.device)\n",
    "\n",
    "    H = rewards.shape[0] - 1\n",
    "    V_n = torch.zeros_like(rewards, device=rewards.device)\n",
    "    V_n[H] = values[H]\n",
    "    for n in range(1, H + 1):\n",
    "        # まずn-step returnを計算します\n",
    "        # 注意: 系列が途中で終わってしまったら，可能な中で最大のnを用いたn-stepを使います\n",
    "        V_n[:-n] = (gamma**n) * values[n:]\n",
    "        for k in range(1, n + 1):\n",
    "            if k == n:\n",
    "                V_n[:-n] += (gamma ** (n - 1)) * rewards[k:]\n",
    "            else:\n",
    "                V_n[:-n] += (gamma ** (k - 1)) * rewards[k : -n + k]\n",
    "\n",
    "        # lambda_でn-step returnを重みづけてλ-returnを計算します\n",
    "        if n == H:\n",
    "            V_lambda += (lambda_ ** (H - 1)) * V_n\n",
    "        else:\n",
    "            V_lambda += (1 - lambda_) * (lambda_ ** (n - 1)) * V_n\n",
    "\n",
    "    return V_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhNpM4fRoV2v"
   },
   "source": [
    "## ここからはモデルの実装編"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkMC4L_coV2v"
   },
   "source": [
    "### S4Block (p26 Figure21参照)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1737217365169,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "PePcmo4koV2v"
   },
   "outputs": [],
   "source": [
    "class S4Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=256,\n",
    "        d_mlp = 512,\n",
    "        n_layers=2,\n",
    "        dropout=0.2,\n",
    "        prenorm=True,\n",
    "        lr=0.001,\n",
    "        dropout_fn=nn.Dropout\n",
    "    ):\n",
    "        super(S4Block, self).__init__()\n",
    "\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.s4_layers.append(\n",
    "                S4D(d_model, dropout=dropout, transposed=True, lr=min(0.001, lr))\n",
    "            )\n",
    "            self.dropouts.append(dropout_fn(dropout))\n",
    "\n",
    "        self.norm_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_mlp),\n",
    "            nn.GELU(),\n",
    "            dropout_fn(dropout),\n",
    "            nn.Linear(d_mlp, d_model),\n",
    "            dropout_fn(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_model), L is the length of continuous observations, B is the batch size\n",
    "        \"\"\"\n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        for norm, s4, dropout in \\\n",
    "            zip(self.norms, self.s4_layers, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x #z.shape=torch.Size([8, 512, 100])\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = s4(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, d_model, L) -> (B, L, d_model)\n",
    "\n",
    "        #TODO: x_にも操作が反映されてたりしないか確認する. residual connectionのため\n",
    "        x_ = x\n",
    "        x = x_ + self.norm_mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def imagine(self, x, S4hidden):\n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        # assert x.size(1) == 1 and S4hidden.size(1) == 1 d_model 1になるか？要確認\n",
    "        for norm, s4, dropout in \\\n",
    "            zip(self.norms, self.s4_layers, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # print(\"z(B, H(d_model), L)\", z.shape)\n",
    "            # print(\"S4hidden(B, H(d_model), L)\", S4hidden.shape)\n",
    "\n",
    "            # Apply S4 block: y, x_kを出力\n",
    "            # A_bar.shape, A_bar.dtype=torch.Size([512, 32]) torch.complex64\n",
    "            # B_bar.shape, B_bar.dtype=torch.Size([512, 32]) torch.complex64\n",
    "            # C.shape=torch.Size([512, 32])\n",
    "            # D.shape=torch.Size([512])\n",
    "            z, S4hidden_next = s4.generate(z, S4hidden)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, d_model, L) -> (B, L, d_model)\n",
    "\n",
    "        #TODO: x_にも操作が反映されてたりしないか確認する. residual connectionのため\n",
    "        x_ = x\n",
    "        x = x_ + self.norm_mlp(x)\n",
    "\n",
    "        return x, S4hidden_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6kDdlwHoV2w"
   },
   "source": [
    "### HistoryEncoder\n",
    "HistoryEncoderはPriorに当たる<br>\n",
    "(次元を表す変数が\"*\\_dim\"だったり\"d\\_\\*\"だったりして紛らわしかもしれません)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1737217375716,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "e-kI32yUoV2w"
   },
   "outputs": [],
   "source": [
    "class HistoryEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "\n",
    "        z_dim, # z_dim=1024にする予定. Encoderからの出力zの次元と合わせる必要がある\n",
    "        action_dim,\n",
    "        gMLP_dim=512,\n",
    "\n",
    "        history_dim=512,\n",
    "        S4_mlp_dim=512,\n",
    "        S4_n_layers=8,\n",
    "        dropout=0.2,\n",
    "        prenorm=True,\n",
    "\n",
    "        zMLP_dim=512,\n",
    "        class_size=32,\n",
    "        category_size=32,\n",
    "        lr=0.001,\n",
    "    ):\n",
    "        super(HistoryEncoder, self).__init__()\n",
    "        self.class_size = class_size\n",
    "        self.category_size = category_size\n",
    "        self.gMLP = nn.Sequential(\n",
    "            nn.Linear(z_dim + action_dim, gMLP_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gMLP_dim, history_dim)\n",
    "        )\n",
    "\n",
    "        self.S4s = nn.ModuleList(\n",
    "            [S4Block(d_model=history_dim, d_mlp=S4_mlp_dim, n_layers=2, dropout=dropout, prenorm=prenorm, lr=lr) for _ in range(S4_n_layers)]\n",
    "            )\n",
    "\n",
    "        self.zMLP = nn.Sequential(\n",
    "            nn.Linear(history_dim, zMLP_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(zMLP_dim, z_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, action):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : torch.Tensor (batch_size, L, z_dim)\n",
    "            環境から得られた観測画像の潜在表現. この時点ではone-hot vectorである\n",
    "\n",
    "        action : torch.Tensor (batch_size, L, action_dim)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        h : torch.Tensor (batch_size, L, history_dim)\n",
    "            観測画像を埋め込み、カテゴリカル分布からサンプルしたもの(この時点ではone-hot vector)\n",
    "            勾配を通してあるのはreward lossからの勾配を計算するため\n",
    "\n",
    "        z : torch.Tensor (batch_size, L, z_dim)\n",
    "            次の環境の観測画像の潜在表現. この時点ではone-hot vectorである\n",
    "\n",
    "        dist: torch.distribution\n",
    "            zの分布.ELBOのKL-divergenceを計算するために必要. (B, L-1, category_size, class_size)の形の分布となる\n",
    "\n",
    "        dist_no_grad: torch.distribution\n",
    "            zの分布.ELBOのKL-divergenceを計算するために必要\n",
    "        \"\"\"\n",
    "        g = self.gMLP(torch.cat([z, action], dim=-1))\n",
    "        for S4 in self.S4s:\n",
    "            g = S4(g) # 最終的にhが出力される\n",
    "        h = g\n",
    "        logits = self.zMLP(h).reshape(*h.shape[:-1], self.category_size, self.class_size) # (batch_size, L, z_dim) -> (batch_size, L, category_dim, class_dim)\n",
    "        probs = torch.softmax(logits, dim=-1) * 0.99 + (0.01 / self.class_size)\n",
    "        one_hot_dist = torch.distributions.OneHotCategorical(probs=probs)\n",
    "        if z.size(1) > 1:\n",
    "            dist = torch.distributions.OneHotCategorical(probs=probs[:, :-1, :, :])\n",
    "            dist_no_grad = torch.distributions.OneHotCategorical(probs=probs[:, :-1, :, :].detach())\n",
    "        else:\n",
    "            dist, dist_no_grad = None, None\n",
    "        stoch = one_hot_dist.sample()\n",
    "        stoch += probs - probs.detach() # using \"straight-through gradients\"\n",
    "        z = torch.flatten(stoch, start_dim=-2, end_dim=-1)\n",
    "\n",
    "        return h, z, dist, dist_no_grad\n",
    "\n",
    "\n",
    "    def imagine(self, z, action, S4_hiddens=None):\n",
    "        # S4_hiddens is list of S4_hidden\n",
    "        g = self.gMLP(torch.cat([z, action], dim=-1))\n",
    "        if S4_hiddens is None:\n",
    "            S4_hidden = torch.zeros(512, 32, dtype=torch.complex64)  #初期隠れ状態として1×1×H×N//2のゼロ行列を作る（history_dim×64//2）\n",
    "            S4_hidden = S4_hidden.reshape(1, 1, 1, *S4_hidden.shape) #リストから取り出すと0次元目が消えるので\n",
    "            S4_hiddens = list(S4_hidden.to(device))\n",
    "        S4_hiddens_next = []\n",
    "        for S4, S4_hidden in zip(self.S4s, S4_hiddens):\n",
    "            g, S4_hidden_next = S4.imagine(g, S4_hidden) # 最終的にhが出力される S4_hiddenがx_k-1\n",
    "            S4_hiddens_next.append(S4_hidden_next)\n",
    "        h = g\n",
    "        logits = self.zMLP(h).reshape(*h.shape[:-1], self.category_size, self.class_size) # (batch_size, L, z_dim) -> (batch_size, L, category_dim, class_dim)\n",
    "        probs = torch.softmax(logits, dim=-1) * 0.99 + (0.01 / self.class_size)\n",
    "        one_hot_dist = torch.distributions.OneHotCategorical(probs=probs)\n",
    "        stoch = one_hot_dist.sample()\n",
    "        stoch += probs - probs.detach() # using \"straight-through gradients\"\n",
    "        z = torch.flatten(stoch, start_dim=-2, end_dim=-1)\n",
    "\n",
    "        return h, z, S4_hiddens_next\n",
    "\n",
    "    def deter(self, z, action):\n",
    "        with torch.no_grad():\n",
    "            g = self.gMLP(torch.cat([z, action], dim=-1))\n",
    "            for S4 in self.S4s:\n",
    "                g = self.S4(g) # 最終的にhが出力される\n",
    "            h = g\n",
    "            logits = self.zMLP(h).reshape(*h.shape[:-1], self.category_size, self.class_size) # (batch_size, L, z_dim) -> (batch_size, L, category_dim, class_dim)\n",
    "\n",
    "        max_indices = torch.argmax(logits, dim=-1)\n",
    "        one_hot = F.one_hot(max_indices, num_classes=logits.size(-1))\n",
    "        one_hot = one_hot.reshape(*one_hot.shape[:1], -1)\n",
    "        assert one_hot.size(-1) == self.category_size * self.class_size\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiIAlKWEoV2w"
   },
   "source": [
    "### Encoder, Decoder\n",
    "EncoderはPosteriorに当たる<br>\n",
    "(Decodeで画像にする必要はあるか？評価する上では画像にする必要はありそうだけど実際にモデルとしては軽いほうがいい)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1737217380035,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "2oZJZuF1oV2x"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    (input_dim, 64, 64)の画像を(1024,)のベクトルに変換する\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=3, # grayscaleなら1\n",
    "        category_size=32,\n",
    "        class_size=32,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.cv1 = nn.Conv2d(input_dim, 32, kernel_size=4, stride=2) # (input_dim, 64, 64) -> (32, 31, 31)\n",
    "        self.cv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2) # (32, 31, 31) -> (64, 14, 14)\n",
    "        self.cv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2) # (64, 14, 14) -> (128, 6, 6)\n",
    "        self.cv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2) # (128, 6, 6) -> (256, 2, 2)\n",
    "        self.category_size = category_size\n",
    "        self.class_size = class_size\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : torch.Tensor (batch_size, L, input_dim, 64, 64), Lは連続した観測画像の系列長\n",
    "            環境から得られた観測画像\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        z : torch.Tensor (batch_size, L, z_dim)\n",
    "            観測画像を埋め込み、カテゴリカル分布からサンプルしたもの(この時点ではone-hot vector)\n",
    "            勾配を通してあるのはreward lossからの勾配を計算するため\n",
    "\n",
    "        dist: torch.distribution\n",
    "            zの分布.ELBOのKL-divergenceを計算するために必要. (B, L-1, category_size, class_size)の形の分布となる\n",
    "\n",
    "        dist_no_grad: torch.distribution\n",
    "            zの分布.ELBOのKL-divergenceを計算するために必要\n",
    "        \"\"\"\n",
    "        B, L = obs.shape[:2]\n",
    "        hidden = F.silu(self.cv1(obs.reshape(B*L, *obs.shape[2:])))\n",
    "        hidden = F.silu(self.cv2(hidden))\n",
    "        hidden = F.silu(self.cv3(hidden))\n",
    "        hidden = F.silu(self.cv4(hidden))\n",
    "        # チャネルのハイパラがあっているかチェック\n",
    "        assert hidden.size(-3) * hidden.size(-2) * hidden.size(-1) == self.category_size * self.class_size\n",
    "        logits = hidden.reshape(B, L, self.category_size, self.class_size) # (B*L, 256, 2, 2) -> (B, L, category_size, class_size)\n",
    "        probs = torch.softmax(logits, dim=-1) * 0.99 + (0.01 / self.class_size)\n",
    "        one_hot_dist = torch.distributions.OneHotCategorical(probs=probs)\n",
    "        if L > 1:\n",
    "            dist = torch.distributions.OneHotCategorical(probs=probs[:, 1:, :, :])\n",
    "            dist_no_grad = torch.distributions.OneHotCategorical(probs=probs[:, 1:, :, :].detach())\n",
    "        else:\n",
    "            dist, dist_no_grad = None, None\n",
    "        stoch = one_hot_dist.sample()\n",
    "        stoch += probs - probs.detach() # using \"straight-through gradients\"\n",
    "        z = torch.flatten(stoch, start_dim=-2, end_dim=-1)\n",
    "        assert z.size(-1) == self.category_size * self.class_size\n",
    "\n",
    "        return z, dist, dist_no_grad\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    (z_dim + hidtory_dim,)のベクトルを(input_dim, 64, 64)の画像に変換する\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim=3, # grayscaleなら1\n",
    "        z_dim=1024,\n",
    "        history_dim=1024,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.fc = nn.Linear(z_dim + history_dim, 1024)\n",
    "        self.cv1 = nn.ConvTranspose2d(1024, 128, kernel_size=5, stride=2) # (1024, 1, 1) -> (128, 5, 5)\n",
    "        self.cv2 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2) # (128, 5, 5) -> (64, 13, 13)\n",
    "        self.cv3 = nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2) # (64, 13, 13) -> (32, 30, 30)\n",
    "        self.cv4 = nn.ConvTranspose2d(32, output_dim, kernel_size=6, stride=2) # (32, 30, 30) -> (input_dim, 64, 64)\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        h: torch.Tensor (batch_size, L, history_dim)\n",
    "            これまでの履歴(S4Blockからの出力)\n",
    "\n",
    "        z : torch.Tensor (batch_size, L, z_dim)\n",
    "            次の観測の潜在表現\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        obs : torch.Tensor (batch_size, L, output_dim, 64, 64)\n",
    "            次の観測画像\n",
    "        \"\"\"\n",
    "        B, L = h.shape[:2]\n",
    "        hidden = self.fc(torch.cat([z, h], dim=-1))\n",
    "        hidden = hidden.reshape(B*L, 1024, 1, 1)\n",
    "        hidden = F.silu(self.cv1(hidden))\n",
    "        hidden = F.silu(self.cv2(hidden))\n",
    "        hidden = F.silu(self.cv3(hidden))\n",
    "        obs = self.cv4(hidden)\n",
    "\n",
    "        return obs.reshape(B, L, *obs.shape[-3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5WIpq6xoV2x"
   },
   "source": [
    "### RewardModel\n",
    "報酬モデル. 1層のMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1737217383187,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "LDVB0EnToV2x"
   },
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        history_dim,\n",
    "        z_dim,\n",
    "        mlp_dim=512,\n",
    "    ):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(history_dim + z_dim, mlp_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(mlp_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        h: torch.Tensor (batch_size, L, history_dim)\n",
    "            これまでの履歴\n",
    "\n",
    "        z : torch.Tensor (batch_size, L, z_dim)\n",
    "            次の観測の潜在表現\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        reward : torch.Tensor (batch_size, L, 1)\n",
    "            報酬の予測値\n",
    "        \"\"\"\n",
    "        reward = self.fc1(torch.cat([h, z], dim=-1))\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEOr8I4eoV2x"
   },
   "source": [
    "### PolicyModel\n",
    "まだ実装しない（世界モデルのみをテストしてから）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1737217385364,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "VL6_IGiMoV2x"
   },
   "outputs": [],
   "source": [
    "class PolicyModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            z_dim,\n",
    "            history_dim,\n",
    "            action_dim,\n",
    "            mlp_dim=512,\n",
    "            min_std = 0.01\n",
    "        ):\n",
    "        super(PolicyModel, self).__init__()\n",
    "        self.min_std = min_std\n",
    "\n",
    "        self.fc1 = nn.Linear(history_dim + z_dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, mlp_dim)\n",
    "        self.fc3 = nn.Linear(mlp_dim, mlp_dim)\n",
    "        self.fc4 = nn.Linear(mlp_dim, mlp_dim)\n",
    "        self.fc_mean = nn.Linear(mlp_dim, action_dim)\n",
    "        self.fc_stddev = nn.Linear(mlp_dim, action_dim)\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        hidden = F.silu(self.fc1(torch.cat([h, z], dim=-1)))\n",
    "        hidden = F.silu(self.fc2(hidden))\n",
    "        hidden = F.silu(self.fc3(hidden))\n",
    "        hidden = F.silu(self.fc4(hidden))\n",
    "        mean = torch.tanh(self.fc_mean(hidden))\n",
    "        std = self.fc_stddev(hidden)\n",
    "        std = F.softplus(std) + self.min_std\n",
    "\n",
    "        eps = (torch.rand_like(mean) - 0.5) * 2\n",
    "        action = mean + eps * std\n",
    "\n",
    "        return action\n",
    "\n",
    "    def deter(self, h, z):\n",
    "        hidden = F.silu(self.fc1(torch.cat([h, z], dim=-1)))\n",
    "        hidden = F.silu(self.fc2(hidden))\n",
    "        hidden = F.silu(self.fc3(hidden))\n",
    "        hidden = F.silu(self.fc4(hidden))\n",
    "        mean = torch.tanh(self.fc_mean(hidden))\n",
    "\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwQvbtB8oV2y"
   },
   "source": [
    "### ValueModel\n",
    "まだ実装しない（世界モデルのみをテストしてから）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1737217387442,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "HM8MrWJ_oV2y"
   },
   "outputs": [],
   "source": [
    "class ValueModel(nn.Module):\n",
    "    def __init__(self, z_dim, history_dim, mlp_dim=512):\n",
    "        super(ValueModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(history_dim + z_dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, mlp_dim)\n",
    "        self.fc3 = nn.Linear(mlp_dim, mlp_dim)\n",
    "        self.fc4 = nn.Linear(mlp_dim, 1)\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        hidden = F.silu(self.fc1(torch.cat([h, z], dim=-1)))\n",
    "        hidden = F.silu(self.fc2(hidden))\n",
    "        hidden = F.silu(self.fc3(hidden))\n",
    "        value = self.fc4(hidden)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_HtwhDnoV2y"
   },
   "source": [
    "## 学習の実装編"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eokVbZFzoV2z"
   },
   "source": [
    "### 学習の実装（世界モデルのみテストする）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114024,
     "status": "ok",
     "timestamp": 1737217505696,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "We9vag6OoV2z",
    "outputId": "be671ce7-647d-4817-e03a-4ff5229bafd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#最初の数エピソードはランダムに行動して経験をリプレイバッファに集める\n",
    "env = make_env()\n",
    "buffer_capacity = 200000\n",
    "replay_buffer = ReplayBuffer(capacity=buffer_capacity, observation_shape=env.observation_space.shape, action_dim=env.action_space.shape[0])\n",
    "seed_episode=50\n",
    "for _ in range(seed_episode):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        replay_buffer.push(obs, action, reward, done)\n",
    "        obs = next_obs\n",
    "del env\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1737217505697,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "j8xpj05yoV2z",
    "outputId": "1ef4784d-4a2b-49b4-84db-8595255a5f7a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可視化のためにTensorBoardを用いるので，Colab上でTensorBoardを表示するための宣言を行う\n",
    "%cd /workspace/assets\n",
    "#学習結果を確認するためにTensorBoardを立ち上げておく\n",
    "log_dir = \"logs\"\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goRLmViLoV20"
   },
   "source": [
    "#### オプティマイザのセットアップ関数\n",
    "S4は重み減衰から外します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1737217505699,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "5raauUt_oV20"
   },
   "outputs": [],
   "source": [
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "def setup_optimizer(model_params, lr, weight_decay, epochs):\n",
    "    \"\"\"\n",
    "    S4 requires a specific optimizer setup.\n",
    "\n",
    "    The S4 layer (A, B, C, dt) parameters typically\n",
    "    require a smaller learning rate (typically 0.001), with no weight decay.\n",
    "\n",
    "    The rest of the model can be trained with a higher learning rate (e.g. 0.004, 0.01)\n",
    "    and weight decay (if desired).\n",
    "    \"\"\"\n",
    "\n",
    "    # General parameters don't contain the special _optim key\n",
    "    params = [p for p in model_params if not hasattr(p, \"_optim\")]\n",
    "\n",
    "    # Create an optimizer with the general parameters\n",
    "    optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Add parameters with special hyperparameters\n",
    "    hps = [getattr(p, \"_optim\") for p in model_params if hasattr(p, \"_optim\")]\n",
    "    hps = [\n",
    "        dict(s) for s in sorted(list(dict.fromkeys(frozenset(hp.items()) for hp in hps)))\n",
    "    ]  # Unique dicts\n",
    "    for hp in hps:\n",
    "        params = [p for p in model_params if getattr(p, \"_optim\", None) == hp]\n",
    "        optimizer.add_param_group(\n",
    "            {\"params\": params, **hp}\n",
    "        )\n",
    "\n",
    "    # Create a lr scheduler\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.2)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs+20)\n",
    "    # We use a linear warmup (1000 gradient steps) and cosine anneal learning rate schedule for S4WM.なので\n",
    "    scheduler = CosineLRScheduler(optimizer, t_initial=1000, lr_min=1e-3, warmup_t=1000, warmup_lr_init=1e-4, warmup_prefix=True)\n",
    "\n",
    "    # Print optimizer info\n",
    "    keys = sorted(set([k for hp in hps for k in hp.keys()]))\n",
    "    for i, g in enumerate(optimizer.param_groups):\n",
    "        group_hps = {k: g.get(k, None) for k in keys}\n",
    "        print(' | '.join([\n",
    "            f\"Optimizer group {i}\",\n",
    "            f\"{len(g['params'])} tensors\",\n",
    "        ] + [f\"{k} {v}\" for k, v in group_hps.items()]))\n",
    "\n",
    "    return optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F-ULMv1RnF6"
   },
   "source": [
    "#### 学習ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6344,
     "status": "ok",
     "timestamp": 1737217512030,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "8IK7Pp3ooV20",
    "outputId": "9ed8fb13-252c-4312-a6c5-1b9db774b483",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "env = make_env()\n",
    "# Hyperparameters\n",
    "grayscale = False\n",
    "category_size = 32\n",
    "class_size = 32\n",
    "z_dim = category_size * class_size\n",
    "history_dim = 512\n",
    "gMLP_dim = 1000\n",
    "S4_mlp_dim = 2048\n",
    "zMLP_dim = 1000\n",
    "dropout = 0.2\n",
    "rewardMLP_dim = 1024\n",
    "alpha = 0.8\n",
    "beta_recon = 1.0\n",
    "beta_rew = 35.0\n",
    "beta_kl = 10.0\n",
    "free_bits = 1.0\n",
    "model_lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "\n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "restart_training = False\n",
    "\n",
    "if not restart_training:\n",
    "    encoder = Encoder(\n",
    "        input_dim=(1 if grayscale else 3),\n",
    "        category_size=category_size,\n",
    "        class_size=class_size\n",
    "    ).to(device)\n",
    "\n",
    "    decoder = Decoder(\n",
    "        output_dim=(1 if grayscale else 3),\n",
    "        z_dim=z_dim,\n",
    "        history_dim=history_dim\n",
    "    ).to(device)\n",
    "\n",
    "    historyEncoder = HistoryEncoder(\n",
    "        z_dim=z_dim,\n",
    "        action_dim=env.action_space.shape[0],\n",
    "        gMLP_dim=gMLP_dim,\n",
    "        history_dim=history_dim,\n",
    "        S4_mlp_dim=S4_mlp_dim,\n",
    "        S4_n_layers=6,\n",
    "        dropout=dropout,\n",
    "        prenorm=True,\n",
    "        zMLP_dim=zMLP_dim,\n",
    "        class_size=class_size,\n",
    "        category_size=category_size,\n",
    "        lr=model_lr\n",
    "    ).to(device)\n",
    "\n",
    "    rewardModel = RewardModel(\n",
    "        history_dim=history_dim,\n",
    "        z_dim=z_dim,\n",
    "        mlp_dim=rewardMLP_dim\n",
    "    ).to(device)\n",
    "\n",
    "EPOCHS = 10 #1000\n",
    "# 世界モデルのパラメータたち\n",
    "model_params = (list(encoder.parameters()) + list(decoder.parameters())  + list(historyEncoder.parameters()) + list(rewardModel.parameters()))\n",
    "\n",
    "model_optimizer, scheduler = setup_optimizer(\n",
    "    model_params, lr=model_lr, weight_decay=weight_decay, epochs=EPOCHS\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "chunk_length = 100\n",
    "clip_grad_norm = 1000\n",
    "\n",
    "encoder.train(); decoder.train(); historyEncoder.train(); rewardModel.train()\n",
    "\n",
    "\n",
    "for episode in range(start_epoch, start_epoch + EPOCHS):\n",
    "    done = False\n",
    "    if len(replay_buffer) > batch_size * chunk_length:\n",
    "        observations, actions, rewards, dones = replay_buffer.sample(batch_size, chunk_length)\n",
    "        observations = preprocess_obs(observations)\n",
    "        observations = torch.tensor(observations, dtype=torch.float32).to(device)\n",
    "        observations = rearrange(observations, \"b t h w c -> b t c h w\")\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "        z_cur, posterior_dist, posterior_dist_no_grad = encoder(observations)\n",
    "        h, z_next, prior_dist, prior_dist_no_grad = historyEncoder(z_cur, actions)\n",
    "        recon_obs = decoder(h, z_next)\n",
    "        rewards_pred = rewardModel(h, z_next)\n",
    "\n",
    "        recon_loss = F.mse_loss(recon_obs[:, :-1, :, :, :], observations[:, 1:, :, :, :], reduction=\"none\").mean([0, 1]).sum()\n",
    "        reward_loss = F.mse_loss(rewards, rewards_pred, reduction=\"none\").mean([0, 1]).sum() # 報酬はインデックスずらさない\n",
    "\n",
    "        # KLダイバージェンスの重み付けスケジューリング\n",
    "        kl_anneal_steps = 1000\n",
    "        kl_weight = min(1.0, (episode + 1) / kl_anneal_steps)\n",
    "\n",
    "        # Free bitsの適用\n",
    "        kl_loss = (alpha * kl_divergence(posterior_dist_no_grad, prior_dist) + (1 - alpha) * kl_divergence(posterior_dist, prior_dist_no_grad)).sum(dim=-1).clamp(min=free_bits).mean([0,1]).sum()\n",
    "\n",
    "        loss = beta_recon * recon_loss + beta_rew * reward_loss + beta_kl * kl_loss\n",
    "\n",
    "        model_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_params, clip_grad_norm)\n",
    "        model_optimizer.step()\n",
    "\n",
    "        # writer.add_scalar('Loss/loss', loss.item(), episode)\n",
    "        # writer.add_scalar('Loss/recon_loss', recon_loss.item(), episode)\n",
    "        # writer.add_scalar('Loss/reward_loss', reward_loss.item(), episode)\n",
    "        # writer.add_scalar('Loss/kl_loss', kl_loss.item(), episode)\n",
    "        print(f\"Episode: {episode}\\n    Loss: {loss.item()}\")\n",
    "        print(f\"    reconstruction loss: {recon_loss.item()}\")\n",
    "        print(f\"    reward loss: {reward_loss.item()}\")\n",
    "        print(f\"    kl loss: {kl_loss.item()}\")\n",
    "\n",
    "        # Evaluation(Imaginationによる評価ではないので正確とは言えない。2個下にあるコードセルにImaginatinoの実装を作った)\n",
    "        if (episode+1) % 5000 == 0:\n",
    "            encoder.eval(); decoder.eval(); historyEncoder.eval(); rewardModel.eval()\n",
    "            env = make_env()\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            recon_error = 0\n",
    "            rew_error = 0\n",
    "            while not done:\n",
    "                action = env.action_space.sample()\n",
    "                next_obs, reward, done, _ = env.step(action)\n",
    "                replay_buffer.push(obs, action, reward, done)\n",
    "                obs_ = preprocess_obs(obs)\n",
    "                obs_ = torch.tensor(obs_, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n",
    "                obs_ = rearrange(obs_, \"b t h w c -> b t c h w\")\n",
    "                next_obs_ = preprocess_obs(next_obs)\n",
    "                next_obs_ = torch.tensor(next_obs_, dtype=torch.float32).to(device).reshape(1, 1, *next_obs.shape)\n",
    "                next_obs_ = rearrange(next_obs_, \"b t h w c -> b t c h w\")\n",
    "                action = torch.tensor(action, dtype=torch.float32).to(device).reshape(1, 1, *action.shape)\n",
    "                reward = torch.tensor(reward, dtype=torch.float32).to(device).reshape(1, 1, 1)\n",
    "                with torch.no_grad():\n",
    "                    z_cur, posterior_dist, posterior_dist_no_grad = encoder(obs_)\n",
    "                    h, z_next, prior_dist, prior_dist_no_grad = historyEncoder(z_cur, action)\n",
    "                    recon_obs = decoder(h, z_next)\n",
    "                    rewards_pred = rewardModel(h, z_next)\n",
    "\n",
    "                recon_error += F.mse_loss(recon_obs, next_obs_).sum()\n",
    "                rew_error += F.mse_loss(rewards_pred, reward).sum()\n",
    "\n",
    "                obs = next_obs\n",
    "\n",
    "            recon_error /= 500\n",
    "            rew_error /= 500\n",
    "            print(f\"Evaluation (episode: {episode})\\n Recon error: {recon_error.item()}, Reward error: {rew_error.item()}\")\n",
    "            # writer.add_scalar('Eval/Recon_error', recon_error.item(), episode)\n",
    "            # writer.add_scalar('Eval/Reward_error', rew_error.item(), episode)\n",
    "\n",
    "\n",
    "            encoder.train(); decoder.train(); historyEncoder.train(); rewardModel.train()\n",
    "            del env\n",
    "            gc.collect()\n",
    "\n",
    "    scheduler.step(episode)\n",
    "\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwbaqeuE2D4v",
    "outputId": "ce35051c-49e7-4f8d-b0fa-508cf0583c57"
   },
   "outputs": [],
   "source": [
    "posterior_dist.probs[0, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJhuNu8J2QY-",
    "outputId": "392b45ea-9a0e-4ea3-929e-f158a887873f"
   },
   "outputs": [],
   "source": [
    "prior_dist.probs[0, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCPIHoBO-07D"
   },
   "source": [
    "### 画像を再構成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPCZbJ6Gdgd_"
   },
   "outputs": [],
   "source": [
    "# 結果を動画で観てみるための関数\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "\n",
    "\n",
    "def display_video(frames: List[np.ndarray]) -> None:\n",
    "    \"\"\"\n",
    "    結果を動画にするための関数．\n",
    "\n",
    "    frames : List[np.ndarray]\n",
    "        観測画像をリスト化したもの．\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8), dpi=50)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        plt.title(\"Step %d\" % (i))\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "    display(HTML(anim.to_jshtml(default_mode=\"once\")))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 707,
     "status": "error",
     "timestamp": 1737217512730,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "k9yWUnZ3dnWn",
    "outputId": "4e49fa2c-0eeb-48f6-ff26-f38388a9ae2b"
   },
   "outputs": [],
   "source": [
    "encoder.eval(); decoder.eval(); historyEncoder.eval(); rewardModel.eval()\n",
    "env = make_env()\n",
    "obs = env.reset()\n",
    "if True:\n",
    "    observations, actions, rewards, dones = replay_buffer.sample(1, chunk_length)\n",
    "    observations = preprocess_obs(observations)\n",
    "    observations = torch.tensor(observations, dtype=torch.float32).to(device)\n",
    "    observations = rearrange(observations, \"b t h w c -> b t c h w\")\n",
    "    actions = torch.tensor(actions, dtype=torch.float32).to(device)\n",
    "\n",
    "    z_cur, posterior_dist, posterior_dist_no_grad = encoder(observations)\n",
    "    h, z_next, prior_dist, prior_dist_no_grad = historyEncoder(z_cur, actions)\n",
    "    recon_obs = decoder(h, z_next)\n",
    "    pred_rewards = rewardModel(h, z_next)\n",
    "\n",
    "    recon_images = recon_obs[0][:-1].cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    recon_images = (recon_images + 0.5).clip(0.0, 1.0)\n",
    "    original_images = observations[0][1:].cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    original_images = (original_images + 0.5).clip(0.0, 1.0)\n",
    "    images = np.concatenate([original_images, recon_images], axis=-2)\n",
    "    print(images.shape)\n",
    "    frames = []\n",
    "    for i in range(chunk_length-1):\n",
    "        frames.append(images[i])\n",
    "\n",
    "    display_video(frames)\n",
    "\n",
    "    rewards = rewards[0, :, 0]\n",
    "    pred_rewards = pred_rewards[0, :, 0].cpu().detach().numpy()\n",
    "    plt.plot(np.arange(len(rewards)), rewards, label=\"real\")\n",
    "    plt.plot(np.arange(len(pred_rewards)), pred_rewards, label=\"pred\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ピアソンの相関係数を計算\n",
    "    #correlation = np.corrcoef(rewards, pred_rewards)[0, 1]\n",
    "\n",
    "    #print(f\"ピアソンの相関係数: {correlation}\")\n",
    "\n",
    "del env\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zi1OwA2zRnF7",
    "outputId": "6f51955d-4b4e-42cd-eb1e-898f446f0d87"
   },
   "outputs": [],
   "source": [
    "torch.sum(torch.abs(z_next[:-1] - z_cur[1:])) / 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJmLplS2RnF8"
   },
   "source": [
    "### 複数ステップ先の予測（Imagination part）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4748,
     "status": "error",
     "timestamp": 1737217528455,
     "user": {
      "displayName": "大蔵春日",
      "userId": "03754790342905464126"
     },
     "user_tz": -540
    },
    "id": "ZNmzRNjrRnF8",
    "outputId": "e014f2ba-5501-4162-e41c-326bef3aed4b"
   },
   "outputs": [],
   "source": [
    "encoder.eval(); decoder.eval(); historyEncoder.eval(); rewardModel.eval()\n",
    "env = make_env()\n",
    "done = False\n",
    "obs = env.reset()\n",
    "obs = preprocess_obs(obs)\n",
    "obs = torch.tensor(obs, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n",
    "obs = rearrange(obs, \"b t h w c -> b t c h w\")\n",
    "frames = []\n",
    "reward_real_list = []\n",
    "reward_pred_list = []\n",
    "S4_hiddens = None\n",
    "with torch.no_grad():\n",
    "    z_cur, posterior_dist, posterior_dist_no_grad = encoder(obs)\n",
    "assert z_cur.shape == (1, 1, z_dim), \"z_curs.shapeは(1, 1, z_dim)を想定しています\"\n",
    "# action = env.action_space.sample()\n",
    "# actions = torch.tensor(action, dtype=torch.float32).to(device).reshape(1, 1, *action.shape)\n",
    "\n",
    "# S4には今までの行動とz_curをすべて入れないと隠れ状態を0として計算してしまいます。\n",
    "# なので、毎回concateして、actionsとz_cursを作っています。（もっと効率的なやり方があれ変えてください）\n",
    "# imagineを使用する方法に書き換え\n",
    "for _ in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, _ = env.step(action)\n",
    "    reward_real_list.append(reward)\n",
    "\n",
    "    next_obs = preprocess_obs(next_obs).reshape(1, 1, *next_obs.shape)\n",
    "    next_obs = torch.tensor(next_obs, dtype=torch.float32).to(device)\n",
    "    next_obs = rearrange(next_obs, \"b t h w c -> b t c h w\")\n",
    "    reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "\n",
    "    action = torch.tensor(action, dtype=torch.float32).to(device).reshape(1, 1, *action.shape) # (6,) -> [1, 1, 6]\n",
    "\n",
    "# z_curs.shape = [1, 1, 1024] actions.shape = [1, 1, 6]\n",
    "    with torch.no_grad():\n",
    "        #print(z_cur.shape)\n",
    "        #print(action.shape)\n",
    "        h, z_cur, S4_hiddens = historyEncoder.imagine(z_cur, action, S4_hiddens) # hs, z_nexts, _, _ = historyEncoder(z_curs, actions)\n",
    "        h, z_cur = h[:, -1].reshape(1, 1, h.size(-1)), z_cur[:, -1].reshape(1, 1, z_cur.size(-1))\n",
    "        recon_obs = decoder(h, z_cur)\n",
    "        pred_reward = rewardModel(h, z_cur)\n",
    "    # z_curs = torch.cat([z_curs, z_next], dim=1) [1, 1, 1024]->[1, 2, 1024]\n",
    "\n",
    "\n",
    "    # action = torch.tensor(action, dtype=torch.float32).to(device).reshape(1, 1, *action.shape)\n",
    "    # actions = action #torch.cat([actions, action], dim=1) [1, 1, 6] -> [1, 2, 6]\n",
    "    # action = env.action_space.sample() # 次の行動をサンプルしておく\n",
    "\n",
    "    recon_image = recon_obs[0][0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    recon_image = (recon_image + 0.5).clip(0.0, 1.0)\n",
    "    original_image = next_obs[0][0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    original_image = (original_image + 0.5).clip(0.0, 1.0)\n",
    "    image = np.concatenate([original_image, recon_image], axis=-2)\n",
    "    frames.append(image)\n",
    "\n",
    "    pred_reward = pred_reward[0][0].cpu().detach().numpy()\n",
    "    reward_pred_list.append(pred_reward.item())\n",
    "\n",
    "# 動画を生成\n",
    "display_video(frames)\n",
    "\n",
    "# 報酬を描画\n",
    "plt.plot(np.arange(len(reward_real_list)), reward_real_list, label=\"real\")\n",
    "plt.plot(np.arange(len(reward_pred_list)), reward_pred_list, label=\"pred\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ピアソンの相関係数を計算\n",
    "correlation = np.corrcoef(reward_real_list, reward_pred_list)[0, 1]\n",
    "\n",
    "print(f\"ピアソンの相関係数: {correlation}\")\n",
    "\n",
    "del env\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNtlmaVsRnF8",
    "outputId": "0f6b7550-668a-441a-b830-7a66c5d6d64b"
   },
   "outputs": [],
   "source": [
    "torch.sum(torch.abs(z_nexts[0, :-1] - z_curs[0, 1:-1])) / (2*49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2s-dYSomL7O",
    "outputId": "6f60e01a-24d9-47bf-8011-a7dde3492d11"
   },
   "outputs": [],
   "source": [
    "torch.abs(z_cur[0, 1:] - z_next[0, :-1]).sum() / (2 * (chunk_length-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3T2G_BesYmj",
    "outputId": "ed16d6ce-67b1-45cf-c6b3-95675909e361"
   },
   "outputs": [],
   "source": [
    "torch.abs(z_cur[0, 1:] - z_next[0, :-1]).sum() / (2 * (chunk_length-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ13nxcPoV20"
   },
   "source": [
    "### 学習の実装（方策モデルも含めてタスクを解く学習）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfiOqKxJoV21"
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "save_model_dir = \"/workspace/assets/params\"\n",
    "env = make_env()\n",
    "\n",
    "# Hyperparameters\n",
    "grayscale = False\n",
    "category_size = 32\n",
    "class_size = 32\n",
    "z_dim = category_size * class_size\n",
    "history_dim = 512\n",
    "gMLP_dim = 1000\n",
    "S4_mlp_dim = 2048\n",
    "zMLP_dim = 1000\n",
    "dropout = 0.2\n",
    "rewardMLP_dim = 1024\n",
    "alpha = 0.8\n",
    "beta_recon = 1.0\n",
    "beta_rew = 35.0\n",
    "beta_kl = 10.0\n",
    "free_bits = 1.0\n",
    "worldModel_lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "gamma = 0.9\n",
    "lambda_ = 0.95\n",
    "\n",
    "\n",
    "\n",
    "start_episode = 0\n",
    "restart_training = False\n",
    "\n",
    "if not restart_training:\n",
    "    encoder = Encoder(\n",
    "        input_dim=(1 if grayscale else 3),\n",
    "        category_size=category_size,\n",
    "        class_size=class_size\n",
    "    ).to(device)\n",
    "\n",
    "    decoder = Decoder(\n",
    "        output_dim=(1 if grayscale else 3),\n",
    "        z_dim=z_dim,\n",
    "        history_dim=history_dim\n",
    "    ).to(device)\n",
    "\n",
    "    historyEncoder = HistoryEncoder(\n",
    "        z_dim=z_dim,\n",
    "        action_dim=env.action_space.shape[0],\n",
    "        gMLP_dim=gMLP_dim,\n",
    "        history_dim=history_dim,\n",
    "        S4_mlp_dim=S4_mlp_dim,\n",
    "        S4_n_layers=6,\n",
    "        dropout=dropout,\n",
    "        prenorm=True,\n",
    "        zMLP_dim=zMLP_dim,\n",
    "        class_size=class_size,\n",
    "        category_size=category_size,\n",
    "        lr=worldModel_lr\n",
    "    ).to(device)\n",
    "\n",
    "    rewardModel = RewardModel(\n",
    "        history_dim=history_dim,\n",
    "        z_dim=z_dim,\n",
    "        mlp_dim=rewardMLP_dim\n",
    "    ).to(device)\n",
    "\n",
    "    policyModel = PolicyModel(\n",
    "        z_dim=z_dim,\n",
    "        history_dim=history_dim,\n",
    "        action_dim=env.action_space.shape[0],\n",
    "        mlp_dim=512\n",
    "    ).to(device)\n",
    "\n",
    "    valueModel = ValueModel(\n",
    "        z_dim=z_dim,\n",
    "        history_dim=history_dim,\n",
    "        mlp_dim=512\n",
    "    ).to(device)\n",
    "\n",
    "EPISODE = 100\n",
    "update_steps = 100\n",
    "imagination_horizon = 15\n",
    "test_interval = 10\n",
    "model_save_interval = 20\n",
    "\n",
    "# 世界モデルのパラメータたち\n",
    "world_model_params = (list(encoder.parameters()) + list(decoder.parameters())  + list(historyEncoder.parameters()) + list(rewardModel.parameters()))\n",
    "\n",
    "worldModel_optimizer, scheduler = setup_optimizer(\n",
    "    world_model_params, lr=worldModel_lr, weight_decay=weight_decay, epochs=EPISODE * update_steps\n",
    ")\n",
    "policy_optimizer = optim.AdamW(policyModel.parameters(), lr=5e-4)\n",
    "value_optimizer = optim.AdamW(valueModel.parameters(), lr=5e-4)\n",
    "\n",
    "batch_size = 8\n",
    "chunk_length = 100\n",
    "clip_grad_norm = 1000\n",
    "\n",
    "encoder.train(); decoder.train(); historyEncoder.train(); rewardModel.train()\n",
    "\n",
    "\n",
    "for episode in range(start_episode, start_episode + EPISODE):\n",
    "\n",
    "    # 実環境とinteraction\n",
    "    env = make_env()\n",
    "    obs = env.reset()\n",
    "    obs_ = preprocess_obs(obs)\n",
    "    obs_ = torch.tensor(obs_, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n",
    "    obs_ = rearrange(obs_, \"b t h w c -> b t c h w\")\n",
    "    z_cur, _, _ = encoder(obs_)\n",
    "    z_curs, actions = None, None\n",
    "    h = torch.zeros(1, 1, history_dim).to(device) # 疑似的なhistory\n",
    "    done = False\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = policyModel(z_cur, h)\n",
    "            action = ((torch.rand_like(action) - 0.5) / 5  + action)[0][0].cpu().detach().numpy()\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            replay_buffer.push(obs, action, reward, done)\n",
    "\n",
    "            obs_ = preprocess_obs(obs)\n",
    "            obs_ = torch.tensor(obs_, dtype=torch.float32).to(device).reshape(1, 1, *next_obs.shape)\n",
    "            obs_ = rearrange(obs_, \"b t h w c -> b t c h w\")\n",
    "            action_ = torch.tensor(action, dtype=torch.float32).to(device).reshape(1, 1, *action.shape)\n",
    "            z_cur, _, _ = encoder(obs_)\n",
    "            if z_curs == None and actions == None:\n",
    "                z_curs, actions = z_cur, action_\n",
    "            else:\n",
    "                z_curs, actions = torch.cat([z_curs, z_cur]), torch.cat([actions, action_])\n",
    "\n",
    "            hs, z_nexts, _, _ = historyEncoder(z_curs, actions)\n",
    "            z_cur, h = z_nexts[:, -1].unsqueeze(1), hs[:, -1].unsqueeze(1)\n",
    "\n",
    "    # 世界モデル更新部分\n",
    "    if len(replay_buffer) > batch_size * chunk_length:\n",
    "        start = time.time()\n",
    "        for step in range(update_steps):\n",
    "            observations, actions, rewards, dones = replay_buffer.sample(batch_size, chunk_length)\n",
    "            observations = preprocess_obs(observations)\n",
    "            observations = torch.tensor(observations, dtype=torch.float32).to(device)\n",
    "            observations = rearrange(observations, \"b t h w c -> b t c h w\")\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "            z_cur, posterior_dist, posterior_dist_no_grad = encoder(observations)\n",
    "            h, z_next, prior_dist, prior_dist_no_grad = historyEncoder(z_cur, actions)\n",
    "            recon_obs = decoder(h, z_next)\n",
    "            rewards_pred = rewardModel(h, z_next)\n",
    "\n",
    "            recon_loss = F.mse_loss(recon_obs[:, :-1, :, :, :], observations[:, 1:, :, :, :], reduction=\"none\").mean([0, 1]).sum()\n",
    "            reward_loss = F.mse_loss(rewards, rewards_pred, reduction=\"none\").mean([0, 1]).sum() # 報酬はインデックスずらさない\n",
    "\n",
    "            # KLダイバージェンスの重み付けスケジューリング\n",
    "            # kl_anneal_steps = 1000\n",
    "            # kl_weight = min(1.0, (episode + 1) / kl_anneal_steps)\n",
    "\n",
    "            # Free bitsの適用\n",
    "            kl_loss = (alpha * kl_divergence(posterior_dist_no_grad, prior_dist) + (1 - alpha) * kl_divergence(posterior_dist, prior_dist_no_grad)).sum(dim=-1).clamp(min=free_bits).mean([0,1]).sum()\n",
    "\n",
    "            loss = beta_recon * recon_loss + beta_rew * reward_loss + beta_kl * kl_loss\n",
    "\n",
    "            worldModel_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(world_model_params, clip_grad_norm)\n",
    "            worldModel_optimizer.step()\n",
    "\n",
    "            print(f\"Step: {episode * update_steps + step}\\n    Loss: {loss.item()}\")\n",
    "            print(f\"    reconstruction loss: {recon_loss.item()}\")\n",
    "            print(f\"    reward loss: {reward_loss.item()}\")\n",
    "            print(f\"    kl loss: {kl_loss.item()}\")\n",
    "\n",
    "            # 方策モデル、価値モデルの学習\n",
    "            zs, hs = z_next[:, -1].unsqueeze(1), h[:, -1].unsqueeze(1)\n",
    "            actions = None\n",
    "            for _ in range(imagination_horizon+1):\n",
    "                z_, h_ = zs[:, -1].unsqueeze(1), hs[:, -1].unsqueeze(1)\n",
    "                action = policyModel(z_, h_)\n",
    "                if actions == None:\n",
    "                    actions = action\n",
    "                else:\n",
    "                    actions = torch.cat([actions, action], dim=1)\n",
    "                hs_, z_nexts_, _, _ = historyEncoder(zs, actions)\n",
    "                h, z_next = hs_[:, -1].unsqueeze(1), z_nexts_[:, -1].unsqueeze(1)\n",
    "                zs = torch.cat([zs, z_next], dim=1)\n",
    "                hs = torch.cat([hs, h], dim=1)\n",
    "\n",
    "            imagined_rewards = rewardModel(hs, zs)\n",
    "            imagined_values = valueModel(hs, zs)\n",
    "            lambda_target_values = lambda_target(imagined_rewards, imagined_values, gamma, lambda_)\n",
    "            action_loss = -lambda_target_values.mean()\n",
    "            policy_optimizer.zero_grad()\n",
    "            action_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(policyModel.parameters(), clip_grad_norm)\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            imagined_values = valueModel(hs.detach(), zs.detach())\n",
    "            value_loss = 0.5 * F.mse_loss(imagined_values, lambda_target_values.detach())\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(valueModel.parameters(), clip_grad_norm)\n",
    "            value_optimizer.step()\n",
    "\n",
    "            print(f\"Episode: {episode}  Policy loss: {action_loss}, Value loss: {value_loss}\")\n",
    "        print('elasped time for update: %.2fs' % (time.time() - start))\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    #    テストフェーズ．探索ノイズなしでの性能を評価する\n",
    "    # --------------------------------------------------------------\n",
    "    if (episode + 1) % test_interval == 0:\n",
    "        start = time.time()\n",
    "        obs = env.reset()\n",
    "        obs = preprocess_obs(obs)\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n",
    "        obs = rearrange(obs, \"b t h w c -> b t c h w\")\n",
    "        with torch.no_grad():\n",
    "            z, _, _ = encoder(obs)\n",
    "        h = torch.zeros(1, 1, history_dim).to(device)\n",
    "        zs, actions = None, None\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = policyModel.deter(h, z).cpu().detach().numpy()\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            action_ = torch.tensor(action, dtype=torch.float32).to(device)\n",
    "            total_reward += reward\n",
    "            obs = preprocess_obs(obs)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32).to(device).reshape(1, 1, *obs.shape)\n",
    "            obs = rearrange(obs, \"b t h w c -> b t c h w\")\n",
    "            with torch.no_grad():\n",
    "                z_, _, _ = encoder(obs)\n",
    "            if zs == None and actions == None:\n",
    "                zs, actions = z_, action_\n",
    "            else:\n",
    "                zs = torch.cat([zs, z_], dim=-1)\n",
    "                actions = torch.cat([actions, action_], dim=-1)\n",
    "            h, z, _, _ = historyEncoder(zs, actions)\n",
    "\n",
    "        writer.add_scalar('total reward at test', total_reward, episode)\n",
    "        print('Total test reward at episode [%4d/%4d] is %f' %\n",
    "                (episode+1, EPISODE, total_reward))\n",
    "        print('elasped time for test: %.2fs' % (time.time() - start))\n",
    "\n",
    "    if (episode + 1) % model_save_interval == 0:\n",
    "        # 定期的に学習済みモデルのパラメータを保存する\n",
    "        model_log_dir = os.path.join(save_model_dir, 'episode_%04d' % (episode + 1))\n",
    "        os.makedirs(model_log_dir, exist_ok=True)\n",
    "        torch.save(encoder.state_dict(), os.path.join(model_log_dir, 'encoder.pth'))\n",
    "        torch.save(decoder.state_dict(), os.path.join(model_log_dir, 'decoder.pth'))\n",
    "        torch.save(historyEncoder.state_dict(), os.path.join(model_log_dir, 'historyEncoder.pth'))\n",
    "        torch.save(rewardModel.state_dict(), os.path.join(model_log_dir, 'rewardModel.pth'))\n",
    "        torch.save(valueModel.state_dict(), os.path.join(model_log_dir, 'valueModel.pth'))\n",
    "        torch.save(policyModel.state_dict(), os.path.join(model_log_dir, 'policyModel.pth'))\n",
    "    del env # envをそのまま残しておくとエラーが出たかららしい\n",
    "    gc.collect()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHyycC0ARnF9"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir='./logs'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
