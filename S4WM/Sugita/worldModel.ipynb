{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!git clone https://github.com/state-spaces/s4.git"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%cd /content/s4\n",
    "# ============= Set Up =============\n",
    "# Requirements\n",
    "!conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n",
    "!pip install -r requirements.txt\n",
    "# Structured Kernels\n",
    "%cd extensions/kernels/\n",
    "!python setup.py install\n",
    "%cd /content/s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.\n",
      "Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : Cuda libraries were not detected on the system or could not be loaded ; using cpu only mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gomag\\anaconda3\\envs\\gymenv2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models.s4.s4 import S4Block as S4  # Can use full version instead of minimal S4D standalone below\n",
    "from models.s4.s4d import S4D\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "from typing import Any, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import pybullet_envs  # PyBulletの環境をgymに登録する\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Dropout broke in PyTorch 1.11\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) == (1, 11):\n",
    "    print(\"WARNING: Dropout is bugged in PyTorch 1.11. Results may be worse.\")\n",
    "    dropout_fn = nn.Dropout\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 12):\n",
    "    dropout_fn = nn.Dropout1d\n",
    "else:\n",
    "    dropout_fn = nn.Dropout2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパラの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "#TODO: このハイパラたちは後で書き換える（学習コード作ってから最後に書いたほうがいい）\n",
    "lr = 0.001\n",
    "weight_decay = 0.01\n",
    "num_workers = 4\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "d_mlp = 512\n",
    "prenorm = True\n",
    "dropout = 0.2\n",
    "grad_clip = 1000\n",
    "\n",
    "hyperparameters = {\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"num_workers\":  num_workers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"d_model\": d_model,\n",
    "    \"d_mlp\": d_mlp,\n",
    "    \"prenorm\": prenorm,\n",
    "    \"dropout\": dropout,\n",
    "    \"grad_clip\": grad_clip,\n",
    "}\n",
    "\n",
    "# ハイパラの種類が今後増える可能性を踏まえ、ファイル名にversionを記載する(hyparaVxxとなるように)\n",
    "current_time = datetime.datetime.now()\n",
    "current_time_str = current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "with open(f'hyparams/hyparaV1_{current_time_str}.json', 'w') as f:\n",
    "    json.dump(hyperparameters, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境のWrapper（カメラに関する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymWrapper_PyBullet(object):\n",
    "    \"\"\"\n",
    "    PyBullet環境のためのラッパー\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"]}\n",
    "    reward_range = (-np.inf, np.inf)\n",
    "\n",
    "    # __init__でカメラ位置に関するパラメータ（ cam_dist:カメラ距離，cam_yaw：カメラの水平面での回転，cam_pitch:カメラの縦方向での回転）を受け取り，カメラの位置を調整できるようにします.\n",
    "    # 　同時に画像の大きさも変更できるようにします\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        cam_dist: int = 3,\n",
    "        cam_yaw: int = 0,\n",
    "        cam_pitch: int = -30,\n",
    "        render_width: int = 320,\n",
    "        render_height: int = 240,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        コンストラクタ．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : gym.Env\n",
    "            gymで提供されている環境のインスタンス．\n",
    "        cam_dist : int\n",
    "            カメラの距離．\n",
    "        cam_yaw : int\n",
    "            カメラの水平面での回転．\n",
    "        cam_pitch : int\n",
    "            カメラの縦方向での回転．\n",
    "        render_width : int\n",
    "            観測画像の幅．\n",
    "        render_height : int\n",
    "            観測画像の高さ．\n",
    "        \"\"\"\n",
    "        self._env = env\n",
    "\n",
    "        self._render_width = render_width\n",
    "        self._render_height = render_height\n",
    "        self._set_nested_attr(self._env, cam_dist, \"_cam_dist\")\n",
    "        self._set_nested_attr(self._env, cam_yaw, \"_cam_yaw\")\n",
    "        self._set_nested_attr(self._env, cam_pitch, \"_cam_pitch\")\n",
    "        self._set_nested_attr(self._env, render_width, \"_render_width\")\n",
    "        self._set_nested_attr(self._env, render_height, \"_render_height\")\n",
    "\n",
    "    def _set_nested_attr(self, env: gym.Env, value: int, attr: str) -> None:\n",
    "        \"\"\"\n",
    "        多重継承の属性に再帰的にアクセスして値を変更する．\n",
    "        カメラの設定に利用．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : int\n",
    "            設定したい値．\n",
    "        attr : str\n",
    "            変更したい属性の名前．\n",
    "        \"\"\"\n",
    "        if hasattr(env, attr):\n",
    "            setattr(env, attr, value)\n",
    "        else:\n",
    "            self._set_nested_attr(env.env, value, attr)\n",
    "\n",
    "    def __getattr(self, name: str) -> Any:\n",
    "        \"\"\"\n",
    "        環境が保持している属性値を取得するメソッド．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            取得したい属性値の名前．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _env.name : Any\n",
    "            環境が保持している属性値．\n",
    "        \"\"\"\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    @property\n",
    "    def observation_space(self) -> gym.spaces.Box:\n",
    "        \"\"\"\n",
    "        観測空間に関する情報を取得するメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        space : gym.spaces.Box\n",
    "            観測空間に関する情報（各画素値の最小値，各画素値の最大値，観測データの形状， データの型）．\n",
    "        \"\"\"\n",
    "        width = self._render_width\n",
    "        height = self._render_height\n",
    "        return gym.spaces.Box(0, 255, (height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    @property\n",
    "    def action_space(self) -> gym.spaces.Box:\n",
    "        \"\"\"\n",
    "        行動空間に関する情報を取得するメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        space : gym.spaces.Box\n",
    "            行動空間に関する情報（各行動の最小値，各行動の最大値，行動空間の次元， データの型） ．\n",
    "        \"\"\"\n",
    "        return self._env.action_space\n",
    "\n",
    "    # 　元の観測（低次元の状態）は今回は捨てて，env.render()で取得した画像を観測とします.\n",
    "    #  画像，報酬，終了シグナルが得られます.\n",
    "    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n",
    "        \"\"\"\n",
    "        環境に行動を与え次の観測，報酬，終了フラグを取得するメソッド．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : np.dnarray (action_dim, )\n",
    "            与える行動．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (height, width, 3)\n",
    "            行動を与えたときの次の観測．\n",
    "        reward : float\n",
    "            行動を与えたときに得られる報酬．\n",
    "        done : bool\n",
    "            エピソードが終了したかどうか表すフラグ．\n",
    "        info : dict\n",
    "            その他の環境に関する情報．\n",
    "        \"\"\"\n",
    "        _, reward, done, info = self._env.step(action)\n",
    "        obs = self._env.render(mode=\"rgb_array\") # 今回状態として画像を扱いたいため\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        環境をリセットするためのメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (height, width, 3)\n",
    "            環境をリセットしたときの初期の観測．\n",
    "        \"\"\"\n",
    "        self._env.reset()\n",
    "        obs = self._env.render(mode=\"rgb_array\")\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode=\"human\", **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        観測をレンダリングするためのメソッド．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mode : str\n",
    "            レンダリング方法に関するオプション． (default='human')\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (height, width, 3)\n",
    "            観測をレンダリングした結果．\n",
    "        \"\"\"\n",
    "        return self._env.render(mode, **kwargs)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        環境を閉じるためのメソッド．\n",
    "        \"\"\"\n",
    "        self._env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### カメラに関するWrapperのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetahBulletEnv-v0\")\n",
    "# カメラのパラメータを与えてカメラの位置と角度，画像の大きさを調整\n",
    "env = GymWrapper_PyBullet(\n",
    "    env, cam_dist=2, cam_pitch=0, render_width=64, render_height=64\n",
    ")\n",
    "\n",
    "env.reset()\n",
    "image = env.render(mode=\"rgb_array\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "env.close()\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境のWrapper（行動の連続入力に関する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatAction(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    同じ行動を指定された回数自動的に繰り返すラッパー．観測は最後の行動に対応するものになる\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: GymWrapper_PyBullet, skip: int = 4) -> None:\n",
    "        \"\"\"\n",
    "        コンストラクタ．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : GymWrapper_PyBullet\n",
    "            環境のインスタンス．今回は先程定義したラッパーでラップした環境を利用する．\n",
    "        skip : int\n",
    "            同じ行動を繰り返す回数．\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        環境をリセットするためのメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (width, height, 3)\n",
    "            環境をリセットしたときの初期の観測．\n",
    "        \"\"\"\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n",
    "        \"\"\"\n",
    "        環境に行動を与え次の観測，報酬，終了フラグを取得するメソッド．\n",
    "        与えられた行動をskipの回数だけ繰り返した結果を返す．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : np.ndarray (action_dim, )\n",
    "            与える行動．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : np.ndarray (width, height, 3)\n",
    "            行動をskipの回数だけ繰り返したあとの観測．\n",
    "        total_reawrd : float\n",
    "            行動をskipの回数だけ繰り返したときの報酬和．\n",
    "        done : bool\n",
    "            エピソードが終了したかどうか表すフラグ．\n",
    "        info : dict\n",
    "            その他の環境に関する情報．\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapperを通した環境を作る関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env() -> RepeatAction:\n",
    "    \"\"\"\n",
    "    作成たラッパーをまとめて適用して環境を作成する関数．\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    env : RepeatAction\n",
    "        ラッパーを適用した環境．\n",
    "    \"\"\"\n",
    "    env = gym.make(\"HalfCheetahBulletEnv-v0\")  # 環境を読み込む．今回はHalfCheetah\n",
    "    # Dreamerでは観測は64x64のRGB画像\n",
    "    env = GymWrapper_PyBullet(\n",
    "        env, cam_dist=2, cam_pitch=0, render_width=64, render_height=64\n",
    "    )\n",
    "    env = RepeatAction(env, skip=2)  # DreamerではActionRepeatは2\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "連続した経験をとってくるのでDQNとは少し違う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 　今回のReplayBuffer\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    RNNを用いて訓練するのに適したリプレイバッファ．\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, capacity: int, observation_shape: List[int], action_dim: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        コンストラクタ．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            リプレイバッファにためておくことができる経験の上限．\n",
    "        observation_shape : List[int]\n",
    "            環境から与えられる観測の形状．\n",
    "        action_dim : int\n",
    "            行動空間の次元数．\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "\n",
    "        self.observations = np.zeros((capacity, *observation_shape), dtype=np.uint8)\n",
    "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.done = np.zeros((capacity, 1), dtype=bool)\n",
    "        # self.done = np.zeros((capacity, 1), dtype=np.bool)\n",
    "\n",
    "        self.index = 0\n",
    "        self.is_filled = False\n",
    "\n",
    "    def push(\n",
    "        self, observation: np.ndarray, action: np.ndarray, reward: float, done: bool\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        リプレイバッファに経験を追加するメソッド．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : np.ndarray (64, 64, 3)\n",
    "            環境から得られた観測．\n",
    "        action : np.ndarray (action_dim, )\n",
    "            エージェントがとった（もしくは経験を貯める際のランダムな）行動．\n",
    "        reward : float\n",
    "            観測に対して行動をとったときに得られる報酬．\n",
    "        done : bool\n",
    "            エピソードが終了するかどうかのフラグ．\n",
    "        \"\"\"\n",
    "        self.observations[self.index] = observation\n",
    "        self.actions[self.index] = action\n",
    "        self.rewards[self.index] = reward\n",
    "        self.done[self.index] = done\n",
    "\n",
    "        # indexは巡回し，最も古い経験を上書きする\n",
    "        if self.index == self.capacity - 1:\n",
    "            self.is_filled = True\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size: int, chunk_length: int) -> Tuple[np.ndarray]:\n",
    "        \"\"\"\n",
    "        経験をリプレイバッファからサンプルします．（ほぼ）一様なサンプルです．\n",
    "        結果として返ってくるのは観測（画像），行動，報酬，終了シグナルについての(batch_size, chunk_length, 各要素の次元)の配列です．\n",
    "        各バッチは連続した経験になっています．\n",
    "        注意: chunk_lengthをあまり大きな値にすると問題が発生する場合があります．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            バッチサイズ．\n",
    "        chunk_length : int\n",
    "            バッチあたりの系列長．\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sampled_observations : np.ndarray (batch size, chunk length, 3, 64, 64)\n",
    "            バッファからサンプリングされた観測．\n",
    "        sampled_actions : np.ndarray (batch size, chunk length, action dim)\n",
    "            バッファからサンプリングされた行動．\n",
    "        sampled_rewards : np.ndarray (batch size, chunk length, 1)\n",
    "            バッファからサンプリングされた報酬．\n",
    "        sampled_done : np.ndarray (batch size, chunk length, 1)\n",
    "            バッファからサンプリングされたエピソードの終了フラグ．\n",
    "        \"\"\"\n",
    "        episode_borders = np.where(self.done)[0]\n",
    "        sampled_indexes = []\n",
    "        for _ in range(batch_size):\n",
    "            cross_border = True\n",
    "            while cross_border:\n",
    "                initial_index = np.random.randint(len(self) - chunk_length + 1)\n",
    "                final_index = initial_index + chunk_length - 1\n",
    "                cross_border = np.logical_and(\n",
    "                    initial_index <= episode_borders, episode_borders < final_index\n",
    "                ).any()  # 論理積\n",
    "            sampled_indexes += list(range(initial_index, final_index + 1))\n",
    "\n",
    "        sampled_observations = self.observations[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, *self.observations.shape[1:]\n",
    "        )\n",
    "        sampled_actions = self.actions[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, self.actions.shape[1]\n",
    "        )\n",
    "        sampled_rewards = self.rewards[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, 1\n",
    "        )\n",
    "        sampled_done = self.done[sampled_indexes].reshape(batch_size, chunk_length, 1)\n",
    "        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        バッファに貯められている経験の数を返すメソッド．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        length : int\n",
    "            バッファに貯められている経験の数．\n",
    "        \"\"\"\n",
    "        return self.capacity if self.is_filled else self.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 観測の前処理を行う関数\n",
    "ラッパーとして最初から適用してしまわないのは，リプレイバッファにはより容量の小さなnp．uint8の形式で保存しておきたいためです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obs(obs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    画像を正規化する．[0, 255] -> [-0.5, 0.5]．\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obs : np.ndarray (64, 64, 3) or (chank length, batch size, 64, 64, 3)\n",
    "        環境から得られた観測．画素値は[0, 255]．\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    normalized_obs : np.ndarray (64, 64, 3) or (chank length, batch size, 64, 64, 3)\n",
    "        画素値を[-0.5, 0.5]で正規化した観測．\n",
    "    \"\"\"\n",
    "    obs = obs.astype(np.float32)\n",
    "    normalized_obs = obs / 255.0 - 0.5\n",
    "    return normalized_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### λ-returnを計算する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_target(\n",
    "    rewards: torch.Tensor, values: torch.Tensor, gamma: float, lambda_: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    価値関数の学習のためのλ-returnを計算する関数．\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rewards : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n",
    "        報酬モデルによる報酬の推定値．\n",
    "    values : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n",
    "        価値関数を近似するValueモデルによる状態価値観数の推定値．\n",
    "    gamma : float\n",
    "        割引率．\n",
    "    lambda_ : float\n",
    "        λ-returnのパラメータλ．\n",
    "\n",
    "    V_lambda : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n",
    "        各状態に対するλ-returnの値．\n",
    "    \"\"\"\n",
    "    V_lambda = torch.zeros_like(rewards, device=rewards.device)\n",
    "\n",
    "    H = rewards.shape[0] - 1\n",
    "    V_n = torch.zeros_like(rewards, device=rewards.device)\n",
    "    V_n[H] = values[H]\n",
    "    for n in range(1, H + 1):\n",
    "        # まずn-step returnを計算します\n",
    "        # 注意: 系列が途中で終わってしまったら，可能な中で最大のnを用いたn-stepを使います\n",
    "        V_n[:-n] = (gamma**n) * values[n:]\n",
    "        for k in range(1, n + 1):\n",
    "            if k == n:\n",
    "                V_n[:-n] += (gamma ** (n - 1)) * rewards[k:]\n",
    "            else:\n",
    "                V_n[:-n] += (gamma ** (k - 1)) * rewards[k : -n + k]\n",
    "\n",
    "        # lambda_でn-step returnを重みづけてλ-returnを計算します\n",
    "        if n == H:\n",
    "            V_lambda += (lambda_ ** (H - 1)) * V_n\n",
    "        else:\n",
    "            V_lambda += (1 - lambda_) * (lambda_ ** (n - 1)) * V_n\n",
    "\n",
    "    return V_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここからはモデルの実装編"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4Block (p26 Figure21参照)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=256,\n",
    "        d_mlp = 512,\n",
    "        n_layers=2,\n",
    "        dropout=0.2,\n",
    "        prenorm=True,\n",
    "    ):\n",
    "        super(S4Block, self).__init__()\n",
    "        \n",
    "        assert d_model % 2 == 0, \"For GLU, d_model must be even!!\"\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts1 = nn.ModuleList()\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.glus = nn.ModuleList()\n",
    "        self.dropouts2= nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.s4_layers.append(\n",
    "                S4D(d_model, dropout=dropout, transposed=True, lr=min(0.001, lr))\n",
    "            )\n",
    "            self.dropouts1.append(dropout_fn(dropout))\n",
    "            self.linears.append(nn.Linear(d_model, 2*d_model))\n",
    "            self.glus.append(nn.GLU())\n",
    "            self.dropouts2.append(dropout_fn(dropout))\n",
    "\n",
    "        self.norm_mlp = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_mlp),\n",
    "            nn.GELU(),\n",
    "            dropout_fn(dropout),\n",
    "            nn.Linear(d_model, d_mlp),\n",
    "            dropout_fn(dropout)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_model), L is the length of continuous observations, B is the batch size\n",
    "        \"\"\"\n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        for s4, norm, dropout1, linear, glu, dropout2 in \\\n",
    "            zip(self.s4_layers, self.norms, self.dropouts1, self.linears, self.glus, self.dropouts2):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = s4(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout1(z)\n",
    "\n",
    "            # Mixing informations\n",
    "            z = linear(z.transpose(-1, -2)).transpose(-1, -2) # (B, L, d_model) -> (B, L, 2*d_model)\n",
    "            z = glu(z) # (B, L, 2*d_model) -> (B, L, d_model)\n",
    "            z = dropout2(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, d_model, L) -> (B, L, d_model)\n",
    "\n",
    "        #TODO: x_にも操作が反映されてたりしないか確認する. residual connectionのため\n",
    "        x_ = x\n",
    "        x = x_ + self.norm_mlp(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HistoryEncoder\n",
    "HistoryEncoderはPriorに当たる<br>\n",
    "(次元を表す変数が\"*\\_dim\"だったり\"d\\_\\*\"だったりして紛らわしかもしれません)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "\n",
    "        z_dim, # z_dim=1024にする予定. Encoderからの出力zの次元と合わせる必要がある\n",
    "        action_dim,\n",
    "        gMLP_dim=512,\n",
    "\n",
    "        history_dim=256,\n",
    "        S4_mlp_dim=512,\n",
    "        S4_n_layers=2,\n",
    "        dropout=0.2,\n",
    "        prenorm=True,\n",
    "\n",
    "        zMLP_dim=512,\n",
    "    ):\n",
    "        super(HistoryEncoder, self).__init__()\n",
    "        self.gMLP = nn.Sequential([\n",
    "            nn.Linear(z_dim + action_dim, gMLP_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gMLP_dim, history_dim)\n",
    "        ])\n",
    "\n",
    "        self.S4 = S4Block(d_model=history_dim, d_mlp=S4_mlp_dim, n_layers=S4_n_layers, dropout=dropout, prenorm=prenorm)\n",
    "        \n",
    "        self.zMLP = nn.Sequential([\n",
    "            nn.Linear(history_dim, zMLP_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(zMLP_dim, z_dim)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, z, action):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : torch.Tensor (batch_size, L, z_dim)\n",
    "            環境から得られた観測画像の潜在表現. この時点ではone-hot vectorである\n",
    "\n",
    "        action : torch.Tensor (batch_size, L, action_dim)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        h : torch.Tensor (batch_size, L, 1024)\n",
    "            観測画像を埋め込み、カテゴリカル分布からサンプルしたもの(この時点ではone-hot vector)\n",
    "            勾配を通してあるのはreward lossからの勾配を計算するため\n",
    "        \n",
    "        z : torch.Tensor (batch_size, L, z_dim)\n",
    "            次の環境の観測画像の潜在表現. この時点ではone-hot vectorである\n",
    "        \n",
    "        dist : torch.distribution\n",
    "            zの分布. ELBOのKL-divergenceを計算するために必要\n",
    "        \"\"\"\n",
    "        g = self.gMLP(torch.cat([z, action], dim=-1))\n",
    "        h = self.S4(g)\n",
    "        logit = self.zMLP(h)\n",
    "        dist = torch.distributions.OneHotCategorical(logits=logit)        \n",
    "        stoch = dist.sample()\n",
    "        stoch += dist.probs - dist.probs.detach() # using \"straight-through gradients\"\n",
    "        z = torch.flatten(stoch, start_dim=-2, end_dim=-1)\n",
    "\n",
    "        return h, z, dist\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder, Decoder\n",
    "EncoderはPosteriorに当たる<br>\n",
    "(Decodeで画像にする必要はあるか？評価する上では画像にする必要はありそうだけど実際にモデルとしては軽いほうがいい)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    (input_dim, 64, 64)の画像を(1024,)のベクトルに変換する\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=3, # grayscaleなら1\n",
    "        category_size=32,\n",
    "        class_size=32,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.cv1 = nn.Conv2d(input_dim, 32, kernel_size=4, stride=2) # (input_dim, 64, 64) -> (32, 31, 31)\n",
    "        self.cv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2) # (32, 31, 31) -> (64, 14, 14)\n",
    "        self.cv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2) # (64, 14, 14) -> (128, 6, 6)\n",
    "        self.cv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2) # (128, 6, 6) -> (256, 2, 2)\n",
    "        self.category_size = category_size\n",
    "        self.class_size = class_size\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : torch.Tensor (batch_size, L, input_dim, 64, 64), Lは連続した観測画像の系列長\n",
    "            環境から得られた観測画像\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        z : torch.Tensor (batch_size, L, 1024)\n",
    "            観測画像を埋め込み、カテゴリカル分布からサンプルしたもの(この時点ではone-hot vector)\n",
    "            勾配を通してあるのはreward lossからの勾配を計算するため\n",
    "        \n",
    "        dist: torch.distribution\n",
    "            zの分布. ELBOのKL-divergenceを計算するために必要\n",
    "        \"\"\"\n",
    "        hidden = F.silu(self.cv1(obs))\n",
    "        hidden = F.silu(self.cv2(hidden))\n",
    "        hidden = F.silu(self.cv3(hidden))\n",
    "        logit = F.silu(self.cv4(hidden)).reshape(*hidden.shape[:-3], self.category_size, self.class_size) # (batch_size, L, 256, 2, 2) -> (batch_size, L, 32, 32)\n",
    "        dist = torch.distributions.OneHotCategorical(logits=logit)        \n",
    "        stoch = dist.sample()\n",
    "        stoch += dist.probs - dist.probs.detach() # using \"straight-through gradients\"\n",
    "        z = torch.flatten(stoch, start_dim=-2, end_dim=-1)\n",
    "        \n",
    "        return z, dist\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    (1024,)のベクトルを(input_dim, 64, 64)の画像に変換する\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim=3, # grayscaleなら1\n",
    "        z_dim=1024,\n",
    "        history_dim=1024,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.fc = nn.Linear(z_dim + history_dim, 1024)\n",
    "        self.cv1 = nn.ConvTranspose2d(1024, 128, kernel_size=5, stride=2) # (1024, 1, 1) -> (128, 5, 5)\n",
    "        self.cv2 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2) # (128, 5, 5) -> (64, 13, 13)\n",
    "        self.cv3 = nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2) # (64, 13, 13) -> (32, 30, 30)\n",
    "        self.cv4 = nn.ConvTranspose2d(32, output_dim, kernel_size=6, stride=2) # (32, 30, 30) -> (input_dim, 64, 64)\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        h: torch.Tensor (batch_size, L, history_dim)\n",
    "            これまでの履歴(S4Blockからの出力)\n",
    "\n",
    "        z : torch.Tensor (batch_size, L, z_dim)\n",
    "            次の観測の潜在表現\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        obs : torch.Tensor (batch_size, L, output_dim, 64, 64)\n",
    "            次の観測画像\n",
    "        \"\"\"\n",
    "        hidden = self.fc(torch.cat([z, h], dim=-1))\n",
    "        hidden = hidden.view(*hidden.shape[:-1],1024, 1, 1)\n",
    "        hidden = F.silu(self.cv1(hidden))\n",
    "        hidden = F.silu(self.cv2(hidden))\n",
    "        hidden = F.silu(self.cv3(hidden))\n",
    "        obs = self.cv4(hidden)\n",
    "        \n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RewardModel\n",
    "報酬モデル. 1層のMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        history_dim,\n",
    "        z_dim,\n",
    "        mlp_dim=512,\n",
    "    ):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc1 = nn.Sequential([\n",
    "            nn.Linear(history_dim + z_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, h, z):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        h: torch.Tensor (batch_size, L, history_dim)\n",
    "            これまでの履歴\n",
    "\n",
    "        z : torch.Tensor (batch_size, L, z_dim)\n",
    "            次の観測の潜在表現\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        reward : torch.Tensor (batch_size, L, 1)\n",
    "            報酬の予測値\n",
    "        \"\"\"\n",
    "        reward = self.fc1(torch.cat([h, z], dim=-1))\n",
    "        \n",
    "        return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolicyModel\n",
    "まだ実装しない（世界モデルのみをテストしてから）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ValueModel\n",
    "まだ実装しない（世界モデルのみをテストしてから）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実装編"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータの設定と学習の準備\n",
    "\n",
    "ハイパーパラメータを設定し，モデルやリプレイバッファを宣言して学習の準備を整えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "\n",
    "# リプレイバッファの宣言\n",
    "buffer_capacity = 200000  # Colabのメモリの都合上，元の実装より小さめにとっています\n",
    "replay_buffer = ReplayBuffer(\n",
    "    capacity=buffer_capacity,\n",
    "    observation_shape=env.observation_space.shape,\n",
    "    action_dim=env.action_space.shape[0],\n",
    ")\n",
    "\n",
    "# モデルの宣言\n",
    "\n",
    "# ハイパラ\n",
    "# encoder用\n",
    "grayscale = True\n",
    "category_size = 16\n",
    "class_size = 16\n",
    "# decoder用\n",
    "z_dim = category_size * class_size\n",
    "history_dim = 256 # history_dimの分だけS4-layerはコピーされるので、あまり大きすぎると計算量が大変かも\n",
    "# rewardModel用\n",
    "rewardMLP_dim = 256\n",
    "# historyEncoder用\n",
    "gMLP_dim = 512\n",
    "S4_mlp_dim = 512\n",
    "zMLP_dim = 512\n",
    "dropout = 0.2\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim=(1 if grayscale else 3),\n",
    "    category_size=category_size,\n",
    "    class_size=class_size\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim=(1 if grayscale else 3),\n",
    "    z_dim=z_dim,\n",
    "    history_dim=history_dim\n",
    ").to(device) # z_dimはcategory_size * class_sizeとなる必要がある\n",
    "\n",
    "rewardModel = RewardModel(\n",
    "    history_dim=history_dim,\n",
    "    z_dim=z_dim,\n",
    "    mlp_dim=rewardMLP_dim\n",
    ")\n",
    "\n",
    "historyEncoder = HistoryEncoder(\n",
    "    z_dim=z_dim,\n",
    "    action_dim=env.action_space.shape[0],\n",
    "    gMLP_dim=gMLP_dim,\n",
    "    history_dim=history_dim,\n",
    "    S4_mlp_dim=S4_mlp_dim,\n",
    "    S4_n_layers=2,\n",
    "    dropout=dropout,\n",
    "    prenorm=True,\n",
    "    zMLP_dim=zMLP_dim\n",
    ")\n",
    "\n",
    "#=======================ここまで=========================\n",
    "\n",
    "# オプティマイザの宣言\n",
    "model_lr = 6e-4  # encoder, rssm, obs_model, reward_modelの学習率\n",
    "value_lr = 8e-5\n",
    "action_lr = 8e-5\n",
    "eps = 1e-4\n",
    "model_params = (\n",
    "    list(encoder.parameters())\n",
    "    + list(rssm.transition.parameters())\n",
    "    + list(rssm.observation.parameters())\n",
    "    + list(rssm.reward.parameters())\n",
    ")\n",
    "model_optimizer = torch.optim.Adam(model_params, lr=model_lr, eps=eps)\n",
    "value_optimizer = torch.optim.Adam(value_model.parameters(), lr=value_lr, eps=eps)\n",
    "action_optimizer = torch.optim.Adam(action_model.parameters(), lr=action_lr, eps=eps)\n",
    "\n",
    "# その他ハイパーパラメータ\n",
    "seed_episodes = 5  # 最初にランダム行動で探索するエピソード数\n",
    "all_episodes = 100  # 学習全体のエピソード数（300ほどで，ある程度収束します）\n",
    "test_interval = 10  # 何エピソードごとに探索ノイズなしのテストを行うか\n",
    "model_save_interval = 20  # NNの重みを何エピソードごとに保存するか\n",
    "collect_interval = 100  # 何回のNNの更新ごとに経験を集めるか（＝1エピソード経験を集めるごとに何回更新するか）\n",
    "\n",
    "action_noise_var = 0.3  # 探索ノイズの強さ\n",
    "\n",
    "batch_size = 50\n",
    "chunk_length = 50  # 1回の更新で用いる系列の長さ\n",
    "imagination_horizon = 15  # Actor-Criticの更新のために，Dreamerで何ステップ先までの想像上の軌道を生成するか\n",
    "\n",
    "\n",
    "gamma = 0.9  # 割引率\n",
    "lambda_ = 0.95  # λ-returnのパラメータ\n",
    "clip_grad_norm = 100  # gradient clippingの値\n",
    "free_nats = 3  # KL誤差（RSSMのTransitionModelにおけるpriorとposteriorの間の誤差）がこの値以下の場合，無視する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "# %tensorboard --logdir='./logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習の実装（世界モデルのみテストする）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習の実装（方策モデルも含めてタスクを解く学習）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
